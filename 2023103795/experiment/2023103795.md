**实验报告：Reversed in Time：一个新颖的强调时间性的跨模态视频-文本检索基准**

2023103795 杜洋

**摘要：** 视频-文本检索是多模态理解领域的一个重要任务。时间理解使得视频-文本检索比图像-文本检索更具挑战性。然而，我们发现广泛使用的视频-文本基准在全面评估模型能力方面存在不足，特别是在时间理解方面，导致大规模图像-文本预训练模型已经能够实现与视频-文本预训练模型相当的零样本性能。在本文中，我们介绍了RTime，一个新颖的强调时间性的视频-文本检索数据集，通过自上而下的三步方案构建。我们首先获取具有显著时间性的动作或事件视频，然后将这些视频反转以创建更难的负样本。我们随后招募标注者来判断候选视频的重要性和可逆性，并为合格的视视频撰写标题。我们还进一步采用GPT-4基于人类撰写的标题扩展更多标题。我们的RTime数据集目前包含21,000个视频，每个视频有10个标题，总计约122小时。基于RTime，我们提出了三个检索基准任务：RTime-Origin、RTime-Hard和RTime-Binary。我们进一步增强了模型训练中更难负样本的使用，并在RTime上基准测试了多种视频-文本模型。广泛的实验分析证明，RTime确实为视频-文本检索提出了新的和更高的挑战。我们将发布我们的RTime基准，以进一步推动视频-文本检索和多模态理解研究。

**关键词：** 视频-文本跨模态检索；视频-文本评测基准；时序理解。

**1. 引言** 视频-文本检索已被广泛应用于各种现实世界场景，如视频搜索引擎和视频推荐系统。它比图像-文本检索更具挑战性，因为它不仅需要在空间上理解多个帧的视觉语义，还需要在时间上进行理解。

**2. 相关工作** 近年来，大规模视觉-语言预训练模型的引入，通过对比学习学习跨模态对齐，为视频-文本检索带来了显著的性能提升。然而，先前的工作指出，缺乏一个强调时间理解的视频-文本基准。我们随机从MSRVTT 测试集中抽样了100个视频，并发现只有10%的视频-文本对涉及时间语义。此外，大多数视频-文本数据集在创建时没有明确地包含具备相似特征但时序语义不同的视频，这使得它们不足以评估模型的时间理解能力。

**3. 数据集介绍与处理** 为了解决当前数据集的上述不足，我们提出构建一个新的强调时间性的数据集，名为RTime，并为视频-文本检索建立新的基准。具体来说，我们采用自上而下的三步方案来构建我们的数据集。我们首先人工设计一些具有强烈时间性的典型活动的常识信息（例如，以格式：打开/关闭某物）来形成初始活动列表，然后使用GPT-4进一步扩展它，随后进行手动验证，以确保每个活动都有其时间上反转的对应项（更难的负样本）。随后，我们使用GPT-4将每个动作中的“某物”替换为典型对象，从而产生大量包含活动和指定对象的短语。这些短语随后被用作查询，通过搜索引擎在互联网上搜索视频，从而收集了大量的视频。接下来，我们招募了一组专业标注者来筛选和标注收集到的视频。我们向标注者提供原始视频和反转视频对，并要求他们检测视频是否可以时间反转。标注者选择符合要求的视频，并为每个视频提供细粒度的描述。我们还进一步应用GPT-4根据人类编写的标题为每个视频重写九个语义相似的句子，以允许更多样化的视觉-语言对齐，这已被证明对视觉-语言对比学习有益。

**4. 评测基准** 为了在RTime上对模型进行评测，我们建立了三个评估任务：

- **RTime-Origin**：典型的视频-文本检索任务，其中检索池仅包含原始检索的视频-文本样本。
- **RTime-Hard**：对于RTime-Hard检索，测试集中添加了视频的反转对应项及其伴随文本，这要求模型具有更强的处理时间理解的能力。
- **RTime-Binary**：对于RTime-Binary检索，给定查询，模型需要从两个候选样本中选择正确的对应样本，这两个样本之间的唯一区别在于时间方面。

**5. 模型结构** 我们选择用来评测的模型采用了双流架构，包括一个独立的视觉编码器和一个文本编码器，随后是一个跨模态对齐模块。视频和文本编码器分别将视频和文本编码成视觉和文本特征。跨模态对齐模块涉及一个轻量级的Transformer层，用于融合视觉和文本特征，并输出视频和文本之间的相似度得分矩阵。我们还在RTime中通过在微调期间将正样本和更难的负样本（即反转的对应项）放在同一个批次中来增强更难负样本的使用，例如UMT-Neg表示我们使用这种增强的更难负样本微调的UMT。

**6. 模型评测** 

我们在三个RTime基准任务上评估了不同的最先进（SOTA）模型：RTime-Origin、RTime-Both和RTime-Binary，这些任务要求越来越强的时间理解能力。具体来说，我们在零样本实验中评估了两个图像-文本预训练模型（即CLIP和BLIP），一个在视频-文本数据集上使用单帧预训练的模型（即Singularity），以及两个视频-文本模型（即VINDLU和UMT ）。我们还微调了两个基于CLIP的时间适应图像-文本模型（即CLIP4Clip和Ts2Net）和UMT。

**6. 实验结果分析** 通过实验，我们得出以下结论：

- **RTime-Origin**：如实验结果所示，由于我们的视频描述比之前的基准更细粒度和更长，它们更不模糊，大多数模型在RTime-Origin上都能取得满意的结果。但在RTime-Origin上，图像-文本预训练模型（CLIP R@1 58.7，BLIP R@1 71.8）、使用单帧预训练的模型（Singularity R@1 74.1）和视频-文本预训练模型（UMT R@1 80.3）在相同模型大小之间存在性能差距。在RTime-Hard上也可以观察到相同趋势，即（CLIP R@1 28.8，BLIP R@1 36.2）对比（Singularity R@1 36.2）对比（UMT R@1 40.2）。与图2中显示的结果显著不同，其中CLIP和Singularity与UMT表现相当，这些比较实验表明我们的RTime更强调时间性，因此更有效地评估不同的视频检索模型。上述现象在更大规模的模型（BLIP-L vs UMT-L）中也是相同的。

- **RTime-Hard**：此外，我们观察到各种模型在RTime-Hard上的性能与RTime-Origin相比都有显著下降，其中R@1指标的性能下降最为显著。这是因为模型可以很容易地根据静态视觉信息找到具有相同视觉外观但不同时间语义的视频。然而，进一步正确区分时间顺序是具有挑战性的，导致R@1的退化。我们还观察到，经过微调后，基于CLIP的模型（CLIP4Clip和Ts2Net）的性能显著提高，这在一定程度上表明了图像-文本模型在时间理解方面的不足，也显示了模型学习时间理解能力的必要性。

- **RTime-Binary**：RTime-Binary任务仅关注评估时间理解能力。实验结果显示，我们采用的所有模型表现不佳，即使经过微调也是如此。这表明RTime确实对当前模型提出了重大挑战。即使是微调后的模型也只能取得略好于随机选择的结果。此外，值得注意的是，采用我们增强的更难负样本使用的UMT-Neg在RTime-Binary任务上取得了明显的增益。

**7. 讨论** 总体而言，我们在RTime任务上使用各种SOTA模型的基准测试结果表明，一方面，RTime确实对视频-文本检索提出了更高的挑战，另一方面，当前的视频-文本模型在时间理解方面仍然不足。

**8. 结论** 本工作的目的是解决现有视频-文本检索基准中时间理解评估的不足。我们介绍了RTime，一个新颖的细粒度强调时间性的视频-文本数据集，通过利用大型语言模型和人类专家的力量，精心构建了一个自上而下的三步流程。我们进一步建立了三个基准任务：RTime-Origin检索、RTime-Hard检索和RTime-Binary检索，这些任务可以支持对视频理解能力，特别是在时间理解方面的全面和忠实评估。广泛的实验分析证实，我们的RTime确实对视频-文本检索提出了更高的挑战。我们希望我们的工作将引起更多对时间理解重要性的关注，并为视频字幕、视频问答和多模态大型语言模型评估等更广泛的视频-语言理解任务的进展做出贡献

**9. 备注** 相关图片、实验结果表格和更多消融实验分析详见overleaf链接中的论文。数据集由于包含大量视频，且未来得及上传至网盘，在此仅提供原始标注文件，如有需要可索取。程序部分为UMT模型。