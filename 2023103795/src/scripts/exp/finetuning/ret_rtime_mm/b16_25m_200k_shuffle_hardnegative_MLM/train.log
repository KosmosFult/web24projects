2024-05-11T13:52:33 | INFO | umt : Logging to: exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM/train.log
2024-05-11T13:52:33 | INFO | utils.config_utils : config: {
  data_dir: /data2/dy/code/unmasked_teacher/umt_data
  data_root: /data2/dy/code/unmasked_teacher/umt_data/videos_images
  anno_root_pt: /data2/dy/code/unmasked_teacher/umt_data/anno_pretrain
  anno_root_downstream: /data2/dy/code/unmasked_teacher/umt_data/anno_downstream
  TextEncoders: {
      bert: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      bert_large: {
          name: bert_large
          pretrained: bert-large-uncased
          config: configs/config_bert_large.json
          d_model: 1024
          fusion_layer: 19 } }
  train_file: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_rewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video']
  test_file: {
      test: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video'] }
  test_types: ['test']
  num_workers: 6
  stop_key: test/
  is_paragraph_retrieval: False
  is_pretrain: True
  num_frames: 12
  num_frames_test: 12
  batch_size: 32
  max_txt_l: 32
  evaluate: False
  train_shuffle: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 12
          sample_type: rand
          num_frames_test: 12
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 32
          video: 32 }
      batch_size: {
          image: 32
          video: 32 }
      batch_size_test: {
          image: 32
          video: 32 } }
  text_enc: bert
  model: {
      model_cls: UMT
      vision_encoder: {
          name: vit_b16
          img_size: 224
          patch_size: 16
          d_model: 768
          encoder_embed_dim: 768
          encoder_depth: 12
          encoder_num_heads: 12
          drop_path_rate: 0.2
          num_frames: 12
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 12
          clip_decoder_embed_dim: 768
          clip_output_dim: 512
          clip_return_layer: 0
          clip_student_return_interval: 1
          pretrained: /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
          clip_teacher: none
          clip_img_size: 224
          clip_return_interval: 1
          video_mask_type: attention
          video_mask_ratio: 0.0
          video_double_mask_ratio: 0.0
          image_mask_type: attention
          image_mask_ratio: 0.0
          image_double_mask_ratio: 0.0
          keep_temporal: True }
      text_encoder: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      multimodal: {
          enable: True }
      embed_dim: 512
      temp: 0.07 }
  criterion: {
      loss_weight: {
          vtc: 1.0
          mlm: 1.0
          vtm: 1.0
          uta: 0.0 }
      vtm_hard_neg: True
      mlm_masking_prob: 0.5
      uta_norm_type: l2
      uta_loss_type: l2 }
  optimizer: {
      opt: adamW
      lr: 1e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 5
      min_lr_multi: 0.01
      warmup_epochs: 1 }
  zero_shot: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: user
      project: umt_ret }
  dist_url: env://
  device: cuda
  mode: pt
  output_dir: exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM
  resume: False
  debug: False
  log_freq: 100
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
  rank: 0
  world_size: 4
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-05-11T13:52:33 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_rewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video'], 'test_file': {'test': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': True, 'num_frames': 12, 'num_frames_test': 12, 'batch_size': 32, 'max_txt_l': 32, 'evaluate': False, 'train_shuffle': False, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 12, 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 32, 'video': 32}, 'batch_size': {'image': 32, 'video': 32}, 'batch_size_test': {'image': 32, 'video': 32}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.2, 'num_frames': 12, 'tubelet_size': 1, 'use_checkpoint': True, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 1.0, 'vtm': 1.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 5, 'min_lr_multi': 0.01, 'warmup_epochs': 1}, 'zero_shot': False, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 42, 'save_latest': True, 'auto_resume': True, 'pretrained_path': '/datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth', 'rank': 0, 'world_size': 4, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl'}
2024-05-11T13:52:33 | INFO | __main__ : train_file: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_rewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video']
2024-05-11T13:52:33 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-11T13:52:33 | INFO | tasks.shared_utils : Creating model
2024-05-11T13:52:45 | INFO | models.umt : Build vision_encoder: vit_b16
2024-05-11T13:52:45 | INFO | models.backbones.vit.vit : Num of patches: 2352
2024-05-11T13:52:45 | INFO | models.backbones.vit.vit : Use checkpoint: True
2024-05-11T13:52:45 | INFO | models.backbones.vit.vit : Checkpoint number: 12
2024-05-11T13:52:45 | INFO | models.backbones.vit.vit : Student return index: []
2024-05-11T13:52:52 | INFO | models.backbones.vit.vit : Loading pretrained weights from /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
2024-05-11T13:52:53 | INFO | models.umt : Build text_encoder bert_base
2024-05-11T13:52:54 | INFO | models.backbones.bert.xbert : build bert with cross_module: ca
2024-05-11T13:52:55 | INFO | models.criterions : Norm type: l2
2024-05-11T13:52:55 | INFO | models.criterions : Loss type: l2
2024-05-11T13:52:55 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.temp: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.word_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.position_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.token_type_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.dense.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.vision_proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.text_proj.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.itm_head.weight: wd: 0.02, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : param module.itm_head.bias: wd: 0, lr: 1e-05
2024-05-11T13:52:55 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0.02 len(p)=141
2024-05-11T13:52:55 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0 len(p)=260
2024-05-11T13:52:55 | INFO | tasks.shared_utils : Auto resuming
2024-05-11T13:52:55 | INFO | tasks.shared_utils : Not found checkpoint in exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM
2024-05-11T13:52:56 | INFO | tasks.shared_utils : _IncompatibleKeys(missing_keys=[], unexpected_keys=['clip_teacher.class_embedding', 'clip_teacher.positional_embedding', 'clip_teacher.proj', 'clip_teacher.conv1.weight', 'clip_teacher.ln_pre.weight', 'clip_teacher.ln_pre.bias', 'clip_teacher.transformer.resblocks.0.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.0.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.0.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.0.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.0.ln_1.weight', 'clip_teacher.transformer.resblocks.0.ln_1.bias', 'clip_teacher.transformer.resblocks.0.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.0.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.0.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.0.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.0.ln_2.weight', 'clip_teacher.transformer.resblocks.0.ln_2.bias', 'clip_teacher.transformer.resblocks.1.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.1.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.1.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.1.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.1.ln_1.weight', 'clip_teacher.transformer.resblocks.1.ln_1.bias', 'clip_teacher.transformer.resblocks.1.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.1.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.1.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.1.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.1.ln_2.weight', 'clip_teacher.transformer.resblocks.1.ln_2.bias', 'clip_teacher.transformer.resblocks.2.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.2.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.2.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.2.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.2.ln_1.weight', 'clip_teacher.transformer.resblocks.2.ln_1.bias', 'clip_teacher.transformer.resblocks.2.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.2.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.2.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.2.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.2.ln_2.weight', 'clip_teacher.transformer.resblocks.2.ln_2.bias', 'clip_teacher.transformer.resblocks.3.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.3.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.3.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.3.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.3.ln_1.weight', 'clip_teacher.transformer.resblocks.3.ln_1.bias', 'clip_teacher.transformer.resblocks.3.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.3.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.3.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.3.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.3.ln_2.weight', 'clip_teacher.transformer.resblocks.3.ln_2.bias', 'clip_teacher.transformer.resblocks.4.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.4.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.4.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.4.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.4.ln_1.weight', 'clip_teacher.transformer.resblocks.4.ln_1.bias', 'clip_teacher.transformer.resblocks.4.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.4.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.4.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.4.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.4.ln_2.weight', 'clip_teacher.transformer.resblocks.4.ln_2.bias', 'clip_teacher.transformer.resblocks.5.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.5.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.5.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.5.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.5.ln_1.weight', 'clip_teacher.transformer.resblocks.5.ln_1.bias', 'clip_teacher.transformer.resblocks.5.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.5.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.5.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.5.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.5.ln_2.weight', 'clip_teacher.transformer.resblocks.5.ln_2.bias', 'clip_teacher.transformer.resblocks.6.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.6.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.6.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.6.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.6.ln_1.weight', 'clip_teacher.transformer.resblocks.6.ln_1.bias', 'clip_teacher.transformer.resblocks.6.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.6.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.6.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.6.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.6.ln_2.weight', 'clip_teacher.transformer.resblocks.6.ln_2.bias', 'clip_teacher.transformer.resblocks.7.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.7.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.7.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.7.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.7.ln_1.weight', 'clip_teacher.transformer.resblocks.7.ln_1.bias', 'clip_teacher.transformer.resblocks.7.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.7.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.7.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.7.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.7.ln_2.weight', 'clip_teacher.transformer.resblocks.7.ln_2.bias', 'clip_teacher.transformer.resblocks.8.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.8.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.8.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.8.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.8.ln_1.weight', 'clip_teacher.transformer.resblocks.8.ln_1.bias', 'clip_teacher.transformer.resblocks.8.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.8.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.8.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.8.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.8.ln_2.weight', 'clip_teacher.transformer.resblocks.8.ln_2.bias', 'clip_teacher.transformer.resblocks.9.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.9.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.9.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.9.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.9.ln_1.weight', 'clip_teacher.transformer.resblocks.9.ln_1.bias', 'clip_teacher.transformer.resblocks.9.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.9.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.9.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.9.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.9.ln_2.weight', 'clip_teacher.transformer.resblocks.9.ln_2.bias', 'clip_teacher.transformer.resblocks.10.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.10.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.10.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.10.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.10.ln_1.weight', 'clip_teacher.transformer.resblocks.10.ln_1.bias', 'clip_teacher.transformer.resblocks.10.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.10.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.10.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.10.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.10.ln_2.weight', 'clip_teacher.transformer.resblocks.10.ln_2.bias', 'clip_teacher.transformer.resblocks.11.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.11.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.11.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.11.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.11.ln_1.weight', 'clip_teacher.transformer.resblocks.11.ln_1.bias', 'clip_teacher.transformer.resblocks.11.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.11.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.11.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.11.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.11.ln_2.weight', 'clip_teacher.transformer.resblocks.11.ln_2.bias', 'clip_teacher.ln_post.weight', 'clip_teacher.ln_post.bias', 'vision_encoder.clip_decoder.0.head.weight', 'vision_encoder.clip_decoder.0.head.bias', 'vision_encoder.clip_decoder.0.norm.weight', 'vision_encoder.clip_decoder.0.norm.bias', 'vision_encoder.clip_decoder.1.head.weight', 'vision_encoder.clip_decoder.1.head.bias', 'vision_encoder.clip_decoder.1.norm.weight', 'vision_encoder.clip_decoder.1.norm.bias', 'vision_encoder.clip_decoder.2.head.weight', 'vision_encoder.clip_decoder.2.head.bias', 'vision_encoder.clip_decoder.2.norm.weight', 'vision_encoder.clip_decoder.2.norm.bias', 'vision_encoder.clip_decoder.3.head.weight', 'vision_encoder.clip_decoder.3.head.bias', 'vision_encoder.clip_decoder.3.norm.weight', 'vision_encoder.clip_decoder.3.norm.bias', 'vision_encoder.clip_decoder.4.head.weight', 'vision_encoder.clip_decoder.4.head.bias', 'vision_encoder.clip_decoder.4.norm.weight', 'vision_encoder.clip_decoder.4.norm.bias', 'vision_encoder.clip_decoder.5.head.weight', 'vision_encoder.clip_decoder.5.head.bias', 'vision_encoder.clip_decoder.5.norm.weight', 'vision_encoder.clip_decoder.5.norm.bias'])
2024-05-11T13:52:56 | INFO | tasks.shared_utils : Loaded checkpoint from /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
2024-05-11T13:52:56 | INFO | __main__ : training
2024-05-11T13:52:57 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1448 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1448 
2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:58 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:52:58 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T13:53:13 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T13:53:13 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T13:53:18 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

2024-05-11T13:53:18 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

2024-05-11T13:53:18 | INFO | utils.basic_utils : Train Epoch: [0]  [   0/1448]  eta: 8:38:13  lr: 0.000000  temperature: 0.0112  video-loss_vtc: 1.8935  video-loss_mlm: 3.6273  video-loss_vtm: 0.3787  time: 21.4737  data: 15.8386  max mem: 33068 res mem: 39648
2024-05-11T14:00:23 | INFO | utils.basic_utils : Train Epoch: [0]  [ 100/1448]  eta: 1:39:18  lr: 0.000001  temperature: 0.0112  video-loss_vtc: 1.3150  video-loss_mlm: 3.0744  video-loss_vtm: 0.1753  time: 4.2564  data: 0.0016  max mem: 34623 res mem: 39872
2024-05-11T14:07:29 | INFO | utils.basic_utils : Train Epoch: [0]  [ 200/1448]  eta: 1:30:15  lr: 0.000001  temperature: 0.0113  video-loss_vtc: 1.1318  video-loss_mlm: 3.2043  video-loss_vtm: 0.2876  time: 4.2507  data: 0.0016  max mem: 34623 res mem: 39872
2024-05-11T14:14:34 | INFO | utils.basic_utils : Train Epoch: [0]  [ 300/1448]  eta: 1:22:30  lr: 0.000002  temperature: 0.0114  video-loss_vtc: 0.8712  video-loss_mlm: 3.1521  video-loss_vtm: 0.1072  time: 4.2542  data: 0.0015  max mem: 34623 res mem: 39872
2024-05-11T14:21:40 | INFO | utils.basic_utils : Train Epoch: [0]  [ 400/1448]  eta: 1:15:05  lr: 0.000003  temperature: 0.0115  video-loss_vtc: 0.9423  video-loss_mlm: 2.4382  video-loss_vtm: 0.2088  time: 4.2572  data: 0.0016  max mem: 34623 res mem: 39872
2024-05-11T14:28:46 | INFO | utils.basic_utils : Train Epoch: [0]  [ 500/1448]  eta: 1:07:48  lr: 0.000003  temperature: 0.0116  video-loss_vtc: 0.8923  video-loss_mlm: 2.9651  video-loss_vtm: 0.1771  time: 4.2634  data: 0.0015  max mem: 34623 res mem: 39872
2024-05-11T14:35:52 | INFO | utils.basic_utils : Train Epoch: [0]  [ 600/1448]  eta: 1:00:33  lr: 0.000004  temperature: 0.0117  video-loss_vtc: 0.9711  video-loss_mlm: 2.8300  video-loss_vtm: 0.0970  time: 4.2511  data: 0.0016  max mem: 34623 res mem: 39872
2024-05-11T14:42:57 | INFO | utils.basic_utils : Train Epoch: [0]  [ 700/1448]  eta: 0:53:22  lr: 0.000005  temperature: 0.0118  video-loss_vtc: 0.9568  video-loss_mlm: 3.0371  video-loss_vtm: 0.1023  time: 4.2453  data: 0.0016  max mem: 34623 res mem: 39872
2024-05-11T14:50:03 | INFO | utils.basic_utils : Train Epoch: [0]  [ 800/1448]  eta: 0:46:12  lr: 0.000006  temperature: 0.0119  video-loss_vtc: 0.8403  video-loss_mlm: 3.0599  video-loss_vtm: 0.2148  time: 4.2632  data: 0.0017  max mem: 34623 res mem: 39872
2024-05-11T14:57:09 | INFO | utils.basic_utils : Train Epoch: [0]  [ 900/1448]  eta: 0:39:02  lr: 0.000006  temperature: 0.0120  video-loss_vtc: 0.8453  video-loss_mlm: 3.1974  video-loss_vtm: 0.0997  time: 4.2539  data: 0.0017  max mem: 34623 res mem: 39872
2024-05-11T15:04:14 | INFO | utils.basic_utils : Train Epoch: [0]  [1000/1448]  eta: 0:31:54  lr: 0.000007  temperature: 0.0121  video-loss_vtc: 0.8338  video-loss_mlm: 3.1006  video-loss_vtm: 0.1073  time: 4.2492  data: 0.0016  max mem: 34623 res mem: 39872
2024-05-11T15:11:20 | INFO | utils.basic_utils : Train Epoch: [0]  [1100/1448]  eta: 0:24:46  lr: 0.000008  temperature: 0.0122  video-loss_vtc: 0.9153  video-loss_mlm: 2.7731  video-loss_vtm: 0.0582  time: 4.2544  data: 0.0015  max mem: 34623 res mem: 39872
2024-05-11T15:18:26 | INFO | utils.basic_utils : Train Epoch: [0]  [1200/1448]  eta: 0:17:39  lr: 0.000008  temperature: 0.0123  video-loss_vtc: 0.7586  video-loss_mlm: 2.8477  video-loss_vtm: 0.0683  time: 4.2640  data: 0.0015  max mem: 34623 res mem: 39872
2024-05-11T15:25:33 | INFO | utils.basic_utils : Train Epoch: [0]  [1300/1448]  eta: 0:10:32  lr: 0.000009  temperature: 0.0124  video-loss_vtc: 0.8562  video-loss_mlm: 2.4034  video-loss_vtm: 0.0713  time: 4.2547  data: 0.0015  max mem: 34623 res mem: 39872
2024-05-11T15:32:39 | INFO | utils.basic_utils : Train Epoch: [0]  [1400/1448]  eta: 0:03:24  lr: 0.000010  temperature: 0.0125  video-loss_vtc: 0.8453  video-loss_mlm: 2.6845  video-loss_vtm: 0.1576  time: 4.2670  data: 0.0016  max mem: 34623 res mem: 39872
2024-05-11T15:35:59 | INFO | utils.basic_utils : Train Epoch: [0]  [1447/1448]  eta: 0:00:04  lr: 0.000010  temperature: 0.0125  video-loss_vtc: 0.7860  video-loss_mlm: 2.7302  video-loss_vtm: 0.0898  time: 4.2480  data: 0.0015  max mem: 34623 res mem: 39872
2024-05-11T15:35:59 | INFO | utils.basic_utils : Train Epoch: [0] Total time: 1:43:02 (4.2695 s / it)
2024-05-11T15:35:59 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0118  video-loss_vtc: 0.9698  video-loss_mlm: 2.9539  video-loss_vtm: 0.1590
2024-05-11T15:35:59 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-11T15:35:59 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-11T15:36:00 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:00 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:20 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:20 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T15:36:21 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:22:21    time: 21.2978  data: 20.1729  max mem: 34623 res mem: 39872
2024-05-11T15:38:58 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3226  data: 1.2221  max mem: 34623 res mem: 39872
2024-05-11T15:38:58 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:58 (2.8303 s / it)
2024-05-11T15:39:18 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-11T15:39:18 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-11T15:39:18 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-11T15:39:18 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-11T15:39:18 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-11T15:39:18 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-11T15:39:18 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:30    time: 0.0614  data: 0.0027  max mem: 34623 res mem: 39872
2024-05-11T15:39:25 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0736  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:39:33 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0744  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:39:40 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0744  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:39:47 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0748  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:39:55 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0749  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:39:55 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0742 s / it)
2024-05-11T15:39:55 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-11T15:39:56 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:12:34    time: 1.5064  data: 0.0008  max mem: 34623 res mem: 39872
2024-05-11T15:41:42 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:07:05    time: 1.0090  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:43:21 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:05:08    time: 0.9942  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:45:17 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:03:34    time: 1.2619  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:47:16 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:51    time: 1.1581  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:49:00 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:01    time: 0.9351  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-11T15:49:00 | INFO | utils.basic_utils : Evaluation: Total time: 0:09:05 (1.0878 s / it)
2024-05-11T15:49:00 | INFO | tasks.retrieval_utils : Evaluation time 0:13:01
2024-05-11T15:49:02 | INFO | __main__ : Epoch 0
2024-05-11T15:49:02 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       44.40   94.30    97.85       78.85   43.50   94.45    97.80       78.58   78.72    51.35     51.5
test_emb/   40.95   89.65    95.70       75.43   40.85   90.30    95.15       75.43   75.43    51.55     51.7
2024-05-11T15:49:05 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1448 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1448 
2024-05-11T15:49:19 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T15:49:19 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T15:49:25 | INFO | utils.basic_utils : Train Epoch: [1]  [   0/1448]  eta: 8:00:38  lr: 0.000010  temperature: 0.0125  video-loss_vtc: 0.7688  video-loss_mlm: 2.7441  video-loss_vtm: 0.0865  time: 19.9161  data: 14.2134  max mem: 34623 res mem: 41898
2024-05-11T15:56:31 | INFO | utils.basic_utils : Train Epoch: [1]  [ 100/1448]  eta: 1:39:14  lr: 0.000010  temperature: 0.0126  video-loss_vtc: 0.7134  video-loss_mlm: 2.7485  video-loss_vtm: 0.0590  time: 4.2661  data: 0.0016  max mem: 34623 res mem: 41898
2024-05-11T16:03:36 | INFO | utils.basic_utils : Train Epoch: [1]  [ 200/1448]  eta: 1:30:10  lr: 0.000010  temperature: 0.0128  video-loss_vtc: 0.6865  video-loss_mlm: 2.3648  video-loss_vtm: 0.0983  time: 4.2505  data: 0.0015  max mem: 34623 res mem: 41898
2024-05-11T16:10:41 | INFO | utils.basic_utils : Train Epoch: [1]  [ 300/1448]  eta: 1:22:24  lr: 0.000010  temperature: 0.0129  video-loss_vtc: 0.6632  video-loss_mlm: 2.5809  video-loss_vtm: 0.0618  time: 4.2475  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T16:17:46 | INFO | utils.basic_utils : Train Epoch: [1]  [ 400/1448]  eta: 1:14:59  lr: 0.000010  temperature: 0.0130  video-loss_vtc: 0.6834  video-loss_mlm: 2.4031  video-loss_vtm: 0.0605  time: 4.2465  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T16:24:52 | INFO | utils.basic_utils : Train Epoch: [1]  [ 500/1448]  eta: 1:07:42  lr: 0.000010  temperature: 0.0131  video-loss_vtc: 0.6993  video-loss_mlm: 2.4791  video-loss_vtm: 0.0996  time: 4.2459  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T16:31:56 | INFO | utils.basic_utils : Train Epoch: [1]  [ 600/1448]  eta: 1:00:28  lr: 0.000010  temperature: 0.0131  video-loss_vtc: 0.8108  video-loss_mlm: 2.3836  video-loss_vtm: 0.0881  time: 4.2555  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T16:39:01 | INFO | utils.basic_utils : Train Epoch: [1]  [ 700/1448]  eta: 0:53:17  lr: 0.000010  temperature: 0.0132  video-loss_vtc: 0.7638  video-loss_mlm: 2.6410  video-loss_vtm: 0.0559  time: 4.2615  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T16:46:07 | INFO | utils.basic_utils : Train Epoch: [1]  [ 800/1448]  eta: 0:46:08  lr: 0.000010  temperature: 0.0133  video-loss_vtc: 0.7738  video-loss_mlm: 3.1518  video-loss_vtm: 0.1855  time: 4.2562  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T16:53:13 | INFO | utils.basic_utils : Train Epoch: [1]  [ 900/1448]  eta: 0:39:00  lr: 0.000009  temperature: 0.0134  video-loss_vtc: 0.7374  video-loss_mlm: 2.7029  video-loss_vtm: 0.0885  time: 4.2637  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T17:00:20 | INFO | utils.basic_utils : Train Epoch: [1]  [1000/1448]  eta: 0:31:53  lr: 0.000009  temperature: 0.0135  video-loss_vtc: 0.6575  video-loss_mlm: 2.6717  video-loss_vtm: 0.0428  time: 4.2672  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T17:07:28 | INFO | utils.basic_utils : Train Epoch: [1]  [1100/1448]  eta: 0:24:46  lr: 0.000009  temperature: 0.0135  video-loss_vtc: 0.8108  video-loss_mlm: 2.8516  video-loss_vtm: 0.0798  time: 4.2806  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T17:14:37 | INFO | utils.basic_utils : Train Epoch: [1]  [1200/1448]  eta: 0:17:39  lr: 0.000009  temperature: 0.0135  video-loss_vtc: 0.6239  video-loss_mlm: 2.6022  video-loss_vtm: 0.0588  time: 4.3062  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T17:21:45 | INFO | utils.basic_utils : Train Epoch: [1]  [1300/1448]  eta: 0:10:32  lr: 0.000009  temperature: 0.0136  video-loss_vtc: 0.7065  video-loss_mlm: 2.3608  video-loss_vtm: 0.0654  time: 4.2684  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T17:28:52 | INFO | utils.basic_utils : Train Epoch: [1]  [1400/1448]  eta: 0:03:25  lr: 0.000009  temperature: 0.0137  video-loss_vtc: 0.8052  video-loss_mlm: 2.3468  video-loss_vtm: 0.1556  time: 4.2784  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T17:32:12 | INFO | utils.basic_utils : Train Epoch: [1]  [1447/1448]  eta: 0:00:04  lr: 0.000009  temperature: 0.0137  video-loss_vtc: 0.6865  video-loss_mlm: 2.5276  video-loss_vtm: 0.0314  time: 4.2509  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T17:32:12 | INFO | utils.basic_utils : Train Epoch: [1] Total time: 1:43:06 (4.2727 s / it)
2024-05-11T17:32:12 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0132  video-loss_vtc: 0.7315  video-loss_mlm: 2.6261  video-loss_vtm: 0.0872
2024-05-11T17:32:12 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-11T17:32:12 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-11T17:32:28 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T17:32:28 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T17:32:29 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:17:07    time: 16.3058  data: 15.1379  max mem: 34625 res mem: 41898
2024-05-11T17:35:06 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3079  data: 1.2300  max mem: 34625 res mem: 41898
2024-05-11T17:35:06 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:53 (2.7511 s / it)
2024-05-11T17:35:20 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-11T17:35:20 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-11T17:35:20 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-11T17:35:20 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-11T17:35:20 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-11T17:35:20 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-11T17:35:20 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:13    time: 0.0279  data: 0.0014  max mem: 34625 res mem: 41898
2024-05-11T17:35:28 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0741  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:35:35 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:35:42 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0744  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:35:50 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:35:57 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:35:57 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0742 s / it)
2024-05-11T17:35:57 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-11T17:35:58 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:08:34    time: 1.0278  data: 0.0007  max mem: 34625 res mem: 41898
2024-05-11T17:37:37 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:06:34    time: 0.9609  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:39:19 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:05:02    time: 1.0241  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:41:02 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:03:23    time: 1.0222  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:42:42 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:41    time: 0.9976  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:44:20 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:01    time: 0.9369  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T17:44:20 | INFO | utils.basic_utils : Evaluation: Total time: 0:08:22 (1.0023 s / it)
2024-05-11T17:44:20 | INFO | tasks.retrieval_utils : Evaluation time 0:12:08
2024-05-11T17:44:22 | INFO | __main__ : Epoch 1
2024-05-11T17:44:22 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       45.40   95.05    98.25       79.57   44.85   95.30    98.20       79.45   79.51    51.30     52.8
test_emb/   41.05   91.80    97.00       76.62   43.60   91.55    96.25       77.13   76.88    52.25     53.8
2024-05-11T17:44:38 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1448 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1448 
2024-05-11T17:44:53 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T17:44:53 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T17:44:57 | INFO | utils.basic_utils : Train Epoch: [2]  [   0/1448]  eta: 7:49:05  lr: 0.000009  temperature: 0.0137  video-loss_vtc: 0.6946  video-loss_mlm: 2.5688  video-loss_vtm: 0.0365  time: 19.4372  data: 15.1094  max mem: 34625 res mem: 41898
2024-05-11T17:52:04 | INFO | utils.basic_utils : Train Epoch: [2]  [ 100/1448]  eta: 1:39:06  lr: 0.000008  temperature: 0.0138  video-loss_vtc: 0.6339  video-loss_mlm: 2.5984  video-loss_vtm: 0.0303  time: 4.2681  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T17:59:10 | INFO | utils.basic_utils : Train Epoch: [2]  [ 200/1448]  eta: 1:30:14  lr: 0.000008  temperature: 0.0138  video-loss_vtc: 0.6185  video-loss_mlm: 2.6172  video-loss_vtm: 0.0496  time: 4.2602  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T18:06:16 | INFO | utils.basic_utils : Train Epoch: [2]  [ 300/1448]  eta: 1:22:31  lr: 0.000008  temperature: 0.0138  video-loss_vtc: 0.6196  video-loss_mlm: 2.2494  video-loss_vtm: 0.0287  time: 4.2638  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T18:13:22 | INFO | utils.basic_utils : Train Epoch: [2]  [ 400/1448]  eta: 1:15:05  lr: 0.000008  temperature: 0.0138  video-loss_vtc: 0.6111  video-loss_mlm: 2.0545  video-loss_vtm: 0.0398  time: 4.2512  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T18:20:27 | INFO | utils.basic_utils : Train Epoch: [2]  [ 500/1448]  eta: 1:07:46  lr: 0.000007  temperature: 0.0138  video-loss_vtc: 0.5428  video-loss_mlm: 2.4343  video-loss_vtm: 0.0238  time: 4.2506  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T18:27:33 | INFO | utils.basic_utils : Train Epoch: [2]  [ 600/1448]  eta: 1:00:32  lr: 0.000007  temperature: 0.0138  video-loss_vtc: 0.7151  video-loss_mlm: 2.2290  video-loss_vtm: 0.0450  time: 4.2630  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T18:34:38 | INFO | utils.basic_utils : Train Epoch: [2]  [ 700/1448]  eta: 0:53:21  lr: 0.000007  temperature: 0.0139  video-loss_vtc: 0.6542  video-loss_mlm: 2.4364  video-loss_vtm: 0.0226  time: 4.2612  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T18:41:44 | INFO | utils.basic_utils : Train Epoch: [2]  [ 800/1448]  eta: 0:46:11  lr: 0.000007  temperature: 0.0139  video-loss_vtc: 0.6791  video-loss_mlm: 2.6029  video-loss_vtm: 0.0356  time: 4.2568  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T18:48:49 | INFO | utils.basic_utils : Train Epoch: [2]  [ 900/1448]  eta: 0:39:02  lr: 0.000006  temperature: 0.0140  video-loss_vtc: 0.6876  video-loss_mlm: 2.7964  video-loss_vtm: 0.0495  time: 4.2540  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T18:55:53 | INFO | utils.basic_utils : Train Epoch: [2]  [1000/1448]  eta: 0:31:53  lr: 0.000006  temperature: 0.0140  video-loss_vtc: 0.6171  video-loss_mlm: 2.6664  video-loss_vtm: 0.0222  time: 4.2543  data: 0.0017  max mem: 34625 res mem: 41898
2024-05-11T19:02:58 | INFO | utils.basic_utils : Train Epoch: [2]  [1100/1448]  eta: 0:24:45  lr: 0.000006  temperature: 0.0140  video-loss_vtc: 0.7211  video-loss_mlm: 2.5228  video-loss_vtm: 0.0124  time: 4.2559  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T19:10:03 | INFO | utils.basic_utils : Train Epoch: [2]  [1200/1448]  eta: 0:17:38  lr: 0.000006  temperature: 0.0140  video-loss_vtc: 0.6293  video-loss_mlm: 2.5633  video-loss_vtm: 0.0142  time: 4.2514  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T19:17:08 | INFO | utils.basic_utils : Train Epoch: [2]  [1300/1448]  eta: 0:10:31  lr: 0.000005  temperature: 0.0140  video-loss_vtc: 0.6848  video-loss_mlm: 2.2098  video-loss_vtm: 0.0701  time: 4.2514  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T19:24:13 | INFO | utils.basic_utils : Train Epoch: [2]  [1400/1448]  eta: 0:03:24  lr: 0.000005  temperature: 0.0140  video-loss_vtc: 0.6439  video-loss_mlm: 2.4126  video-loss_vtm: 0.0964  time: 4.2543  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T19:27:33 | INFO | utils.basic_utils : Train Epoch: [2]  [1447/1448]  eta: 0:00:04  lr: 0.000005  temperature: 0.0140  video-loss_vtc: 0.6897  video-loss_mlm: 2.3941  video-loss_vtm: 0.0207  time: 4.2394  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T19:27:33 | INFO | utils.basic_utils : Train Epoch: [2] Total time: 1:42:54 (4.2645 s / it)
2024-05-11T19:27:33 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0139  video-loss_vtc: 0.6663  video-loss_mlm: 2.5005  video-loss_vtm: 0.0594
2024-05-11T19:27:33 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-11T19:27:33 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-11T19:27:49 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T19:27:49 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T19:27:50 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:17:02    time: 16.2329  data: 15.1249  max mem: 34625 res mem: 41898
2024-05-11T19:30:26 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3188  data: 1.2481  max mem: 34625 res mem: 41898
2024-05-11T19:30:26 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:52 (2.7305 s / it)
2024-05-11T19:30:40 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-11T19:30:40 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-11T19:30:41 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-11T19:30:41 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-11T19:30:41 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-11T19:30:41 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-11T19:30:41 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:14    time: 0.0288  data: 0.0013  max mem: 34625 res mem: 41898
2024-05-11T19:30:48 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0740  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:30:55 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:31:03 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:31:10 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:31:18 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0753  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:31:18 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0746 s / it)
2024-05-11T19:31:18 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-11T19:31:19 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:09:44    time: 1.1661  data: 0.0008  max mem: 34625 res mem: 41898
2024-05-11T19:32:58 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:06:38    time: 0.9888  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:34:36 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:56    time: 0.9792  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:36:14 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:03:17    time: 0.9696  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:37:51 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:38    time: 0.9644  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:39:28 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.9698  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T19:39:28 | INFO | utils.basic_utils : Evaluation: Total time: 0:08:10 (0.9789 s / it)
2024-05-11T19:39:28 | INFO | tasks.retrieval_utils : Evaluation time 0:11:55
2024-05-11T19:39:30 | INFO | __main__ : Epoch 2
2024-05-11T19:39:30 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        45.3    95.0    98.15       79.48   46.25   95.55    98.35       80.05   79.77    51.15    53.80
test_emb/    42.7    92.1    96.70       77.17   44.75   91.90    96.35       77.67   77.42    53.50    56.05
2024-05-11T19:39:46 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1448 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1448 
2024-05-11T19:40:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T19:40:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T19:40:05 | INFO | utils.basic_utils : Train Epoch: [3]  [   0/1448]  eta: 7:52:27  lr: 0.000005  temperature: 0.0140  video-loss_vtc: 0.6942  video-loss_mlm: 2.3803  video-loss_vtm: 0.0156  time: 19.5773  data: 15.1336  max mem: 34625 res mem: 41898
2024-05-11T19:47:11 | INFO | utils.basic_utils : Train Epoch: [3]  [ 100/1448]  eta: 1:38:57  lr: 0.000005  temperature: 0.0141  video-loss_vtc: 0.5704  video-loss_mlm: 2.5821  video-loss_vtm: 0.0169  time: 4.2557  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T19:54:17 | INFO | utils.basic_utils : Train Epoch: [3]  [ 200/1448]  eta: 1:30:10  lr: 0.000004  temperature: 0.0141  video-loss_vtc: 0.6844  video-loss_mlm: 2.3110  video-loss_vtm: 0.0335  time: 4.2732  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T20:01:23 | INFO | utils.basic_utils : Train Epoch: [3]  [ 300/1448]  eta: 1:22:28  lr: 0.000004  temperature: 0.0141  video-loss_vtc: 0.5626  video-loss_mlm: 2.5081  video-loss_vtm: 0.0098  time: 4.2667  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T20:08:30 | INFO | utils.basic_utils : Train Epoch: [3]  [ 400/1448]  eta: 1:15:05  lr: 0.000004  temperature: 0.0141  video-loss_vtc: 0.6102  video-loss_mlm: 1.7725  video-loss_vtm: 0.0145  time: 4.2658  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T20:15:36 | INFO | utils.basic_utils : Train Epoch: [3]  [ 500/1448]  eta: 1:07:48  lr: 0.000004  temperature: 0.0141  video-loss_vtc: 0.5370  video-loss_mlm: 2.8851  video-loss_vtm: 0.0306  time: 4.2561  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T20:22:41 | INFO | utils.basic_utils : Train Epoch: [3]  [ 600/1448]  eta: 1:00:34  lr: 0.000003  temperature: 0.0141  video-loss_vtc: 0.6585  video-loss_mlm: 2.1457  video-loss_vtm: 0.0218  time: 4.2545  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T20:29:47 | INFO | utils.basic_utils : Train Epoch: [3]  [ 700/1448]  eta: 0:53:22  lr: 0.000003  temperature: 0.0141  video-loss_vtc: 0.6389  video-loss_mlm: 2.3444  video-loss_vtm: 0.0290  time: 4.2626  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T20:36:53 | INFO | utils.basic_utils : Train Epoch: [3]  [ 800/1448]  eta: 0:46:12  lr: 0.000003  temperature: 0.0141  video-loss_vtc: 0.6089  video-loss_mlm: 2.6644  video-loss_vtm: 0.0214  time: 4.2582  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T20:44:00 | INFO | utils.basic_utils : Train Epoch: [3]  [ 900/1448]  eta: 0:39:03  lr: 0.000003  temperature: 0.0142  video-loss_vtc: 0.6922  video-loss_mlm: 2.3188  video-loss_vtm: 0.0648  time: 4.2554  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T20:51:06 | INFO | utils.basic_utils : Train Epoch: [3]  [1000/1448]  eta: 0:31:55  lr: 0.000002  temperature: 0.0142  video-loss_vtc: 0.5894  video-loss_mlm: 2.2821  video-loss_vtm: 0.0151  time: 4.2554  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T20:58:12 | INFO | utils.basic_utils : Train Epoch: [3]  [1100/1448]  eta: 0:24:47  lr: 0.000002  temperature: 0.0142  video-loss_vtc: 0.7450  video-loss_mlm: 2.9575  video-loss_vtm: 0.0137  time: 4.2644  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T21:05:18 | INFO | utils.basic_utils : Train Epoch: [3]  [1200/1448]  eta: 0:17:39  lr: 0.000002  temperature: 0.0141  video-loss_vtc: 0.5528  video-loss_mlm: 2.4686  video-loss_vtm: 0.0085  time: 4.2666  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T21:12:23 | INFO | utils.basic_utils : Train Epoch: [3]  [1300/1448]  eta: 0:10:32  lr: 0.000002  temperature: 0.0141  video-loss_vtc: 0.6572  video-loss_mlm: 2.2860  video-loss_vtm: 0.0701  time: 4.2516  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T21:19:29 | INFO | utils.basic_utils : Train Epoch: [3]  [1400/1448]  eta: 0:03:25  lr: 0.000002  temperature: 0.0141  video-loss_vtc: 0.6474  video-loss_mlm: 2.3612  video-loss_vtm: 0.1780  time: 4.2759  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T21:22:50 | INFO | utils.basic_utils : Train Epoch: [3]  [1447/1448]  eta: 0:00:04  lr: 0.000001  temperature: 0.0142  video-loss_vtc: 0.6482  video-loss_mlm: 2.6690  video-loss_vtm: 0.0125  time: 4.2595  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T21:22:50 | INFO | utils.basic_utils : Train Epoch: [3] Total time: 1:43:03 (4.2706 s / it)
2024-05-11T21:22:50 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0141  video-loss_vtc: 0.6345  video-loss_mlm: 2.4388  video-loss_vtm: 0.0453
2024-05-11T21:22:50 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-11T21:22:50 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-11T21:23:06 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T21:23:06 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T21:23:07 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:17:04    time: 16.2610  data: 15.0767  max mem: 34625 res mem: 41898
2024-05-11T21:25:43 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3076  data: 1.2383  max mem: 34625 res mem: 41898
2024-05-11T21:25:43 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:51 (2.7297 s / it)
2024-05-11T21:25:58 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-11T21:25:58 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-11T21:25:58 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-11T21:25:58 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-11T21:25:58 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-11T21:25:58 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-11T21:25:58 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:14    time: 0.0280  data: 0.0013  max mem: 34625 res mem: 41898
2024-05-11T21:26:06 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0742  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:26:13 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:26:21 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:26:28 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:26:36 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0749  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:26:36 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0743 s / it)
2024-05-11T21:26:36 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-11T21:26:37 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:08:50    time: 1.0581  data: 0.0008  max mem: 34625 res mem: 41898
2024-05-11T21:28:13 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:06:28    time: 0.9480  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:29:50 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:50    time: 0.9615  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:31:27 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:03:14    time: 0.9795  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:33:04 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:37    time: 0.9740  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:34:41 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.9657  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T21:34:41 | INFO | utils.basic_utils : Evaluation: Total time: 0:08:05 (0.9689 s / it)
2024-05-11T21:34:50 | INFO | tasks.retrieval_utils : Evaluation time 0:12:00
2024-05-11T21:34:53 | INFO | __main__ : Epoch 3
2024-05-11T21:34:53 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       45.65    95.2     98.1       79.65   48.05    95.8    98.35       80.73   80.19    51.45     54.8
test_emb/   43.70    91.9     96.9       77.50   44.95    92.1    96.35       77.80   77.65    54.60     56.7
2024-05-11T21:35:07 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1448 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1448 
2024-05-11T21:35:22 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T21:35:22 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T21:35:27 | INFO | utils.basic_utils : Train Epoch: [4]  [   0/1448]  eta: 8:12:20  lr: 0.000001  temperature: 0.0142  video-loss_vtc: 0.6804  video-loss_mlm: 2.5414  video-loss_vtm: 0.0064  time: 20.4012  data: 14.7124  max mem: 34625 res mem: 41898
2024-05-11T21:42:32 | INFO | utils.basic_utils : Train Epoch: [4]  [ 100/1448]  eta: 1:39:04  lr: 0.000001  temperature: 0.0142  video-loss_vtc: 0.5919  video-loss_mlm: 2.2613  video-loss_vtm: 0.0138  time: 4.2680  data: 0.0017  max mem: 34625 res mem: 41898
2024-05-11T21:49:39 | INFO | utils.basic_utils : Train Epoch: [4]  [ 200/1448]  eta: 1:30:12  lr: 0.000001  temperature: 0.0142  video-loss_vtc: 0.5696  video-loss_mlm: 2.1254  video-loss_vtm: 0.0400  time: 4.2611  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T21:56:45 | INFO | utils.basic_utils : Train Epoch: [4]  [ 300/1448]  eta: 1:22:30  lr: 0.000001  temperature: 0.0142  video-loss_vtc: 0.5876  video-loss_mlm: 2.2786  video-loss_vtm: 0.0156  time: 4.2598  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T22:03:51 | INFO | utils.basic_utils : Train Epoch: [4]  [ 400/1448]  eta: 1:15:04  lr: 0.000001  temperature: 0.0142  video-loss_vtc: 0.6098  video-loss_mlm: 2.2231  video-loss_vtm: 0.0150  time: 4.2504  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T22:10:56 | INFO | utils.basic_utils : Train Epoch: [4]  [ 500/1448]  eta: 1:07:45  lr: 0.000001  temperature: 0.0142  video-loss_vtc: 0.5174  video-loss_mlm: 2.4741  video-loss_vtm: 0.0117  time: 4.2484  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T22:18:01 | INFO | utils.basic_utils : Train Epoch: [4]  [ 600/1448]  eta: 1:00:31  lr: 0.000001  temperature: 0.0142  video-loss_vtc: 0.6690  video-loss_mlm: 2.2255  video-loss_vtm: 0.0130  time: 4.2562  data: 0.0015  max mem: 34625 res mem: 41898
2024-05-11T22:25:06 | INFO | utils.basic_utils : Train Epoch: [4]  [ 700/1448]  eta: 0:53:20  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.6331  video-loss_mlm: 2.1749  video-loss_vtm: 0.0105  time: 4.2540  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T22:32:12 | INFO | utils.basic_utils : Train Epoch: [4]  [ 800/1448]  eta: 0:46:10  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.6232  video-loss_mlm: 2.5320  video-loss_vtm: 0.0220  time: 4.2697  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T22:39:19 | INFO | utils.basic_utils : Train Epoch: [4]  [ 900/1448]  eta: 0:39:02  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.6864  video-loss_mlm: 2.1905  video-loss_vtm: 0.1075  time: 4.2662  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T22:46:25 | INFO | utils.basic_utils : Train Epoch: [4]  [1000/1448]  eta: 0:31:54  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.6164  video-loss_mlm: 2.4385  video-loss_vtm: 0.0081  time: 4.2657  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T22:53:31 | INFO | utils.basic_utils : Train Epoch: [4]  [1100/1448]  eta: 0:24:46  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.6911  video-loss_mlm: 2.5506  video-loss_vtm: 0.0123  time: 4.2683  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T23:00:37 | INFO | utils.basic_utils : Train Epoch: [4]  [1200/1448]  eta: 0:17:39  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.5922  video-loss_mlm: 2.5186  video-loss_vtm: 0.0402  time: 4.2642  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T23:07:43 | INFO | utils.basic_utils : Train Epoch: [4]  [1300/1448]  eta: 0:10:32  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.6770  video-loss_mlm: 2.3170  video-loss_vtm: 0.0689  time: 4.2662  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T23:14:49 | INFO | utils.basic_utils : Train Epoch: [4]  [1400/1448]  eta: 0:03:24  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.6405  video-loss_mlm: 2.2938  video-loss_vtm: 0.1519  time: 4.2653  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T23:18:09 | INFO | utils.basic_utils : Train Epoch: [4]  [1447/1448]  eta: 0:00:04  lr: 0.000000  temperature: 0.0142  video-loss_vtc: 0.6727  video-loss_mlm: 2.1942  video-loss_vtm: 0.0312  time: 4.2433  data: 0.0016  max mem: 34625 res mem: 41898
2024-05-11T23:18:09 | INFO | utils.basic_utils : Train Epoch: [4] Total time: 1:43:01 (4.2690 s / it)
2024-05-11T23:18:09 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0142  video-loss_vtc: 0.6230  video-loss_mlm: 2.4213  video-loss_vtm: 0.0394
2024-05-11T23:18:09 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-11T23:18:09 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-11T23:18:25 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T23:18:25 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T23:18:26 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:17:16    time: 16.4453  data: 15.3200  max mem: 34625 res mem: 41898
2024-05-11T23:21:02 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3380  data: 1.2512  max mem: 34625 res mem: 41898
2024-05-11T23:21:02 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:52 (2.7419 s / it)
2024-05-11T23:21:16 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-11T23:21:16 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-11T23:21:16 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-11T23:21:16 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-11T23:21:16 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-11T23:21:16 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-11T23:21:16 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:14    time: 0.0289  data: 0.0012  max mem: 34625 res mem: 41898
2024-05-11T23:21:24 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0739  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:21:31 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0742  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:21:39 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0745  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:21:46 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0742  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:21:53 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0743  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:21:53 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0741 s / it)
2024-05-11T23:21:54 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-11T23:21:55 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:08:35    time: 1.0284  data: 0.0007  max mem: 34625 res mem: 41898
2024-05-11T23:23:36 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:06:48    time: 1.0122  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:25:18 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:05:06    time: 1.0174  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:27:00 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:03:24    time: 1.0232  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:28:42 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:42    time: 1.0164  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:30:19 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:01    time: 0.9407  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:30:19 | INFO | utils.basic_utils : Evaluation: Total time: 0:08:25 (1.0082 s / it)
2024-05-11T23:30:20 | INFO | tasks.retrieval_utils : Evaluation time 0:12:11
2024-05-11T23:30:22 | INFO | __main__ : Epoch 4
2024-05-11T23:30:22 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       45.75   95.45    98.20        79.8    48.4    95.9     98.3       80.87   80.33     51.6    55.05
test_emb/   42.75   92.00    96.85        77.2    45.1    92.4     96.5       78.00   77.60     54.3    56.55
2024-05-11T23:30:38 | INFO | __main__ : Training time 9:37:41
2024-05-11T23:30:38 | INFO | __main__ : best epoch 4 [config.stop_key test/]
2024-05-11T23:30:38 | INFO | __main__ : Checkpoints and Logs saved at exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM
2024-05-11T23:30:39 | INFO | __main__ : ===========> START eval_after_training [['test']]
2024-05-11T23:30:39 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_rewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video'], 'test_file': {'test': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': True, 'num_frames': 12, 'num_frames_test': 12, 'batch_size': 32, 'max_txt_l': 32, 'evaluate': True, 'train_shuffle': False, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 12, 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 32, 'video': 32}, 'batch_size': {'image': 32, 'video': 32}, 'batch_size_test': {'image': 32, 'video': 32}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.2, 'num_frames': 12, 'tubelet_size': 1, 'use_checkpoint': True, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 1.0, 'vtm': 1.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 5, 'min_lr_multi': 0.01, 'warmup_epochs': 1, 'num_training_steps': 7240, 'num_warmup_steps': 1448}, 'zero_shot': False, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM/eval_after_training', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 42, 'save_latest': True, 'auto_resume': True, 'pretrained_path': 'exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM/ckpt_best.pth', 'rank': 0, 'world_size': 4, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl', 'result_dir': 'exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM/eval_after_training'}
2024-05-11T23:30:39 | INFO | __main__ : train_file: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_rewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video']
2024-05-11T23:30:39 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-11T23:30:39 | INFO | tasks.shared_utils : Creating model
2024-05-11T23:30:49 | INFO | models.umt : Build vision_encoder: vit_b16
2024-05-11T23:30:49 | INFO | models.backbones.vit.vit : Num of patches: 2352
2024-05-11T23:30:49 | INFO | models.backbones.vit.vit : Use checkpoint: True
2024-05-11T23:30:49 | INFO | models.backbones.vit.vit : Checkpoint number: 12
2024-05-11T23:30:49 | INFO | models.backbones.vit.vit : Student return index: []
2024-05-11T23:30:57 | INFO | models.backbones.vit.vit : Loading pretrained weights from /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
2024-05-11T23:30:58 | INFO | models.umt : Build text_encoder bert_base
2024-05-11T23:30:59 | INFO | models.backbones.bert.xbert : build bert with cross_module: ca
2024-05-11T23:31:00 | INFO | models.criterions : Norm type: l2
2024-05-11T23:31:00 | INFO | models.criterions : Loss type: l2
2024-05-11T23:31:00 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.temp: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.word_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.position_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.token_type_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.dense.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.dense.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.vision_proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_proj.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.text_proj.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.itm_head.weight: wd: 0.02, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : param module.itm_head.bias: wd: 0, lr: 1e-05
2024-05-11T23:31:00 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0.02 len(p)=141
2024-05-11T23:31:00 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0 len(p)=260
2024-05-11T23:31:00 | INFO | tasks.shared_utils : Auto resuming
2024-05-11T23:31:00 | INFO | tasks.shared_utils : Not found checkpoint in exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM/eval_after_training
2024-05-11T23:31:02 | INFO | tasks.shared_utils : <All keys matched successfully>
2024-05-11T23:31:02 | INFO | tasks.shared_utils : Loaded checkpoint from exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM/ckpt_best.pth
2024-05-11T23:31:02 | INFO | __main__ : Start evaluation
2024-05-11T23:31:02 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-11T23:31:02 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-11T23:31:03 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:03 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:03 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:03 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:04 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:04 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:04 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:04 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:04 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:04 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:04 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:04 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:19 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:19 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:19 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:19 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-11T23:31:20 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:18:22    time: 17.5047  data: 16.3696  max mem: 34625 res mem: 41898
2024-05-11T23:33:55 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3055  data: 1.2379  max mem: 34625 res mem: 41898
2024-05-11T23:33:55 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:51 (2.7301 s / it)
2024-05-11T23:34:08 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-11T23:34:08 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-11T23:34:08 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-11T23:34:08 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-11T23:34:08 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-11T23:34:08 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-11T23:34:08 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:15    time: 0.0302  data: 0.0011  max mem: 34625 res mem: 41898
2024-05-11T23:34:15 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0737  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:34:22 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0734  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:34:30 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0738  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:34:37 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0739  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:34:45 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0740  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:34:45 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:36 (0.0736 s / it)
2024-05-11T23:34:45 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-11T23:34:46 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:08:21    time: 1.0003  data: 0.0007  max mem: 34625 res mem: 41898
2024-05-11T23:36:24 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:06:33    time: 0.9668  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:38:01 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:53    time: 0.9601  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:39:37 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:03:15    time: 0.9635  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:41:13 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:37    time: 0.9504  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:42:46 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.8962  data: 0.0000  max mem: 34625 res mem: 41898
2024-05-11T23:42:46 | INFO | utils.basic_utils : Evaluation: Total time: 0:08:01 (0.9609 s / it)
2024-05-11T23:42:48 | INFO | tasks.retrieval_utils : Evaluation time 0:11:46
2024-05-11T23:42:50 | INFO | __main__ : Epoch 0
2024-05-11T23:42:50 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       45.75   95.45    98.20        79.8    48.4    95.9     98.3       80.87   80.33     51.6    55.05
test_emb/   42.75   92.00    96.85        77.2    45.1    92.4     96.5       78.00   77.60     54.3    56.55
2024-05-11T23:42:50 | INFO | __main__ : Training time 0:11:48
2024-05-11T23:42:50 | INFO | __main__ : best epoch 0 [config.stop_key test/]
2024-05-11T23:42:50 | INFO | __main__ : Checkpoints and Logs saved at exp/finetuning/ret_rtime/b16_25m_200k_shuffle_hardnegative_MLM/eval_after_training
