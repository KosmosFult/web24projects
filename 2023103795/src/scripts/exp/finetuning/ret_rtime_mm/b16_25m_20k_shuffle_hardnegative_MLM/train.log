2024-05-12T02:59:55 | INFO | umt : Logging to: exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM/train.log
2024-05-12T02:59:55 | INFO | utils.config_utils : config: {
  data_dir: /data2/dy/code/unmasked_teacher/umt_data
  data_root: /data2/dy/code/unmasked_teacher/umt_data/videos_images
  anno_root_pt: /data2/dy/code/unmasked_teacher/umt_data/anno_pretrain
  anno_root_downstream: /data2/dy/code/unmasked_teacher/umt_data/anno_downstream
  TextEncoders: {
      bert: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      bert_large: {
          name: bert_large
          pretrained: bert-large-uncased
          config: configs/config_bert_large.json
          d_model: 1024
          fusion_layer: 19 } }
  train_file: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_shuffle.json', '/data2/dy/temporal_video/dataset', 'video']
  test_file: {
      test: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video'] }
  test_types: ['test']
  num_workers: 6
  stop_key: test/
  is_paragraph_retrieval: False
  is_pretrain: True
  num_frames: 12
  num_frames_test: 12
  batch_size: 32
  max_txt_l: 32
  evaluate: False
  train_shuffle: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 12
          sample_type: rand
          num_frames_test: 12
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 32
          video: 32 }
      batch_size: {
          image: 32
          video: 32 }
      batch_size_test: {
          image: 32
          video: 32 } }
  text_enc: bert
  model: {
      model_cls: UMT
      vision_encoder: {
          name: vit_b16
          img_size: 224
          patch_size: 16
          d_model: 768
          encoder_embed_dim: 768
          encoder_depth: 12
          encoder_num_heads: 12
          drop_path_rate: 0.2
          num_frames: 12
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 12
          clip_decoder_embed_dim: 768
          clip_output_dim: 512
          clip_return_layer: 0
          clip_student_return_interval: 1
          pretrained: /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
          clip_teacher: none
          clip_img_size: 224
          clip_return_interval: 1
          video_mask_type: attention
          video_mask_ratio: 0.0
          video_double_mask_ratio: 0.0
          image_mask_type: attention
          image_mask_ratio: 0.0
          image_double_mask_ratio: 0.0
          keep_temporal: True }
      text_encoder: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      multimodal: {
          enable: True }
      embed_dim: 512
      temp: 0.07 }
  criterion: {
      loss_weight: {
          vtc: 1.0
          mlm: 1.0
          vtm: 1.0
          uta: 0.0 }
      vtm_hard_neg: True
      mlm_masking_prob: 0.5
      uta_norm_type: l2
      uta_loss_type: l2 }
  optimizer: {
      opt: adamW
      lr: 1e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 5
      min_lr_multi: 0.01
      warmup_epochs: 1 }
  zero_shot: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: user
      project: umt_ret }
  dist_url: env://
  device: cuda
  mode: pt
  output_dir: exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM
  resume: False
  debug: False
  log_freq: 100
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
  rank: 0
  world_size: 4
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-05-12T02:59:55 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_shuffle.json', '/data2/dy/temporal_video/dataset', 'video'], 'test_file': {'test': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': True, 'num_frames': 12, 'num_frames_test': 12, 'batch_size': 32, 'max_txt_l': 32, 'evaluate': False, 'train_shuffle': False, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 12, 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 32, 'video': 32}, 'batch_size': {'image': 32, 'video': 32}, 'batch_size_test': {'image': 32, 'video': 32}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.2, 'num_frames': 12, 'tubelet_size': 1, 'use_checkpoint': True, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 1.0, 'vtm': 1.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 5, 'min_lr_multi': 0.01, 'warmup_epochs': 1}, 'zero_shot': False, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 42, 'save_latest': True, 'auto_resume': True, 'pretrained_path': '/datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth', 'rank': 0, 'world_size': 4, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl'}
2024-05-12T02:59:55 | INFO | __main__ : train_file: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_shuffle.json', '/data2/dy/temporal_video/dataset', 'video']
2024-05-12T02:59:55 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-12T03:10:14 | INFO | umt : Logging to: exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM/train.log
2024-05-12T03:10:14 | INFO | utils.config_utils : config: {
  data_dir: /data2/dy/code/unmasked_teacher/umt_data
  data_root: /data2/dy/code/unmasked_teacher/umt_data/videos_images
  anno_root_pt: /data2/dy/code/unmasked_teacher/umt_data/anno_pretrain
  anno_root_downstream: /data2/dy/code/unmasked_teacher/umt_data/anno_downstream
  TextEncoders: {
      bert: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      bert_large: {
          name: bert_large
          pretrained: bert-large-uncased
          config: configs/config_bert_large.json
          d_model: 1024
          fusion_layer: 19 } }
  train_file: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_norewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video']
  test_file: {
      test: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video'] }
  test_types: ['test']
  num_workers: 6
  stop_key: test/
  is_paragraph_retrieval: False
  is_pretrain: True
  num_frames: 12
  num_frames_test: 12
  batch_size: 32
  max_txt_l: 32
  evaluate: False
  train_shuffle: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 12
          sample_type: rand
          num_frames_test: 12
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 32
          video: 32 }
      batch_size: {
          image: 32
          video: 32 }
      batch_size_test: {
          image: 32
          video: 32 } }
  text_enc: bert
  model: {
      model_cls: UMT
      vision_encoder: {
          name: vit_b16
          img_size: 224
          patch_size: 16
          d_model: 768
          encoder_embed_dim: 768
          encoder_depth: 12
          encoder_num_heads: 12
          drop_path_rate: 0.2
          num_frames: 12
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 12
          clip_decoder_embed_dim: 768
          clip_output_dim: 512
          clip_return_layer: 0
          clip_student_return_interval: 1
          pretrained: /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
          clip_teacher: none
          clip_img_size: 224
          clip_return_interval: 1
          video_mask_type: attention
          video_mask_ratio: 0.0
          video_double_mask_ratio: 0.0
          image_mask_type: attention
          image_mask_ratio: 0.0
          image_double_mask_ratio: 0.0
          keep_temporal: True }
      text_encoder: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      multimodal: {
          enable: True }
      embed_dim: 512
      temp: 0.07 }
  criterion: {
      loss_weight: {
          vtc: 1.0
          mlm: 1.0
          vtm: 1.0
          uta: 0.0 }
      vtm_hard_neg: True
      mlm_masking_prob: 0.5
      uta_norm_type: l2
      uta_loss_type: l2 }
  optimizer: {
      opt: adamW
      lr: 1e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 5
      min_lr_multi: 0.01
      warmup_epochs: 1 }
  zero_shot: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: user
      project: umt_ret }
  dist_url: env://
  device: cuda
  mode: pt
  output_dir: exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM
  resume: False
  debug: False
  log_freq: 100
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
  rank: 0
  world_size: 4
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-05-12T03:10:14 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_norewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video'], 'test_file': {'test': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': True, 'num_frames': 12, 'num_frames_test': 12, 'batch_size': 32, 'max_txt_l': 32, 'evaluate': False, 'train_shuffle': False, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 12, 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 32, 'video': 32}, 'batch_size': {'image': 32, 'video': 32}, 'batch_size_test': {'image': 32, 'video': 32}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.2, 'num_frames': 12, 'tubelet_size': 1, 'use_checkpoint': True, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 1.0, 'vtm': 1.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 5, 'min_lr_multi': 0.01, 'warmup_epochs': 1}, 'zero_shot': False, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 42, 'save_latest': True, 'auto_resume': True, 'pretrained_path': '/datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth', 'rank': 0, 'world_size': 4, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl'}
2024-05-12T03:10:14 | INFO | __main__ : train_file: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_norewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video']
2024-05-12T03:10:14 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-12T03:10:15 | INFO | tasks.shared_utils : Creating model
2024-05-12T03:10:30 | INFO | models.umt : Build vision_encoder: vit_b16
2024-05-12T03:10:30 | INFO | models.backbones.vit.vit : Num of patches: 2352
2024-05-12T03:10:30 | INFO | models.backbones.vit.vit : Use checkpoint: True
2024-05-12T03:10:30 | INFO | models.backbones.vit.vit : Checkpoint number: 12
2024-05-12T03:10:30 | INFO | models.backbones.vit.vit : Student return index: []
2024-05-12T03:10:38 | INFO | models.backbones.vit.vit : Loading pretrained weights from /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
2024-05-12T03:10:38 | INFO | models.umt : Build text_encoder bert_base
2024-05-12T03:10:39 | INFO | models.backbones.bert.xbert : build bert with cross_module: ca
2024-05-12T03:10:40 | INFO | models.criterions : Norm type: l2
2024-05-12T03:10:40 | INFO | models.criterions : Loss type: l2
2024-05-12T03:10:40 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.temp: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.word_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.position_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.token_type_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.dense.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.vision_proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.text_proj.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.itm_head.weight: wd: 0.02, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : param module.itm_head.bias: wd: 0, lr: 1e-05
2024-05-12T03:10:40 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0.02 len(p)=141
2024-05-12T03:10:40 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0 len(p)=260
2024-05-12T03:10:40 | INFO | tasks.shared_utils : Auto resuming
2024-05-12T03:10:40 | INFO | tasks.shared_utils : Not found checkpoint in exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM
2024-05-12T03:10:41 | INFO | tasks.shared_utils : _IncompatibleKeys(missing_keys=[], unexpected_keys=['clip_teacher.class_embedding', 'clip_teacher.positional_embedding', 'clip_teacher.proj', 'clip_teacher.conv1.weight', 'clip_teacher.ln_pre.weight', 'clip_teacher.ln_pre.bias', 'clip_teacher.transformer.resblocks.0.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.0.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.0.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.0.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.0.ln_1.weight', 'clip_teacher.transformer.resblocks.0.ln_1.bias', 'clip_teacher.transformer.resblocks.0.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.0.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.0.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.0.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.0.ln_2.weight', 'clip_teacher.transformer.resblocks.0.ln_2.bias', 'clip_teacher.transformer.resblocks.1.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.1.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.1.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.1.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.1.ln_1.weight', 'clip_teacher.transformer.resblocks.1.ln_1.bias', 'clip_teacher.transformer.resblocks.1.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.1.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.1.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.1.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.1.ln_2.weight', 'clip_teacher.transformer.resblocks.1.ln_2.bias', 'clip_teacher.transformer.resblocks.2.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.2.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.2.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.2.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.2.ln_1.weight', 'clip_teacher.transformer.resblocks.2.ln_1.bias', 'clip_teacher.transformer.resblocks.2.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.2.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.2.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.2.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.2.ln_2.weight', 'clip_teacher.transformer.resblocks.2.ln_2.bias', 'clip_teacher.transformer.resblocks.3.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.3.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.3.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.3.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.3.ln_1.weight', 'clip_teacher.transformer.resblocks.3.ln_1.bias', 'clip_teacher.transformer.resblocks.3.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.3.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.3.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.3.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.3.ln_2.weight', 'clip_teacher.transformer.resblocks.3.ln_2.bias', 'clip_teacher.transformer.resblocks.4.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.4.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.4.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.4.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.4.ln_1.weight', 'clip_teacher.transformer.resblocks.4.ln_1.bias', 'clip_teacher.transformer.resblocks.4.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.4.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.4.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.4.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.4.ln_2.weight', 'clip_teacher.transformer.resblocks.4.ln_2.bias', 'clip_teacher.transformer.resblocks.5.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.5.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.5.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.5.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.5.ln_1.weight', 'clip_teacher.transformer.resblocks.5.ln_1.bias', 'clip_teacher.transformer.resblocks.5.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.5.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.5.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.5.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.5.ln_2.weight', 'clip_teacher.transformer.resblocks.5.ln_2.bias', 'clip_teacher.transformer.resblocks.6.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.6.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.6.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.6.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.6.ln_1.weight', 'clip_teacher.transformer.resblocks.6.ln_1.bias', 'clip_teacher.transformer.resblocks.6.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.6.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.6.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.6.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.6.ln_2.weight', 'clip_teacher.transformer.resblocks.6.ln_2.bias', 'clip_teacher.transformer.resblocks.7.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.7.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.7.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.7.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.7.ln_1.weight', 'clip_teacher.transformer.resblocks.7.ln_1.bias', 'clip_teacher.transformer.resblocks.7.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.7.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.7.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.7.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.7.ln_2.weight', 'clip_teacher.transformer.resblocks.7.ln_2.bias', 'clip_teacher.transformer.resblocks.8.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.8.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.8.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.8.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.8.ln_1.weight', 'clip_teacher.transformer.resblocks.8.ln_1.bias', 'clip_teacher.transformer.resblocks.8.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.8.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.8.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.8.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.8.ln_2.weight', 'clip_teacher.transformer.resblocks.8.ln_2.bias', 'clip_teacher.transformer.resblocks.9.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.9.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.9.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.9.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.9.ln_1.weight', 'clip_teacher.transformer.resblocks.9.ln_1.bias', 'clip_teacher.transformer.resblocks.9.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.9.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.9.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.9.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.9.ln_2.weight', 'clip_teacher.transformer.resblocks.9.ln_2.bias', 'clip_teacher.transformer.resblocks.10.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.10.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.10.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.10.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.10.ln_1.weight', 'clip_teacher.transformer.resblocks.10.ln_1.bias', 'clip_teacher.transformer.resblocks.10.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.10.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.10.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.10.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.10.ln_2.weight', 'clip_teacher.transformer.resblocks.10.ln_2.bias', 'clip_teacher.transformer.resblocks.11.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.11.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.11.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.11.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.11.ln_1.weight', 'clip_teacher.transformer.resblocks.11.ln_1.bias', 'clip_teacher.transformer.resblocks.11.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.11.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.11.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.11.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.11.ln_2.weight', 'clip_teacher.transformer.resblocks.11.ln_2.bias', 'clip_teacher.ln_post.weight', 'clip_teacher.ln_post.bias', 'vision_encoder.clip_decoder.0.head.weight', 'vision_encoder.clip_decoder.0.head.bias', 'vision_encoder.clip_decoder.0.norm.weight', 'vision_encoder.clip_decoder.0.norm.bias', 'vision_encoder.clip_decoder.1.head.weight', 'vision_encoder.clip_decoder.1.head.bias', 'vision_encoder.clip_decoder.1.norm.weight', 'vision_encoder.clip_decoder.1.norm.bias', 'vision_encoder.clip_decoder.2.head.weight', 'vision_encoder.clip_decoder.2.head.bias', 'vision_encoder.clip_decoder.2.norm.weight', 'vision_encoder.clip_decoder.2.norm.bias', 'vision_encoder.clip_decoder.3.head.weight', 'vision_encoder.clip_decoder.3.head.bias', 'vision_encoder.clip_decoder.3.norm.weight', 'vision_encoder.clip_decoder.3.norm.bias', 'vision_encoder.clip_decoder.4.head.weight', 'vision_encoder.clip_decoder.4.head.bias', 'vision_encoder.clip_decoder.4.norm.weight', 'vision_encoder.clip_decoder.4.norm.bias', 'vision_encoder.clip_decoder.5.head.weight', 'vision_encoder.clip_decoder.5.head.bias', 'vision_encoder.clip_decoder.5.norm.weight', 'vision_encoder.clip_decoder.5.norm.bias'])
2024-05-12T03:10:41 | INFO | tasks.shared_utils : Loaded checkpoint from /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
2024-05-12T03:10:41 | INFO | __main__ : training
2024-05-12T03:10:41 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 144 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=144 
2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:56 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T03:10:56 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T03:11:03 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

2024-05-12T03:11:03 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

2024-05-12T03:11:03 | INFO | utils.basic_utils : Train Epoch: [0]  [  0/144]  eta: 0:50:57  lr: 0.000000  temperature: 0.0112  video-loss_vtc: 1.5328  video-loss_mlm: 2.9993  video-loss_vtm: 0.3451  time: 21.2314  data: 13.6534  max mem: 33068 res mem: 39648
2024-05-12T03:18:07 | INFO | utils.basic_utils : Train Epoch: [0]  [100/144]  eta: 0:03:13  lr: 0.000007  temperature: 0.0114  video-loss_vtc: 0.9329  video-loss_mlm: 2.2695  video-loss_vtm: 0.0924  time: 4.2570  data: 0.0029  max mem: 34623 res mem: 39872
2024-05-12T03:21:11 | INFO | utils.basic_utils : Train Epoch: [0]  [143/144]  eta: 0:00:04  lr: 0.000010  temperature: 0.0116  video-loss_vtc: 0.9250  video-loss_mlm: 2.6817  video-loss_vtm: 0.0866  time: 4.2668  data: 0.0023  max mem: 34623 res mem: 39872
2024-05-12T03:21:11 | INFO | utils.basic_utils : Train Epoch: [0] Total time: 0:10:29 (4.3686 s / it)
2024-05-12T03:21:11 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0114  video-loss_vtc: 1.1176  video-loss_mlm: 2.5522  video-loss_vtm: 0.2004
2024-05-12T03:21:11 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-12T03:21:11 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-12T03:21:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:13 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:13 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:13 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:13 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:28 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:28 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T03:21:29 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:18:12    time: 17.3448  data: 16.2139  max mem: 34623 res mem: 39872
2024-05-12T03:24:02 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.2801  data: 1.1965  max mem: 34623 res mem: 39872
2024-05-12T03:24:02 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:50 (2.6993 s / it)
2024-05-12T03:24:16 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-12T03:24:16 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-12T03:24:16 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-12T03:24:16 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-12T03:24:16 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-12T03:24:16 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-12T03:24:16 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:22    time: 0.0447  data: 0.0013  max mem: 34623 res mem: 39872
2024-05-12T03:24:23 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0740  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:24:31 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0739  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:24:38 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0740  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:24:45 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0743  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:24:53 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0744  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:24:53 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0740 s / it)
2024-05-12T03:24:53 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-12T03:24:54 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:08:06    time: 0.9706  data: 0.0007  max mem: 34623 res mem: 39872
2024-05-12T03:26:22 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:05:52    time: 0.8680  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:27:49 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:24    time: 0.8738  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:29:17 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:02:56    time: 0.8749  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:30:45 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:28    time: 0.8722  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:32:12 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.8660  data: 0.0000  max mem: 34623 res mem: 39872
2024-05-12T03:32:12 | INFO | utils.basic_utils : Evaluation: Total time: 0:07:18 (0.8759 s / it)
2024-05-12T03:33:12 | INFO | tasks.retrieval_utils : Evaluation time 0:12:01
2024-05-12T03:33:14 | INFO | __main__ : Epoch 0
2024-05-12T03:33:14 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       43.20   92.85    96.95       77.67   42.80    92.7    97.05       77.52   77.59    50.75    51.40
test_emb/   37.45   87.20    93.45       72.70   36.85    87.6    93.70       72.72   72.71    51.25    50.65
2024-05-12T03:33:17 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 144 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=144 
2024-05-12T03:33:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T03:33:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T03:33:38 | INFO | utils.basic_utils : Train Epoch: [1]  [  0/144]  eta: 0:49:20  lr: 0.000010  temperature: 0.0116  video-loss_vtc: 0.8128  video-loss_mlm: 2.3028  video-loss_vtm: 0.1577  time: 20.5575  data: 14.1006  max mem: 34623 res mem: 41898
2024-05-12T03:40:46 | INFO | utils.basic_utils : Train Epoch: [1]  [100/144]  eta: 0:03:15  lr: 0.000009  temperature: 0.0118  video-loss_vtc: 0.7907  video-loss_mlm: 2.3184  video-loss_vtm: 0.0933  time: 4.2835  data: 0.0020  max mem: 34623 res mem: 41898
2024-05-12T03:43:50 | INFO | utils.basic_utils : Train Epoch: [1]  [143/144]  eta: 0:00:04  lr: 0.000009  temperature: 0.0119  video-loss_vtc: 0.8056  video-loss_mlm: 2.7068  video-loss_vtm: 0.0782  time: 4.2728  data: 0.0018  max mem: 34623 res mem: 41898
2024-05-12T03:43:50 | INFO | utils.basic_utils : Train Epoch: [1] Total time: 0:10:32 (4.3920 s / it)
2024-05-12T03:43:50 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0118  video-loss_vtc: 0.8529  video-loss_mlm: 2.3297  video-loss_vtm: 0.1226
2024-05-12T03:43:50 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-12T03:43:50 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-12T03:44:06 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T03:44:06 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T03:44:07 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:17:13    time: 16.4109  data: 15.2579  max mem: 34623 res mem: 41898
2024-05-12T03:46:42 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.2973  data: 1.2273  max mem: 34623 res mem: 41898
2024-05-12T03:46:42 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:51 (2.7158 s / it)
2024-05-12T03:46:56 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-12T03:46:56 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-12T03:46:56 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-12T03:46:56 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-12T03:46:56 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-12T03:46:56 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-12T03:46:56 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:13    time: 0.0278  data: 0.0015  max mem: 34623 res mem: 41898
2024-05-12T03:47:03 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0738  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:47:10 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0739  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:47:18 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0742  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:47:25 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0743  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:47:33 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0746  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:47:33 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0739 s / it)
2024-05-12T03:47:33 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-12T03:47:34 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:07:53    time: 0.9460  data: 0.0009  max mem: 34623 res mem: 41898
2024-05-12T03:49:01 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:05:51    time: 0.8629  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:50:28 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:22    time: 0.8671  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:51:55 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:02:54    time: 0.8695  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:53:21 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:27    time: 0.8628  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:54:48 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.8530  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T03:54:48 | INFO | utils.basic_utils : Evaluation: Total time: 0:07:15 (0.8688 s / it)
2024-05-12T03:55:45 | INFO | tasks.retrieval_utils : Evaluation time 0:11:55
2024-05-12T03:55:47 | INFO | __main__ : Epoch 1
2024-05-12T03:55:47 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       44.45   93.55    97.35       78.45   42.85   93.60    97.35       77.93   78.19    51.65    51.75
test_emb/   39.80   89.00    95.10       74.63   38.95   89.25    94.45       74.22   74.42    51.85    51.30
2024-05-12T03:56:02 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 144 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=144 
2024-05-12T03:56:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T03:56:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T03:56:22 | INFO | utils.basic_utils : Train Epoch: [2]  [  0/144]  eta: 0:47:59  lr: 0.000009  temperature: 0.0119  video-loss_vtc: 0.8203  video-loss_mlm: 2.5067  video-loss_vtm: 0.1471  time: 19.9941  data: 13.3129  max mem: 34623 res mem: 41898
2024-05-12T04:03:30 | INFO | utils.basic_utils : Train Epoch: [2]  [100/144]  eta: 0:03:15  lr: 0.000006  temperature: 0.0120  video-loss_vtc: 0.7687  video-loss_mlm: 2.3192  video-loss_vtm: 0.0209  time: 4.2878  data: 0.0020  max mem: 34623 res mem: 41898
2024-05-12T04:06:34 | INFO | utils.basic_utils : Train Epoch: [2]  [143/144]  eta: 0:00:04  lr: 0.000005  temperature: 0.0121  video-loss_vtc: 0.7756  video-loss_mlm: 2.0560  video-loss_vtm: 0.0735  time: 4.2680  data: 0.0018  max mem: 34623 res mem: 41898
2024-05-12T04:06:34 | INFO | utils.basic_utils : Train Epoch: [2] Total time: 0:10:32 (4.3900 s / it)
2024-05-12T04:06:34 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0120  video-loss_vtc: 0.7952  video-loss_mlm: 2.2657  video-loss_vtm: 0.0954
2024-05-12T04:06:34 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-12T04:06:34 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-12T04:06:50 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T04:06:50 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T04:06:51 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:16:56    time: 16.1334  data: 15.0039  max mem: 34623 res mem: 41898
2024-05-12T04:09:26 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3406  data: 1.2721  max mem: 34623 res mem: 41898
2024-05-12T04:09:26 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:50 (2.7112 s / it)
2024-05-12T04:09:39 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-12T04:09:39 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-12T04:09:39 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-12T04:09:39 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-12T04:09:39 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-12T04:09:39 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-12T04:09:39 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:13    time: 0.0269  data: 0.0012  max mem: 34623 res mem: 41898
2024-05-12T04:09:46 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0737  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:09:54 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0740  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:10:01 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0741  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:10:08 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0743  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:10:16 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0745  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:10:16 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0739 s / it)
2024-05-12T04:10:16 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-12T04:10:17 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:08:04    time: 0.9676  data: 0.0007  max mem: 34623 res mem: 41898
2024-05-12T04:11:45 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:05:52    time: 0.8728  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:13:12 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:24    time: 0.8734  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:14:39 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:02:55    time: 0.8413  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:16:00 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:26    time: 0.7971  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:17:21 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.7734  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:17:21 | INFO | utils.basic_utils : Evaluation: Total time: 0:07:05 (0.8487 s / it)
2024-05-12T04:18:46 | INFO | tasks.retrieval_utils : Evaluation time 0:12:11
2024-05-12T04:18:47 | INFO | __main__ : Epoch 2
2024-05-12T04:18:47 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       44.60   93.70    97.35       78.55   43.60   93.95    97.50       78.35   78.45    51.85     51.7
test_emb/   40.45   89.15    95.25       74.95   40.85   89.35    95.15       75.12   75.03    52.90     52.7
2024-05-12T04:19:02 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 144 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=144 
2024-05-12T04:19:16 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T04:19:16 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T04:19:23 | INFO | utils.basic_utils : Train Epoch: [3]  [  0/144]  eta: 0:49:06  lr: 0.000005  temperature: 0.0121  video-loss_vtc: 0.7970  video-loss_mlm: 2.0939  video-loss_vtm: 0.0591  time: 20.4650  data: 13.4910  max mem: 34623 res mem: 41898
2024-05-12T04:26:32 | INFO | utils.basic_utils : Train Epoch: [3]  [100/144]  eta: 0:03:15  lr: 0.000002  temperature: 0.0121  video-loss_vtc: 0.7016  video-loss_mlm: 1.7774  video-loss_vtm: 0.0339  time: 4.2850  data: 0.0017  max mem: 34623 res mem: 41898
2024-05-12T04:29:35 | INFO | utils.basic_utils : Train Epoch: [3]  [143/144]  eta: 0:00:04  lr: 0.000001  temperature: 0.0121  video-loss_vtc: 0.7353  video-loss_mlm: 1.8690  video-loss_vtm: 0.0630  time: 4.2804  data: 0.0016  max mem: 34623 res mem: 41898
2024-05-12T04:29:35 | INFO | utils.basic_utils : Train Epoch: [3] Total time: 0:10:33 (4.3968 s / it)
2024-05-12T04:29:35 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0121  video-loss_vtc: 0.7624  video-loss_mlm: 2.2486  video-loss_vtm: 0.0769
2024-05-12T04:29:35 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-12T04:29:35 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-12T04:29:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T04:29:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T04:29:53 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:17:48    time: 16.9675  data: 15.7660  max mem: 34623 res mem: 41898
2024-05-12T04:32:31 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3088  data: 1.2269  max mem: 34623 res mem: 41898
2024-05-12T04:32:31 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:54 (2.7704 s / it)
2024-05-12T04:32:44 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-12T04:32:44 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-12T04:32:44 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-12T04:32:44 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-12T04:32:44 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-12T04:32:44 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-12T04:32:44 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:13    time: 0.0274  data: 0.0011  max mem: 34623 res mem: 41898
2024-05-12T04:32:51 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0742  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:32:59 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0738  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:33:06 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0742  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:33:13 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0739  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:33:21 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0748  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:33:21 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0740 s / it)
2024-05-12T04:33:21 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-12T04:33:22 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:07:36    time: 0.9102  data: 0.0008  max mem: 34623 res mem: 41898
2024-05-12T04:34:51 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:05:56    time: 0.8826  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:36:19 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:26    time: 0.8750  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:37:47 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:02:57    time: 0.8829  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:39:15 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:29    time: 0.8803  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:40:42 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.8620  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:40:42 | INFO | utils.basic_utils : Evaluation: Total time: 0:07:20 (0.8802 s / it)
2024-05-12T04:41:50 | INFO | tasks.retrieval_utils : Evaluation time 0:12:15
2024-05-12T04:41:52 | INFO | __main__ : Epoch 3
2024-05-12T04:41:52 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       44.75   94.05    97.25       78.68   44.05   94.25    97.85       78.72   78.70    51.75    51.90
test_emb/   40.90   89.70    95.55       75.38   40.80   89.75    95.15       75.23   75.31    52.75    52.75
2024-05-12T04:42:07 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 144 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=144 
2024-05-12T04:42:20 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T04:42:20 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T04:42:27 | INFO | utils.basic_utils : Train Epoch: [4]  [  0/144]  eta: 0:49:18  lr: 0.000001  temperature: 0.0121  video-loss_vtc: 0.8133  video-loss_mlm: 2.2463  video-loss_vtm: 0.1107  time: 20.5478  data: 13.2652  max mem: 34623 res mem: 41898
2024-05-12T04:49:36 | INFO | utils.basic_utils : Train Epoch: [4]  [100/144]  eta: 0:03:15  lr: 0.000000  temperature: 0.0121  video-loss_vtc: 0.7233  video-loss_mlm: 2.4038  video-loss_vtm: 0.0191  time: 4.2836  data: 0.0017  max mem: 34623 res mem: 41898
2024-05-12T04:52:40 | INFO | utils.basic_utils : Train Epoch: [4]  [143/144]  eta: 0:00:04  lr: 0.000000  temperature: 0.0121  video-loss_vtc: 0.7954  video-loss_mlm: 2.1609  video-loss_vtm: 0.0484  time: 4.2856  data: 0.0015  max mem: 34623 res mem: 41898
2024-05-12T04:52:40 | INFO | utils.basic_utils : Train Epoch: [4] Total time: 0:10:33 (4.3993 s / it)
2024-05-12T04:52:40 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0121  video-loss_vtc: 0.7503  video-loss_mlm: 2.2184  video-loss_vtm: 0.0692
2024-05-12T04:52:40 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-12T04:52:40 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-12T04:52:56 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T04:52:56 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T04:52:58 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:17:07    time: 16.3016  data: 15.1607  max mem: 34623 res mem: 41898
2024-05-12T04:55:32 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.2928  data: 1.2236  max mem: 34623 res mem: 41898
2024-05-12T04:55:32 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:50 (2.7064 s / it)
2024-05-12T04:55:46 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-12T04:55:46 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-12T04:55:46 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-12T04:55:46 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-12T04:55:46 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-12T04:55:46 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-12T04:55:46 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:14    time: 0.0293  data: 0.0013  max mem: 34623 res mem: 41898
2024-05-12T04:55:53 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:29    time: 0.0743  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:56:00 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:22    time: 0.0743  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:56:08 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:14    time: 0.0748  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:56:15 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0749  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:56:23 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0750  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:56:23 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:37 (0.0743 s / it)
2024-05-12T04:56:23 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-12T04:56:24 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:07:37    time: 0.9136  data: 0.0008  max mem: 34623 res mem: 41898
2024-05-12T04:57:52 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:05:53    time: 0.8669  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T04:59:20 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:24    time: 0.8745  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:00:47 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:02:56    time: 0.8793  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:02:15 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:28    time: 0.8702  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:03:42 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.8657  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:03:42 | INFO | utils.basic_utils : Evaluation: Total time: 0:07:19 (0.8771 s / it)
2024-05-12T05:04:53 | INFO | tasks.retrieval_utils : Evaluation time 0:12:12
2024-05-12T05:04:54 | INFO | __main__ : Epoch 4
2024-05-12T05:04:54 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        45.2    94.1    97.35       78.88   44.35   94.25    97.95       78.85   78.87    51.95    52.30
test_emb/    41.0    89.7    95.70       75.47   40.75   89.70    95.15       75.20   75.33    52.90    52.65
2024-05-12T05:05:10 | INFO | __main__ : Training time 1:54:28
2024-05-12T05:05:10 | INFO | __main__ : best epoch 4 [config.stop_key test/]
2024-05-12T05:05:10 | INFO | __main__ : Checkpoints and Logs saved at exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM
2024-05-12T05:05:10 | INFO | __main__ : ===========> START eval_after_training [['test']]
2024-05-12T05:05:10 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_norewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video'], 'test_file': {'test': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': True, 'num_frames': 12, 'num_frames_test': 12, 'batch_size': 32, 'max_txt_l': 32, 'evaluate': True, 'train_shuffle': False, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 12, 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 32, 'video': 32}, 'batch_size': {'image': 32, 'video': 32}, 'batch_size_test': {'image': 32, 'video': 32}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.2, 'num_frames': 12, 'tubelet_size': 1, 'use_checkpoint': True, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 1.0, 'vtm': 1.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 5, 'min_lr_multi': 0.01, 'warmup_epochs': 1, 'num_training_steps': 720, 'num_warmup_steps': 144}, 'zero_shot': False, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM/eval_after_training', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 42, 'save_latest': True, 'auto_resume': True, 'pretrained_path': 'exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM/ckpt_best.pth', 'rank': 0, 'world_size': 4, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl', 'result_dir': 'exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM/eval_after_training'}
2024-05-12T05:05:10 | INFO | __main__ : train_file: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/shuffle/train_reverse_norewrite_shuffle.json', '/data2/dy/temporal_video/dataset', 'video']
2024-05-12T05:05:10 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-12T05:05:10 | INFO | tasks.shared_utils : Creating model
2024-05-12T05:05:21 | INFO | models.umt : Build vision_encoder: vit_b16
2024-05-12T05:05:21 | INFO | models.backbones.vit.vit : Num of patches: 2352
2024-05-12T05:05:21 | INFO | models.backbones.vit.vit : Use checkpoint: True
2024-05-12T05:05:21 | INFO | models.backbones.vit.vit : Checkpoint number: 12
2024-05-12T05:05:21 | INFO | models.backbones.vit.vit : Student return index: []
2024-05-12T05:05:28 | INFO | models.backbones.vit.vit : Loading pretrained weights from /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
2024-05-12T05:05:29 | INFO | models.umt : Build text_encoder bert_base
2024-05-12T05:05:30 | INFO | models.backbones.bert.xbert : build bert with cross_module: ca
2024-05-12T05:05:31 | INFO | models.criterions : Norm type: l2
2024-05-12T05:05:31 | INFO | models.criterions : Loss type: l2
2024-05-12T05:05:31 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.temp: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.word_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.position_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.token_type_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.embeddings.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.bert.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.dense.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.dense.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_encoder.cls.predictions.transform.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.vision_proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_proj.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.text_proj.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.itm_head.weight: wd: 0.02, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : param module.itm_head.bias: wd: 0, lr: 1e-05
2024-05-12T05:05:31 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0.02 len(p)=141
2024-05-12T05:05:31 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0 len(p)=260
2024-05-12T05:05:31 | INFO | tasks.shared_utils : Auto resuming
2024-05-12T05:05:31 | INFO | tasks.shared_utils : Not found checkpoint in exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM/eval_after_training
2024-05-12T05:05:32 | INFO | tasks.shared_utils : <All keys matched successfully>
2024-05-12T05:05:32 | INFO | tasks.shared_utils : Loaded checkpoint from exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM/ckpt_best.pth
2024-05-12T05:05:33 | INFO | __main__ : Start evaluation
2024-05-12T05:05:33 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-12T05:05:33 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-12T05:05:34 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:34 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:34 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:34 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:35 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:35 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:35 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:35 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:35 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:35 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:35 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:35 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:50 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:50 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:50 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:50 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-12T05:05:51 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:18:13    time: 17.3588  data: 16.2071  max mem: 34623 res mem: 41898
2024-05-12T05:08:25 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:02    time: 2.3228  data: 1.2533  max mem: 34623 res mem: 41898
2024-05-12T05:08:25 | INFO | utils.basic_utils : extracting image feats Total time: 0:02:50 (2.7133 s / it)
2024-05-12T05:08:38 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-12T05:08:38 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-12T05:08:38 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-12T05:08:38 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-12T05:08:38 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([501, 2000])
2024-05-12T05:08:38 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-12T05:08:38 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:00:15    time: 0.0313  data: 0.0016  max mem: 34623 res mem: 41898
2024-05-12T05:08:46 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:00:30    time: 0.0779  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:08:54 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:00:23    time: 0.0777  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:09:02 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:00:15    time: 0.0767  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:09:09 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:00:07    time: 0.0776  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:09:17 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.0748  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:09:17 | INFO | utils.basic_utils : Evaluation: Total time: 0:00:38 (0.0768 s / it)
2024-05-12T05:09:17 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([501, 2000])
2024-05-12T05:09:18 | INFO | utils.basic_utils : Evaluation:  [  0/501]  eta: 0:11:02    time: 1.3232  data: 0.0008  max mem: 34623 res mem: 41898
2024-05-12T05:10:51 | INFO | utils.basic_utils : Evaluation:  [100/501]  eta: 0:06:14    time: 0.8538  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:12:17 | INFO | utils.basic_utils : Evaluation:  [200/501]  eta: 0:04:28    time: 0.8478  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:13:41 | INFO | utils.basic_utils : Evaluation:  [300/501]  eta: 0:02:56    time: 0.8375  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:15:05 | INFO | utils.basic_utils : Evaluation:  [400/501]  eta: 0:01:27    time: 0.8456  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:16:30 | INFO | utils.basic_utils : Evaluation:  [500/501]  eta: 0:00:00    time: 0.8342  data: 0.0000  max mem: 34623 res mem: 41898
2024-05-12T05:16:30 | INFO | utils.basic_utils : Evaluation: Total time: 0:07:12 (0.8641 s / it)
2024-05-12T05:18:00 | INFO | tasks.retrieval_utils : Evaluation time 0:12:26
2024-05-12T05:18:02 | INFO | __main__ : Epoch 0
2024-05-12T05:18:02 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        45.2    94.1    97.35       78.88   44.35   94.25    97.95       78.85   78.87    51.95    52.30
test_emb/    41.0    89.7    95.70       75.47   40.75   89.70    95.15       75.20   75.33    52.90    52.65
2024-05-12T05:18:02 | INFO | __main__ : Training time 0:12:29
2024-05-12T05:18:02 | INFO | __main__ : best epoch 0 [config.stop_key test/]
2024-05-12T05:18:02 | INFO | __main__ : Checkpoints and Logs saved at exp/finetuning/ret_rtime/b16_25m_20k_shuffle_hardnegative_MLM/eval_after_training
