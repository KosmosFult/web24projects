2024-05-31T14:49:21 | INFO | umt : Logging to: exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm/train.log
2024-05-31T14:49:21 | INFO | utils.config_utils : config: {
  data_dir: /data2/dy/code/unmasked_teacher/umt_data
  data_root: /data2/dy/code/unmasked_teacher/umt_data/videos_images
  anno_root_pt: /data2/dy/code/unmasked_teacher/umt_data/anno_pretrain
  anno_root_downstream: /data2/dy/code/unmasked_teacher/umt_data/anno_downstream
  TextEncoders: {
      bert: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      bert_large: {
          name: bert_large
          pretrained: bert-large-uncased
          config: configs/config_bert_large.json
          d_model: 1024
          fusion_layer: 19 } }
  train_file: ['/datassd1/WebVid/webvid_temporal_2M.json', '/datassd1/WebVid/videos', 'video']
  test_file: {
      test: ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video'] }
  test_types: ['test']
  num_workers: 6
  rtime: False
  webvid: True
  pos_num: 1
  neg_num: 1
  stop_key: test/
  is_paragraph_retrieval: False
  is_pretrain: False
  num_frames: 4
  num_frames_test: 4
  batch_size: 32
  max_txt_l: 50
  evaluate: False
  zero_shot: False
  train_shuffle: True
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 4
          sample_type: rand
          num_frames_test: 4
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 50
          video: 50 }
      batch_size: {
          image: 32
          video: 32 }
      batch_size_test: {
          image: 32
          video: 32 } }
  text_enc: bert
  model: {
      model_cls: UMT_webvid
      vision_encoder: {
          name: vit_b16
          img_size: 224
          patch_size: 16
          d_model: 768
          encoder_embed_dim: 768
          encoder_depth: 12
          encoder_num_heads: 12
          drop_path_rate: 0.1
          num_frames: 4
          tubelet_size: 1
          use_checkpoint: False
          checkpoint_num: 12
          clip_decoder_embed_dim: 768
          clip_output_dim: 512
          clip_return_layer: 0
          clip_student_return_interval: 1
          pretrained: /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
          clip_teacher: none
          clip_img_size: 224
          clip_return_interval: 1
          video_mask_type: attention
          video_mask_ratio: 0.0
          video_double_mask_ratio: 0.0
          image_mask_type: attention
          image_mask_ratio: 0.0
          image_double_mask_ratio: 0.0
          keep_temporal: True }
      text_encoder: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      multimodal: {
          enable: True }
      embed_dim: 512
      temp: 0.07 }
  criterion: {
      loss_weight: {
          vtc: 1.0
          mlm: 0.0
          vtm: 0.0
          uta: 0.0 }
      vtm_hard_neg: True
      mlm_masking_prob: 0.5
      uta_norm_type: l2
      uta_loss_type: l2 }
  optimizer: {
      opt: adamW
      lr: 0.0001
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 10
      min_lr_multi: 0.01
      warmup_epochs: 1 }
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: user
      project: umt_ret }
  dist_url: env://
  device: cuda
  mode: pt
  output_dir: exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm
  resume: False
  debug: False
  log_freq: 100
  seed: 3407
  save_latest: True
  auto_resume: True
  pretrained_path: /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
  rank: 0
  world_size: 4
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-05-31T14:49:21 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/datassd1/WebVid/webvid_temporal_2M.json', '/datassd1/WebVid/videos', 'video'], 'test_file': {'test': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'rtime': False, 'webvid': True, 'pos_num': 1, 'neg_num': 1, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': False, 'num_frames': 4, 'num_frames_test': 4, 'batch_size': 32, 'max_txt_l': 50, 'evaluate': False, 'zero_shot': False, 'train_shuffle': True, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 4, 'sample_type': 'rand', 'num_frames_test': 4, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 50, 'video': 50}, 'batch_size': {'image': 32, 'video': 32}, 'batch_size_test': {'image': 32, 'video': 32}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT_webvid', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.1, 'num_frames': 4, 'tubelet_size': 1, 'use_checkpoint': False, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 0.0, 'vtm': 0.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 0.0001, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 10, 'min_lr_multi': 0.01, 'warmup_epochs': 1}, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 3407, 'save_latest': True, 'auto_resume': True, 'pretrained_path': '/datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth', 'rank': 0, 'world_size': 4, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl'}
2024-05-31T14:49:21 | INFO | __main__ : train_file: ['/datassd1/WebVid/webvid_temporal_2M.json', '/datassd1/WebVid/videos', 'video']
2024-05-31T14:49:21 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-31T14:49:22 | INFO | tasks.shared_utils : Creating model
2024-05-31T14:49:32 | INFO | models.umt : Build vision_encoder: vit_b16
2024-05-31T14:49:32 | INFO | models.backbones.vit.vit : Num of patches: 784
2024-05-31T14:49:32 | INFO | models.backbones.vit.vit : Use checkpoint: False
2024-05-31T14:49:32 | INFO | models.backbones.vit.vit : Checkpoint number: 12
2024-05-31T14:49:32 | INFO | models.backbones.vit.vit : Student return index: []
2024-05-31T14:49:35 | INFO | models.backbones.vit.vit : Loading pretrained weights from /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
2024-05-31T14:49:36 | INFO | models.umt : Build text_encoder bert_base
2024-05-31T14:49:37 | INFO | models.backbones.bert.xbert : build bert with cross_module: ca
2024-05-31T14:49:37 | INFO | models.criterions : Norm type: l2
2024-05-31T14:49:37 | INFO | models.criterions : Loss type: l2
2024-05-31T14:49:38 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.temp: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.embeddings.word_embeddings.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.embeddings.position_embeddings.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.embeddings.token_type_embeddings.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.embeddings.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.embeddings.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.vision_proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : param module.text_proj.bias: wd: 0, lr: 0.0001
2024-05-31T14:49:38 | INFO | utils.optimizer : optimizer -- lr=0.0001 wd=0.02 len(p)=139
2024-05-31T14:49:38 | INFO | utils.optimizer : optimizer -- lr=0.0001 wd=0 len(p)=255
2024-05-31T14:49:38 | INFO | tasks.shared_utils : Auto resuming
2024-05-31T14:49:38 | INFO | tasks.shared_utils : Not found checkpoint in exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm
2024-05-31T14:49:39 | INFO | tasks.shared_utils : _IncompatibleKeys(missing_keys=[], unexpected_keys=['clip_teacher.class_embedding', 'clip_teacher.positional_embedding', 'clip_teacher.proj', 'clip_teacher.conv1.weight', 'clip_teacher.ln_pre.weight', 'clip_teacher.ln_pre.bias', 'clip_teacher.transformer.resblocks.0.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.0.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.0.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.0.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.0.ln_1.weight', 'clip_teacher.transformer.resblocks.0.ln_1.bias', 'clip_teacher.transformer.resblocks.0.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.0.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.0.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.0.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.0.ln_2.weight', 'clip_teacher.transformer.resblocks.0.ln_2.bias', 'clip_teacher.transformer.resblocks.1.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.1.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.1.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.1.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.1.ln_1.weight', 'clip_teacher.transformer.resblocks.1.ln_1.bias', 'clip_teacher.transformer.resblocks.1.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.1.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.1.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.1.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.1.ln_2.weight', 'clip_teacher.transformer.resblocks.1.ln_2.bias', 'clip_teacher.transformer.resblocks.2.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.2.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.2.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.2.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.2.ln_1.weight', 'clip_teacher.transformer.resblocks.2.ln_1.bias', 'clip_teacher.transformer.resblocks.2.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.2.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.2.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.2.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.2.ln_2.weight', 'clip_teacher.transformer.resblocks.2.ln_2.bias', 'clip_teacher.transformer.resblocks.3.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.3.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.3.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.3.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.3.ln_1.weight', 'clip_teacher.transformer.resblocks.3.ln_1.bias', 'clip_teacher.transformer.resblocks.3.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.3.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.3.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.3.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.3.ln_2.weight', 'clip_teacher.transformer.resblocks.3.ln_2.bias', 'clip_teacher.transformer.resblocks.4.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.4.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.4.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.4.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.4.ln_1.weight', 'clip_teacher.transformer.resblocks.4.ln_1.bias', 'clip_teacher.transformer.resblocks.4.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.4.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.4.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.4.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.4.ln_2.weight', 'clip_teacher.transformer.resblocks.4.ln_2.bias', 'clip_teacher.transformer.resblocks.5.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.5.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.5.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.5.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.5.ln_1.weight', 'clip_teacher.transformer.resblocks.5.ln_1.bias', 'clip_teacher.transformer.resblocks.5.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.5.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.5.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.5.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.5.ln_2.weight', 'clip_teacher.transformer.resblocks.5.ln_2.bias', 'clip_teacher.transformer.resblocks.6.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.6.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.6.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.6.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.6.ln_1.weight', 'clip_teacher.transformer.resblocks.6.ln_1.bias', 'clip_teacher.transformer.resblocks.6.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.6.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.6.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.6.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.6.ln_2.weight', 'clip_teacher.transformer.resblocks.6.ln_2.bias', 'clip_teacher.transformer.resblocks.7.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.7.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.7.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.7.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.7.ln_1.weight', 'clip_teacher.transformer.resblocks.7.ln_1.bias', 'clip_teacher.transformer.resblocks.7.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.7.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.7.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.7.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.7.ln_2.weight', 'clip_teacher.transformer.resblocks.7.ln_2.bias', 'clip_teacher.transformer.resblocks.8.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.8.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.8.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.8.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.8.ln_1.weight', 'clip_teacher.transformer.resblocks.8.ln_1.bias', 'clip_teacher.transformer.resblocks.8.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.8.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.8.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.8.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.8.ln_2.weight', 'clip_teacher.transformer.resblocks.8.ln_2.bias', 'clip_teacher.transformer.resblocks.9.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.9.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.9.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.9.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.9.ln_1.weight', 'clip_teacher.transformer.resblocks.9.ln_1.bias', 'clip_teacher.transformer.resblocks.9.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.9.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.9.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.9.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.9.ln_2.weight', 'clip_teacher.transformer.resblocks.9.ln_2.bias', 'clip_teacher.transformer.resblocks.10.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.10.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.10.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.10.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.10.ln_1.weight', 'clip_teacher.transformer.resblocks.10.ln_1.bias', 'clip_teacher.transformer.resblocks.10.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.10.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.10.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.10.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.10.ln_2.weight', 'clip_teacher.transformer.resblocks.10.ln_2.bias', 'clip_teacher.transformer.resblocks.11.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.11.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.11.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.11.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.11.ln_1.weight', 'clip_teacher.transformer.resblocks.11.ln_1.bias', 'clip_teacher.transformer.resblocks.11.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.11.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.11.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.11.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.11.ln_2.weight', 'clip_teacher.transformer.resblocks.11.ln_2.bias', 'clip_teacher.ln_post.weight', 'clip_teacher.ln_post.bias', 'itm_head.weight', 'itm_head.bias', 'vision_encoder.clip_decoder.0.head.weight', 'vision_encoder.clip_decoder.0.head.bias', 'vision_encoder.clip_decoder.0.norm.weight', 'vision_encoder.clip_decoder.0.norm.bias', 'vision_encoder.clip_decoder.1.head.weight', 'vision_encoder.clip_decoder.1.head.bias', 'vision_encoder.clip_decoder.1.norm.weight', 'vision_encoder.clip_decoder.1.norm.bias', 'vision_encoder.clip_decoder.2.head.weight', 'vision_encoder.clip_decoder.2.head.bias', 'vision_encoder.clip_decoder.2.norm.weight', 'vision_encoder.clip_decoder.2.norm.bias', 'vision_encoder.clip_decoder.3.head.weight', 'vision_encoder.clip_decoder.3.head.bias', 'vision_encoder.clip_decoder.3.norm.weight', 'vision_encoder.clip_decoder.3.norm.bias', 'vision_encoder.clip_decoder.4.head.weight', 'vision_encoder.clip_decoder.4.head.bias', 'vision_encoder.clip_decoder.4.norm.weight', 'vision_encoder.clip_decoder.4.norm.bias', 'vision_encoder.clip_decoder.5.head.weight', 'vision_encoder.clip_decoder.5.head.bias', 'vision_encoder.clip_decoder.5.norm.weight', 'vision_encoder.clip_decoder.5.norm.bias', 'text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias'])
2024-05-31T14:49:39 | INFO | tasks.shared_utils : Loaded checkpoint from /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
2024-05-31T14:49:39 | INFO | __main__ : training
2024-05-31T14:49:39 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:39 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:46 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:46 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

2024-05-31T14:49:46 | INFO | utils.basic_utils : Train Epoch: [0]  [   0/1943]  eta: 3:59:05  lr: 0.000001  temperature: 0.0112  video-loss_vtc: 0.4682  time: 7.3829  data: 4.4144  max mem: 27803 res mem: 29422
2024-05-31T14:51:09 | INFO | utils.basic_utils : Train Epoch: [0]  [ 100/1943]  eta: 0:27:12  lr: 0.000005  temperature: 0.0111  video-loss_vtc: 0.2364  time: 0.8028  data: 0.0015  max mem: 29148 res mem: 30666
2024-05-31T14:52:30 | INFO | utils.basic_utils : Train Epoch: [0]  [ 200/1943]  eta: 0:24:45  lr: 0.000010  temperature: 0.0109  video-loss_vtc: 0.3092  time: 0.7481  data: 0.0018  max mem: 29148 res mem: 30668
2024-05-31T14:53:55 | INFO | utils.basic_utils : Train Epoch: [0]  [ 300/1943]  eta: 0:23:17  lr: 0.000015  temperature: 0.0109  video-loss_vtc: 0.2156  time: 0.7856  data: 0.0013  max mem: 29149 res mem: 30668
2024-05-31T14:55:17 | INFO | utils.basic_utils : Train Epoch: [0]  [ 400/1943]  eta: 0:21:39  lr: 0.000021  temperature: 0.0108  video-loss_vtc: 0.1811  time: 0.8613  data: 0.1598  max mem: 29150 res mem: 30668
2024-05-31T14:56:43 | INFO | utils.basic_utils : Train Epoch: [0]  [ 500/1943]  eta: 0:20:21  lr: 0.000026  temperature: 0.0112  video-loss_vtc: 0.1035  time: 0.7549  data: 0.0540  max mem: 29150 res mem: 30668
2024-05-31T14:58:09 | INFO | utils.basic_utils : Train Epoch: [0]  [ 600/1943]  eta: 0:18:58  lr: 0.000031  temperature: 0.0114  video-loss_vtc: 0.2224  time: 0.8548  data: 0.1560  max mem: 29150 res mem: 30668
2024-05-31T14:59:28 | INFO | utils.basic_utils : Train Epoch: [0]  [ 700/1943]  eta: 0:17:23  lr: 0.000036  temperature: 0.0115  video-loss_vtc: 0.2516  time: 0.8504  data: 0.1478  max mem: 29150 res mem: 30668
2024-05-31T15:00:50 | INFO | utils.basic_utils : Train Epoch: [0]  [ 800/1943]  eta: 0:15:57  lr: 0.000041  temperature: 0.0121  video-loss_vtc: 0.2285  time: 0.7888  data: 0.0392  max mem: 29150 res mem: 30668
2024-05-31T15:02:09 | INFO | utils.basic_utils : Train Epoch: [0]  [ 900/1943]  eta: 0:14:28  lr: 0.000046  temperature: 0.0122  video-loss_vtc: 0.2235  time: 0.7472  data: 0.0439  max mem: 29150 res mem: 30668
2024-05-31T15:03:31 | INFO | utils.basic_utils : Train Epoch: [0]  [1000/1943]  eta: 0:13:03  lr: 0.000052  temperature: 0.0131  video-loss_vtc: 0.3711  time: 0.7912  data: 0.0582  max mem: 29150 res mem: 30668
2024-05-31T15:04:54 | INFO | utils.basic_utils : Train Epoch: [0]  [1100/1943]  eta: 0:11:40  lr: 0.000057  temperature: 0.0133  video-loss_vtc: 0.2224  time: 0.8173  data: 0.0018  max mem: 29150 res mem: 30668
2024-05-31T15:06:15 | INFO | utils.basic_utils : Train Epoch: [0]  [1200/1943]  eta: 0:10:16  lr: 0.000062  temperature: 0.0137  video-loss_vtc: 0.2654  time: 0.7916  data: 0.0017  max mem: 29150 res mem: 30668
2024-05-31T15:07:36 | INFO | utils.basic_utils : Train Epoch: [0]  [1300/1943]  eta: 0:08:52  lr: 0.000067  temperature: 0.0142  video-loss_vtc: 0.3946  time: 0.7886  data: 0.0015  max mem: 29150 res mem: 30668
2024-05-31T15:09:00 | INFO | utils.basic_utils : Train Epoch: [0]  [1400/1943]  eta: 0:07:30  lr: 0.000072  temperature: 0.0147  video-loss_vtc: 0.2893  time: 0.9254  data: 0.0015  max mem: 29150 res mem: 30668
2024-05-31T15:10:24 | INFO | utils.basic_utils : Train Epoch: [0]  [1500/1943]  eta: 0:06:07  lr: 0.000077  temperature: 0.0151  video-loss_vtc: 0.3452  time: 0.7777  data: 0.0016  max mem: 29150 res mem: 30668
2024-05-31T15:11:44 | INFO | utils.basic_utils : Train Epoch: [0]  [1600/1943]  eta: 0:04:43  lr: 0.000082  temperature: 0.0157  video-loss_vtc: 0.1994  time: 0.7901  data: 0.0012  max mem: 29150 res mem: 30668
2024-05-31T15:13:05 | INFO | utils.basic_utils : Train Epoch: [0]  [1700/1943]  eta: 0:03:20  lr: 0.000088  temperature: 0.0163  video-loss_vtc: 0.3335  time: 0.7924  data: 0.0016  max mem: 29150 res mem: 30668
2024-05-31T15:14:22 | INFO | utils.basic_utils : Train Epoch: [0]  [1800/1943]  eta: 0:01:57  lr: 0.000093  temperature: 0.0169  video-loss_vtc: 0.2908  time: 0.7933  data: 0.0015  max mem: 29150 res mem: 30668
2024-05-31T15:15:43 | INFO | utils.basic_utils : Train Epoch: [0]  [1900/1943]  eta: 0:00:35  lr: 0.000098  temperature: 0.0174  video-loss_vtc: 0.5352  time: 0.8245  data: 0.0016  max mem: 29150 res mem: 30668
2024-05-31T15:16:20 | INFO | utils.basic_utils : Train Epoch: [0]  [1942/1943]  eta: 0:00:00  lr: 0.000100  temperature: 0.0172  video-loss_vtc: 0.4257  time: 0.9519  data: 0.2609  max mem: 29150 res mem: 30668
2024-05-31T15:16:20 | INFO | utils.basic_utils : Train Epoch: [0] Total time: 0:26:40 (0.8238 s / it)
2024-05-31T15:16:20 | INFO | __main__ : Averaged train stats: lr: 0.0001  temperature: 0.0132  video-loss_vtc: 0.3053
2024-05-31T15:16:20 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T15:16:20 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:21 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:22 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:22 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T15:16:32 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:11:28    time: 10.9257  data: 10.6688  max mem: 29150 res mem: 30668
2024-05-31T15:18:12 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4601  data: 1.2282  max mem: 29150 res mem: 30668
2024-05-31T15:18:12 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:50 (1.7607 s / it)
2024-05-31T15:18:16 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T15:18:16 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T15:18:16 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T15:18:16 | INFO | tasks.retrieval_utils : Evaluation time 0:01:56
2024-05-31T15:18:17 | INFO | __main__ : Epoch 0
2024-05-31T15:18:17 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25     0.50        0.27    0.05    0.25     0.50        0.27    0.27     0.00     0.00
test_emb/   26.00   64.25    74.25       54.83   24.50   65.35    76.45       55.43   55.13    49.65    48.85
2024-05-31T15:18:20 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T15:18:27 | INFO | utils.basic_utils : Train Epoch: [1]  [   0/1943]  eta: 3:38:08  lr: 0.000100  temperature: 0.0173  video-loss_vtc: 0.3143  time: 6.7360  data: 5.3992  max mem: 29150 res mem: 31124
2024-05-31T15:19:48 | INFO | utils.basic_utils : Train Epoch: [1]  [ 100/1943]  eta: 0:26:53  lr: 0.000100  temperature: 0.0172  video-loss_vtc: 0.3566  time: 0.7748  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T15:21:09 | INFO | utils.basic_utils : Train Epoch: [1]  [ 200/1943]  eta: 0:24:21  lr: 0.000100  temperature: 0.0179  video-loss_vtc: 0.3937  time: 0.7855  data: 0.0700  max mem: 29150 res mem: 31126
2024-05-31T15:22:27 | INFO | utils.basic_utils : Train Epoch: [1]  [ 300/1943]  eta: 0:22:25  lr: 0.000100  temperature: 0.0180  video-loss_vtc: 0.4190  time: 0.7426  data: 0.0389  max mem: 29150 res mem: 31126
2024-05-31T15:23:47 | INFO | utils.basic_utils : Train Epoch: [1]  [ 400/1943]  eta: 0:20:56  lr: 0.000100  temperature: 0.0183  video-loss_vtc: 0.3687  time: 0.8274  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T15:25:08 | INFO | utils.basic_utils : Train Epoch: [1]  [ 500/1943]  eta: 0:19:34  lr: 0.000100  temperature: 0.0186  video-loss_vtc: 0.3846  time: 0.7866  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T15:25:15 | WARNING | dataset.base_dataset : Caught exception Error reading /datassd1/WebVid/videos/3741062.mp4... when loading video /datassd1/WebVid/videos/3741062.mp4, randomly sample a new video as replacement
2024-05-31T15:26:27 | INFO | utils.basic_utils : Train Epoch: [1]  [ 600/1943]  eta: 0:18:07  lr: 0.000100  temperature: 0.0189  video-loss_vtc: 0.5333  time: 0.7512  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T15:27:49 | INFO | utils.basic_utils : Train Epoch: [1]  [ 700/1943]  eta: 0:16:48  lr: 0.000100  temperature: 0.0193  video-loss_vtc: 0.4228  time: 0.7996  data: 0.0931  max mem: 29150 res mem: 31126
2024-05-31T15:29:09 | INFO | utils.basic_utils : Train Epoch: [1]  [ 800/1943]  eta: 0:15:26  lr: 0.000099  temperature: 0.0192  video-loss_vtc: 0.2930  time: 0.7464  data: 0.0380  max mem: 29150 res mem: 31126
2024-05-31T15:30:33 | INFO | utils.basic_utils : Train Epoch: [1]  [ 900/1943]  eta: 0:14:08  lr: 0.000099  temperature: 0.0190  video-loss_vtc: 0.2971  time: 0.7924  data: 0.0860  max mem: 29150 res mem: 31126
2024-05-31T15:31:55 | INFO | utils.basic_utils : Train Epoch: [1]  [1000/1943]  eta: 0:12:47  lr: 0.000099  temperature: 0.0194  video-loss_vtc: 0.3216  time: 0.8230  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T15:33:15 | INFO | utils.basic_utils : Train Epoch: [1]  [1100/1943]  eta: 0:11:25  lr: 0.000099  temperature: 0.0195  video-loss_vtc: 0.3205  time: 0.8235  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T15:34:36 | INFO | utils.basic_utils : Train Epoch: [1]  [1200/1943]  eta: 0:10:03  lr: 0.000099  temperature: 0.0198  video-loss_vtc: 0.4240  time: 0.8943  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T15:35:56 | INFO | utils.basic_utils : Train Epoch: [1]  [1300/1943]  eta: 0:08:41  lr: 0.000099  temperature: 0.0197  video-loss_vtc: 0.3445  time: 0.8581  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T15:37:15 | INFO | utils.basic_utils : Train Epoch: [1]  [1400/1943]  eta: 0:07:19  lr: 0.000098  temperature: 0.0198  video-loss_vtc: 0.3509  time: 0.7977  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T15:38:36 | INFO | utils.basic_utils : Train Epoch: [1]  [1500/1943]  eta: 0:05:58  lr: 0.000098  temperature: 0.0198  video-loss_vtc: 0.4580  time: 0.8250  data: 0.0507  max mem: 29150 res mem: 31126
2024-05-31T15:39:57 | INFO | utils.basic_utils : Train Epoch: [1]  [1600/1943]  eta: 0:04:37  lr: 0.000098  temperature: 0.0198  video-loss_vtc: 0.6366  time: 0.8662  data: 0.0572  max mem: 29150 res mem: 31126
2024-05-31T15:41:16 | INFO | utils.basic_utils : Train Epoch: [1]  [1700/1943]  eta: 0:03:16  lr: 0.000098  temperature: 0.0199  video-loss_vtc: 0.4705  time: 0.7843  data: 0.0392  max mem: 29150 res mem: 31126
2024-05-31T15:42:36 | INFO | utils.basic_utils : Train Epoch: [1]  [1800/1943]  eta: 0:01:55  lr: 0.000097  temperature: 0.0203  video-loss_vtc: 0.3437  time: 0.8030  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T15:43:58 | INFO | utils.basic_utils : Train Epoch: [1]  [1900/1943]  eta: 0:00:34  lr: 0.000097  temperature: 0.0202  video-loss_vtc: 0.3698  time: 0.8952  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T15:44:33 | INFO | utils.basic_utils : Train Epoch: [1]  [1942/1943]  eta: 0:00:00  lr: 0.000097  temperature: 0.0203  video-loss_vtc: 0.4120  time: 0.8151  data: 0.0021  max mem: 29150 res mem: 31126
2024-05-31T15:44:33 | INFO | utils.basic_utils : Train Epoch: [1] Total time: 0:26:12 (0.8095 s / it)
2024-05-31T15:44:33 | INFO | __main__ : Averaged train stats: lr: 0.0001  temperature: 0.0190  video-loss_vtc: 0.3882
2024-05-31T15:44:33 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T15:44:33 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T15:44:44 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:10:41    time: 10.1833  data: 9.9658  max mem: 29150 res mem: 31126
2024-05-31T15:46:23 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4430  data: 1.2174  max mem: 29150 res mem: 31126
2024-05-31T15:46:23 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:49 (1.7361 s / it)
2024-05-31T15:46:28 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T15:46:28 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T15:46:28 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T15:46:28 | INFO | tasks.retrieval_utils : Evaluation time 0:01:54
2024-05-31T15:46:29 | INFO | __main__ : Epoch 1
2024-05-31T15:46:29 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25      0.5        0.27    0.05    0.25     0.50        0.27    0.27      0.0      0.0
test_emb/   24.45   59.40     70.5       51.45   24.65   64.10    75.85       54.87   53.16     50.2     49.7
2024-05-31T15:46:29 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T15:46:35 | INFO | utils.basic_utils : Train Epoch: [2]  [   0/1943]  eta: 3:15:19  lr: 0.000097  temperature: 0.0203  video-loss_vtc: 0.1791  time: 6.0315  data: 3.7801  max mem: 29150 res mem: 31126
2024-05-31T15:47:58 | INFO | utils.basic_utils : Train Epoch: [2]  [ 100/1943]  eta: 0:26:58  lr: 0.000097  temperature: 0.0187  video-loss_vtc: 0.2347  time: 0.8979  data: 0.1963  max mem: 29150 res mem: 31126
2024-05-31T15:49:18 | INFO | utils.basic_utils : Train Epoch: [2]  [ 200/1943]  eta: 0:24:23  lr: 0.000096  temperature: 0.0189  video-loss_vtc: 0.2174  time: 0.7979  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T15:50:36 | INFO | utils.basic_utils : Train Epoch: [2]  [ 300/1943]  eta: 0:22:29  lr: 0.000096  temperature: 0.0193  video-loss_vtc: 0.2289  time: 0.8281  data: 0.0148  max mem: 29150 res mem: 31126
2024-05-31T15:51:57 | INFO | utils.basic_utils : Train Epoch: [2]  [ 400/1943]  eta: 0:21:03  lr: 0.000096  temperature: 0.0192  video-loss_vtc: 0.3275  time: 0.8086  data: 0.0627  max mem: 29150 res mem: 31126
2024-05-31T15:53:19 | INFO | utils.basic_utils : Train Epoch: [2]  [ 500/1943]  eta: 0:19:41  lr: 0.000095  temperature: 0.0195  video-loss_vtc: 0.2196  time: 0.8534  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T15:54:40 | INFO | utils.basic_utils : Train Epoch: [2]  [ 600/1943]  eta: 0:18:18  lr: 0.000095  temperature: 0.0195  video-loss_vtc: 0.3749  time: 0.8408  data: 0.0039  max mem: 29150 res mem: 31126
2024-05-31T15:56:01 | INFO | utils.basic_utils : Train Epoch: [2]  [ 700/1943]  eta: 0:16:55  lr: 0.000094  temperature: 0.0195  video-loss_vtc: 0.1917  time: 0.7863  data: 0.0286  max mem: 29150 res mem: 31126
2024-05-31T15:57:24 | INFO | utils.basic_utils : Train Epoch: [2]  [ 800/1943]  eta: 0:15:34  lr: 0.000094  temperature: 0.0202  video-loss_vtc: 0.2033  time: 0.8038  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T15:58:45 | INFO | utils.basic_utils : Train Epoch: [2]  [ 900/1943]  eta: 0:14:12  lr: 0.000094  temperature: 0.0200  video-loss_vtc: 0.2242  time: 0.7859  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T16:00:06 | INFO | utils.basic_utils : Train Epoch: [2]  [1000/1943]  eta: 0:12:49  lr: 0.000093  temperature: 0.0202  video-loss_vtc: 0.2891  time: 0.7718  data: 0.0698  max mem: 29150 res mem: 31126
2024-05-31T16:01:26 | INFO | utils.basic_utils : Train Epoch: [2]  [1100/1943]  eta: 0:11:27  lr: 0.000093  temperature: 0.0201  video-loss_vtc: 0.3346  time: 0.8329  data: 0.0018  max mem: 29150 res mem: 31126
2024-05-31T16:02:48 | INFO | utils.basic_utils : Train Epoch: [2]  [1200/1943]  eta: 0:10:05  lr: 0.000092  temperature: 0.0199  video-loss_vtc: 0.2968  time: 0.7975  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T16:04:08 | INFO | utils.basic_utils : Train Epoch: [2]  [1300/1943]  eta: 0:08:43  lr: 0.000092  temperature: 0.0198  video-loss_vtc: 0.3208  time: 0.8179  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T16:05:29 | INFO | utils.basic_utils : Train Epoch: [2]  [1400/1943]  eta: 0:07:21  lr: 0.000091  temperature: 0.0199  video-loss_vtc: 0.3330  time: 0.8182  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T16:06:55 | INFO | utils.basic_utils : Train Epoch: [2]  [1500/1943]  eta: 0:06:02  lr: 0.000091  temperature: 0.0200  video-loss_vtc: 0.3586  time: 0.7967  data: 0.0018  max mem: 29150 res mem: 31126
2024-05-31T16:08:18 | INFO | utils.basic_utils : Train Epoch: [2]  [1600/1943]  eta: 0:04:40  lr: 0.000090  temperature: 0.0198  video-loss_vtc: 0.2766  time: 0.8492  data: 0.1031  max mem: 29150 res mem: 31126
2024-05-31T16:09:38 | INFO | utils.basic_utils : Train Epoch: [2]  [1700/1943]  eta: 0:03:18  lr: 0.000090  temperature: 0.0205  video-loss_vtc: 0.2244  time: 0.7299  data: 0.0214  max mem: 29150 res mem: 31126
2024-05-31T16:10:57 | INFO | utils.basic_utils : Train Epoch: [2]  [1800/1943]  eta: 0:01:56  lr: 0.000089  temperature: 0.0203  video-loss_vtc: 0.3984  time: 0.7556  data: 0.0318  max mem: 29150 res mem: 31126
2024-05-31T16:12:22 | INFO | utils.basic_utils : Train Epoch: [2]  [1900/1943]  eta: 0:00:35  lr: 0.000089  temperature: 0.0206  video-loss_vtc: 0.2534  time: 0.9658  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T16:12:54 | INFO | utils.basic_utils : Train Epoch: [2]  [1942/1943]  eta: 0:00:00  lr: 0.000088  temperature: 0.0206  video-loss_vtc: 0.2215  time: 0.7474  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T16:12:54 | INFO | utils.basic_utils : Train Epoch: [2] Total time: 0:26:24 (0.8157 s / it)
2024-05-31T16:12:54 | INFO | __main__ : Averaged train stats: lr: 0.0001  temperature: 0.0198  video-loss_vtc: 0.2964
2024-05-31T16:12:54 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T16:12:54 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T16:13:05 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:11:01    time: 10.5011  data: 10.2883  max mem: 29150 res mem: 31126
2024-05-31T16:14:44 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4423  data: 1.2194  max mem: 29150 res mem: 31126
2024-05-31T16:14:44 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:49 (1.7368 s / it)
2024-05-31T16:14:49 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T16:14:49 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T16:14:49 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T16:14:49 | INFO | tasks.retrieval_utils : Evaluation time 0:01:54
2024-05-31T16:14:50 | INFO | __main__ : Epoch 2
2024-05-31T16:14:50 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25     0.50        0.27    0.05    0.25      0.5        0.27    0.27      0.0     0.00
test_emb/   25.50   58.35    69.95       51.27   25.00   65.95     77.3       56.08   53.68     49.9    50.25
2024-05-31T16:14:50 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T16:14:56 | INFO | utils.basic_utils : Train Epoch: [3]  [   0/1943]  eta: 3:38:28  lr: 0.000088  temperature: 0.0206  video-loss_vtc: 0.2257  time: 6.7464  data: 4.3995  max mem: 29150 res mem: 31126
2024-05-31T16:16:20 | INFO | utils.basic_utils : Train Epoch: [3]  [ 100/1943]  eta: 0:27:24  lr: 0.000088  temperature: 0.0192  video-loss_vtc: 0.1280  time: 0.9355  data: 0.2404  max mem: 29150 res mem: 31126
2024-05-31T16:17:37 | INFO | utils.basic_utils : Train Epoch: [3]  [ 200/1943]  eta: 0:24:13  lr: 0.000087  temperature: 0.0186  video-loss_vtc: 0.2673  time: 0.7629  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T16:17:57 | WARNING | dataset.base_dataset : Caught exception [16:17:57] /github/workspace/src/video/video_reader.cc:486: Error: av_read_frame failed with 1094995529 when loading video /datassd1/WebVid/videos/1049845012.mp4, randomly sample a new video as replacement
2024-05-31T16:19:00 | INFO | utils.basic_utils : Train Epoch: [3]  [ 300/1943]  eta: 0:22:44  lr: 0.000087  temperature: 0.0191  video-loss_vtc: 0.1572  time: 0.8033  data: 0.0018  max mem: 29150 res mem: 31126
2024-05-31T16:20:22 | INFO | utils.basic_utils : Train Epoch: [3]  [ 400/1943]  eta: 0:21:20  lr: 0.000086  temperature: 0.0192  video-loss_vtc: 0.3416  time: 0.9002  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T16:21:42 | INFO | utils.basic_utils : Train Epoch: [3]  [ 500/1943]  eta: 0:19:48  lr: 0.000085  temperature: 0.0195  video-loss_vtc: 0.2010  time: 0.7857  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T16:23:04 | INFO | utils.basic_utils : Train Epoch: [3]  [ 600/1943]  eta: 0:18:23  lr: 0.000085  temperature: 0.0195  video-loss_vtc: 0.3784  time: 0.8642  data: 0.0369  max mem: 29150 res mem: 31126
2024-05-31T16:24:22 | INFO | utils.basic_utils : Train Epoch: [3]  [ 700/1943]  eta: 0:16:54  lr: 0.000084  temperature: 0.0192  video-loss_vtc: 0.4109  time: 0.7884  data: 0.0826  max mem: 29150 res mem: 31126
2024-05-31T16:25:47 | INFO | utils.basic_utils : Train Epoch: [3]  [ 800/1943]  eta: 0:15:37  lr: 0.000083  temperature: 0.0197  video-loss_vtc: 0.2432  time: 0.8041  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T16:27:07 | INFO | utils.basic_utils : Train Epoch: [3]  [ 900/1943]  eta: 0:14:13  lr: 0.000083  temperature: 0.0201  video-loss_vtc: 0.3326  time: 0.7094  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T16:28:29 | INFO | utils.basic_utils : Train Epoch: [3]  [1000/1943]  eta: 0:12:51  lr: 0.000082  temperature: 0.0193  video-loss_vtc: 0.2611  time: 0.8891  data: 0.1874  max mem: 29150 res mem: 31126
2024-05-31T16:29:53 | INFO | utils.basic_utils : Train Epoch: [3]  [1100/1943]  eta: 0:11:31  lr: 0.000081  temperature: 0.0194  video-loss_vtc: 0.1992  time: 0.8873  data: 0.1705  max mem: 29150 res mem: 31126
2024-05-31T16:31:15 | INFO | utils.basic_utils : Train Epoch: [3]  [1200/1943]  eta: 0:10:09  lr: 0.000081  temperature: 0.0198  video-loss_vtc: 0.1228  time: 0.7779  data: 0.0515  max mem: 29150 res mem: 31126
2024-05-31T16:32:34 | INFO | utils.basic_utils : Train Epoch: [3]  [1300/1943]  eta: 0:08:46  lr: 0.000080  temperature: 0.0199  video-loss_vtc: 0.1927  time: 0.7155  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T16:33:58 | INFO | utils.basic_utils : Train Epoch: [3]  [1400/1943]  eta: 0:07:25  lr: 0.000079  temperature: 0.0200  video-loss_vtc: 0.2782  time: 0.8467  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T16:35:17 | INFO | utils.basic_utils : Train Epoch: [3]  [1500/1943]  eta: 0:06:02  lr: 0.000078  temperature: 0.0196  video-loss_vtc: 0.2737  time: 0.8102  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T16:36:40 | INFO | utils.basic_utils : Train Epoch: [3]  [1600/1943]  eta: 0:04:40  lr: 0.000078  temperature: 0.0199  video-loss_vtc: 0.1840  time: 0.8393  data: 0.1355  max mem: 29150 res mem: 31126
2024-05-31T16:38:02 | INFO | utils.basic_utils : Train Epoch: [3]  [1700/1943]  eta: 0:03:18  lr: 0.000077  temperature: 0.0199  video-loss_vtc: 0.4341  time: 0.7437  data: 0.0064  max mem: 29150 res mem: 31126
2024-05-31T16:39:23 | INFO | utils.basic_utils : Train Epoch: [3]  [1800/1943]  eta: 0:01:56  lr: 0.000076  temperature: 0.0201  video-loss_vtc: 0.1735  time: 0.8523  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T16:40:44 | INFO | utils.basic_utils : Train Epoch: [3]  [1900/1943]  eta: 0:00:35  lr: 0.000075  temperature: 0.0197  video-loss_vtc: 0.1660  time: 0.7397  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T16:41:18 | INFO | utils.basic_utils : Train Epoch: [3]  [1942/1943]  eta: 0:00:00  lr: 0.000075  temperature: 0.0199  video-loss_vtc: 0.3751  time: 0.8213  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T16:41:18 | INFO | utils.basic_utils : Train Epoch: [3] Total time: 0:26:27 (0.8173 s / it)
2024-05-31T16:41:18 | INFO | __main__ : Averaged train stats: lr: 0.0001  temperature: 0.0196  video-loss_vtc: 0.2294
2024-05-31T16:41:18 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T16:41:18 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T16:41:29 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:10:54    time: 10.3912  data: 10.1764  max mem: 29150 res mem: 31126
2024-05-31T16:43:07 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4169  data: 1.1943  max mem: 29150 res mem: 31126
2024-05-31T16:43:07 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:48 (1.7172 s / it)
2024-05-31T16:43:11 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T16:43:11 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T16:43:11 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T16:43:11 | INFO | tasks.retrieval_utils : Evaluation time 0:01:53
2024-05-31T16:43:13 | INFO | __main__ : Epoch 3
2024-05-31T16:43:13 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25      0.5        0.27    0.05    0.25      0.5        0.27    0.27     0.00     0.00
test_emb/   23.40   55.40     67.2       48.67   25.00   65.35     76.2       55.52   52.09    49.65    49.65
2024-05-31T16:43:14 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T16:43:19 | INFO | utils.basic_utils : Train Epoch: [4]  [   0/1943]  eta: 2:53:23  lr: 0.000075  temperature: 0.0199  video-loss_vtc: 0.1879  time: 5.3546  data: 4.6916  max mem: 29150 res mem: 31126
2024-05-31T16:44:41 | INFO | utils.basic_utils : Train Epoch: [4]  [ 100/1943]  eta: 0:26:30  lr: 0.000074  temperature: 0.0188  video-loss_vtc: 0.1859  time: 0.8784  data: 0.0089  max mem: 29150 res mem: 31126
2024-05-31T16:46:03 | INFO | utils.basic_utils : Train Epoch: [4]  [ 200/1943]  eta: 0:24:27  lr: 0.000073  temperature: 0.0191  video-loss_vtc: 0.2428  time: 0.7893  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T16:47:23 | INFO | utils.basic_utils : Train Epoch: [4]  [ 300/1943]  eta: 0:22:39  lr: 0.000073  temperature: 0.0180  video-loss_vtc: 0.1928  time: 0.8270  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T16:48:45 | INFO | utils.basic_utils : Train Epoch: [4]  [ 400/1943]  eta: 0:21:15  lr: 0.000072  temperature: 0.0183  video-loss_vtc: 0.1431  time: 0.7849  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T16:50:04 | INFO | utils.basic_utils : Train Epoch: [4]  [ 500/1943]  eta: 0:19:42  lr: 0.000071  temperature: 0.0189  video-loss_vtc: 0.2073  time: 0.8367  data: 0.1334  max mem: 29150 res mem: 31126
2024-05-31T16:51:23 | INFO | utils.basic_utils : Train Epoch: [4]  [ 600/1943]  eta: 0:18:11  lr: 0.000070  temperature: 0.0189  video-loss_vtc: 0.2434  time: 0.7741  data: 0.0026  max mem: 29150 res mem: 31126
2024-05-31T16:51:45 | WARNING | dataset.base_dataset : Caught exception Error reading /datassd1/WebVid/videos/3741062.mp4... when loading video /datassd1/WebVid/videos/3741062.mp4, randomly sample a new video as replacement
2024-05-31T16:52:44 | INFO | utils.basic_utils : Train Epoch: [4]  [ 700/1943]  eta: 0:16:50  lr: 0.000069  temperature: 0.0190  video-loss_vtc: 0.2302  time: 0.8061  data: 0.0157  max mem: 29150 res mem: 31126
2024-05-31T16:54:06 | INFO | utils.basic_utils : Train Epoch: [4]  [ 800/1943]  eta: 0:15:30  lr: 0.000069  temperature: 0.0194  video-loss_vtc: 0.1817  time: 0.8337  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T16:55:26 | INFO | utils.basic_utils : Train Epoch: [4]  [ 900/1943]  eta: 0:14:07  lr: 0.000068  temperature: 0.0194  video-loss_vtc: 0.2172  time: 0.8422  data: 0.1116  max mem: 29150 res mem: 31126
2024-05-31T16:56:48 | INFO | utils.basic_utils : Train Epoch: [4]  [1000/1943]  eta: 0:12:46  lr: 0.000067  temperature: 0.0189  video-loss_vtc: 0.2281  time: 0.9197  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T16:58:05 | INFO | utils.basic_utils : Train Epoch: [4]  [1100/1943]  eta: 0:11:22  lr: 0.000066  temperature: 0.0188  video-loss_vtc: 0.2787  time: 0.7787  data: 0.0764  max mem: 29150 res mem: 31126
2024-05-31T16:59:27 | INFO | utils.basic_utils : Train Epoch: [4]  [1200/1943]  eta: 0:10:01  lr: 0.000065  temperature: 0.0194  video-loss_vtc: 0.1683  time: 0.7492  data: 0.0455  max mem: 29150 res mem: 31126
2024-05-31T17:00:46 | INFO | utils.basic_utils : Train Epoch: [4]  [1300/1943]  eta: 0:08:39  lr: 0.000064  temperature: 0.0190  video-loss_vtc: 0.1451  time: 0.7603  data: 0.0521  max mem: 29150 res mem: 31126
2024-05-31T17:02:09 | INFO | utils.basic_utils : Train Epoch: [4]  [1400/1943]  eta: 0:07:19  lr: 0.000063  temperature: 0.0189  video-loss_vtc: 0.1933  time: 0.8208  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T17:03:30 | INFO | utils.basic_utils : Train Epoch: [4]  [1500/1943]  eta: 0:05:59  lr: 0.000063  temperature: 0.0190  video-loss_vtc: 0.2188  time: 0.7917  data: 0.0894  max mem: 29150 res mem: 31126
2024-05-31T17:04:53 | INFO | utils.basic_utils : Train Epoch: [4]  [1600/1943]  eta: 0:04:38  lr: 0.000062  temperature: 0.0192  video-loss_vtc: 0.1747  time: 0.8318  data: 0.0074  max mem: 29150 res mem: 31126
2024-05-31T17:06:12 | INFO | utils.basic_utils : Train Epoch: [4]  [1700/1943]  eta: 0:03:16  lr: 0.000061  temperature: 0.0191  video-loss_vtc: 0.1990  time: 0.7318  data: 0.0285  max mem: 29150 res mem: 31126
2024-05-31T17:07:32 | INFO | utils.basic_utils : Train Epoch: [4]  [1800/1943]  eta: 0:01:55  lr: 0.000060  temperature: 0.0189  video-loss_vtc: 0.2067  time: 0.7836  data: 0.0349  max mem: 29150 res mem: 31126
2024-05-31T17:08:54 | INFO | utils.basic_utils : Train Epoch: [4]  [1900/1943]  eta: 0:00:34  lr: 0.000059  temperature: 0.0189  video-loss_vtc: 0.1420  time: 0.7666  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T17:09:27 | INFO | utils.basic_utils : Train Epoch: [4]  [1942/1943]  eta: 0:00:00  lr: 0.000059  temperature: 0.0189  video-loss_vtc: 0.2063  time: 0.8081  data: 0.0316  max mem: 29150 res mem: 31126
2024-05-31T17:09:27 | INFO | utils.basic_utils : Train Epoch: [4] Total time: 0:26:13 (0.8097 s / it)
2024-05-31T17:09:27 | INFO | __main__ : Averaged train stats: lr: 0.0001  temperature: 0.0189  video-loss_vtc: 0.1733
2024-05-31T17:09:27 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T17:09:27 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T17:09:38 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:10:54    time: 10.3939  data: 10.1825  max mem: 29150 res mem: 31126
2024-05-31T17:11:15 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4092  data: 1.1902  max mem: 29150 res mem: 31126
2024-05-31T17:11:15 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:47 (1.7041 s / it)
2024-05-31T17:11:20 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T17:11:20 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T17:11:20 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T17:11:20 | INFO | tasks.retrieval_utils : Evaluation time 0:01:53
2024-05-31T17:11:21 | INFO | __main__ : Epoch 4
2024-05-31T17:11:21 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25     0.50        0.27    0.05    0.25      0.5        0.27    0.27      0.0     0.00
test_emb/   26.35   59.95    70.25       52.18   25.90   68.55     80.0       58.15   55.17     50.1    49.25
2024-05-31T17:11:24 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T17:11:30 | INFO | utils.basic_utils : Train Epoch: [5]  [   0/1943]  eta: 3:17:59  lr: 0.000059  temperature: 0.0189  video-loss_vtc: 0.0719  time: 6.1138  data: 5.4513  max mem: 29150 res mem: 31126
2024-05-31T17:12:53 | INFO | utils.basic_utils : Train Epoch: [5]  [ 100/1943]  eta: 0:27:09  lr: 0.000058  temperature: 0.0182  video-loss_vtc: 0.1560  time: 0.8655  data: 0.1668  max mem: 29150 res mem: 31126
2024-05-31T17:14:18 | INFO | utils.basic_utils : Train Epoch: [5]  [ 200/1943]  eta: 0:25:11  lr: 0.000057  temperature: 0.0179  video-loss_vtc: 0.1464  time: 0.8492  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T17:15:43 | INFO | utils.basic_utils : Train Epoch: [5]  [ 300/1943]  eta: 0:23:31  lr: 0.000056  temperature: 0.0178  video-loss_vtc: 0.1695  time: 0.7124  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T17:17:03 | INFO | utils.basic_utils : Train Epoch: [5]  [ 400/1943]  eta: 0:21:45  lr: 0.000055  temperature: 0.0181  video-loss_vtc: 0.0956  time: 0.7738  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T17:18:25 | INFO | utils.basic_utils : Train Epoch: [5]  [ 500/1943]  eta: 0:20:11  lr: 0.000054  temperature: 0.0178  video-loss_vtc: 0.0898  time: 0.8093  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T17:18:44 | WARNING | dataset.base_dataset : Caught exception [17:18:44] /github/workspace/src/video/video_reader.cc:486: Error: av_read_frame failed with 1094995529 when loading video /datassd1/WebVid/videos/1049845012.mp4, randomly sample a new video as replacement
2024-05-31T17:19:47 | INFO | utils.basic_utils : Train Epoch: [5]  [ 600/1943]  eta: 0:18:45  lr: 0.000053  temperature: 0.0180  video-loss_vtc: 0.1229  time: 0.9129  data: 0.2115  max mem: 29150 res mem: 31126
2024-05-31T17:21:08 | INFO | utils.basic_utils : Train Epoch: [5]  [ 700/1943]  eta: 0:17:16  lr: 0.000052  temperature: 0.0179  video-loss_vtc: 0.1007  time: 0.8753  data: 0.1779  max mem: 29150 res mem: 31126
2024-05-31T17:22:30 | INFO | utils.basic_utils : Train Epoch: [5]  [ 800/1943]  eta: 0:15:50  lr: 0.000052  temperature: 0.0182  video-loss_vtc: 0.1866  time: 0.7887  data: 0.0102  max mem: 29150 res mem: 31126
2024-05-31T17:23:52 | INFO | utils.basic_utils : Train Epoch: [5]  [ 900/1943]  eta: 0:14:25  lr: 0.000051  temperature: 0.0180  video-loss_vtc: 0.0953  time: 0.7662  data: 0.0155  max mem: 29150 res mem: 31126
2024-05-31T17:25:14 | INFO | utils.basic_utils : Train Epoch: [5]  [1000/1943]  eta: 0:13:02  lr: 0.000050  temperature: 0.0178  video-loss_vtc: 0.0768  time: 0.8512  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T17:26:37 | INFO | utils.basic_utils : Train Epoch: [5]  [1100/1943]  eta: 0:11:39  lr: 0.000049  temperature: 0.0175  video-loss_vtc: 0.1766  time: 0.9207  data: 0.2242  max mem: 29150 res mem: 31126
2024-05-31T17:27:55 | INFO | utils.basic_utils : Train Epoch: [5]  [1200/1943]  eta: 0:10:13  lr: 0.000048  temperature: 0.0179  video-loss_vtc: 0.1660  time: 0.7086  data: 0.0059  max mem: 29150 res mem: 31126
2024-05-31T17:29:16 | INFO | utils.basic_utils : Train Epoch: [5]  [1300/1943]  eta: 0:08:49  lr: 0.000047  temperature: 0.0177  video-loss_vtc: 0.0701  time: 0.7462  data: 0.0026  max mem: 29150 res mem: 31126
2024-05-31T17:30:37 | INFO | utils.basic_utils : Train Epoch: [5]  [1400/1943]  eta: 0:07:26  lr: 0.000046  temperature: 0.0181  video-loss_vtc: 0.0950  time: 0.8143  data: 0.0485  max mem: 29150 res mem: 31126
2024-05-31T17:31:59 | INFO | utils.basic_utils : Train Epoch: [5]  [1500/1943]  eta: 0:06:04  lr: 0.000045  temperature: 0.0176  video-loss_vtc: 0.0898  time: 0.7680  data: 0.0075  max mem: 29150 res mem: 31126
2024-05-31T17:33:21 | INFO | utils.basic_utils : Train Epoch: [5]  [1600/1943]  eta: 0:04:42  lr: 0.000044  temperature: 0.0175  video-loss_vtc: 0.1031  time: 0.8176  data: 0.0692  max mem: 29150 res mem: 31126
2024-05-31T17:34:13 | WARNING | dataset.base_dataset : Caught exception Error reading /datassd1/WebVid/videos/3741062.mp4... when loading video /datassd1/WebVid/videos/3741062.mp4, randomly sample a new video as replacement
2024-05-31T17:34:43 | INFO | utils.basic_utils : Train Epoch: [5]  [1700/1943]  eta: 0:03:19  lr: 0.000043  temperature: 0.0180  video-loss_vtc: 0.0821  time: 0.8006  data: 0.0989  max mem: 29150 res mem: 31126
2024-05-31T17:36:02 | INFO | utils.basic_utils : Train Epoch: [5]  [1800/1943]  eta: 0:01:57  lr: 0.000043  temperature: 0.0178  video-loss_vtc: 0.1625  time: 0.8585  data: 0.1597  max mem: 29150 res mem: 31126
2024-05-31T17:37:23 | INFO | utils.basic_utils : Train Epoch: [5]  [1900/1943]  eta: 0:00:35  lr: 0.000042  temperature: 0.0174  video-loss_vtc: 0.1124  time: 0.7504  data: 0.0486  max mem: 29150 res mem: 31126
2024-05-31T17:37:56 | INFO | utils.basic_utils : Train Epoch: [5]  [1942/1943]  eta: 0:00:00  lr: 0.000041  temperature: 0.0172  video-loss_vtc: 0.1366  time: 0.7169  data: 0.0244  max mem: 29150 res mem: 31126
2024-05-31T17:37:56 | INFO | utils.basic_utils : Train Epoch: [5] Total time: 0:26:32 (0.8195 s / it)
2024-05-31T17:37:56 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0179  video-loss_vtc: 0.1240
2024-05-31T17:37:56 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T17:37:56 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T17:38:08 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:11:02    time: 10.5218  data: 10.3083  max mem: 29150 res mem: 31126
2024-05-31T17:39:45 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4008  data: 1.1800  max mem: 29150 res mem: 31126
2024-05-31T17:39:45 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:48 (1.7185 s / it)
2024-05-31T17:39:50 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T17:39:50 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T17:39:50 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T17:39:50 | INFO | tasks.retrieval_utils : Evaluation time 0:01:54
2024-05-31T17:39:52 | INFO | __main__ : Epoch 5
2024-05-31T17:39:52 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25     0.50        0.27    0.05    0.25      0.5        0.27    0.27     0.00      0.0
test_emb/   26.75   62.60    74.35       54.57   26.80   69.80     81.9       59.50   57.03    49.55     49.3
2024-05-31T17:39:53 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T17:39:58 | INFO | utils.basic_utils : Train Epoch: [6]  [   0/1943]  eta: 3:13:04  lr: 0.000041  temperature: 0.0173  video-loss_vtc: 0.0668  time: 5.9624  data: 4.4444  max mem: 29150 res mem: 31126
2024-05-31T17:40:04 | WARNING | dataset.base_dataset : Caught exception [17:40:04] /github/workspace/src/video/video_reader.cc:486: Error: av_read_frame failed with 1094995529 when loading video /datassd1/WebVid/videos/1049845012.mp4, randomly sample a new video as replacement
2024-05-31T17:41:20 | INFO | utils.basic_utils : Train Epoch: [6]  [ 100/1943]  eta: 0:26:40  lr: 0.000040  temperature: 0.0170  video-loss_vtc: 0.1194  time: 0.8353  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T17:42:41 | INFO | utils.basic_utils : Train Epoch: [6]  [ 200/1943]  eta: 0:24:20  lr: 0.000040  temperature: 0.0168  video-loss_vtc: 0.0490  time: 0.7875  data: 0.0012  max mem: 29150 res mem: 31126
2024-05-31T17:44:01 | INFO | utils.basic_utils : Train Epoch: [6]  [ 300/1943]  eta: 0:22:37  lr: 0.000039  temperature: 0.0170  video-loss_vtc: 0.1574  time: 0.8069  data: 0.1068  max mem: 29150 res mem: 31126
2024-05-31T17:45:20 | INFO | utils.basic_utils : Train Epoch: [6]  [ 400/1943]  eta: 0:21:00  lr: 0.000038  temperature: 0.0168  video-loss_vtc: 0.1302  time: 0.7863  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T17:46:44 | INFO | utils.basic_utils : Train Epoch: [6]  [ 500/1943]  eta: 0:19:43  lr: 0.000037  temperature: 0.0169  video-loss_vtc: 0.0859  time: 0.7676  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T17:48:04 | INFO | utils.basic_utils : Train Epoch: [6]  [ 600/1943]  eta: 0:18:17  lr: 0.000036  temperature: 0.0169  video-loss_vtc: 0.0776  time: 0.7616  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T17:49:27 | INFO | utils.basic_utils : Train Epoch: [6]  [ 700/1943]  eta: 0:16:59  lr: 0.000035  temperature: 0.0167  video-loss_vtc: 0.0750  time: 0.9352  data: 0.0861  max mem: 29150 res mem: 31126
2024-05-31T17:50:50 | INFO | utils.basic_utils : Train Epoch: [6]  [ 800/1943]  eta: 0:15:37  lr: 0.000034  temperature: 0.0164  video-loss_vtc: 0.0801  time: 0.8884  data: 0.1883  max mem: 29150 res mem: 31126
2024-05-31T17:52:13 | INFO | utils.basic_utils : Train Epoch: [6]  [ 900/1943]  eta: 0:14:16  lr: 0.000033  temperature: 0.0167  video-loss_vtc: 0.0589  time: 0.8568  data: 0.0347  max mem: 29150 res mem: 31126
2024-05-31T17:53:35 | INFO | utils.basic_utils : Train Epoch: [6]  [1000/1943]  eta: 0:12:54  lr: 0.000033  temperature: 0.0164  video-loss_vtc: 0.1562  time: 0.8487  data: 0.0811  max mem: 29150 res mem: 31126
2024-05-31T17:54:55 | INFO | utils.basic_utils : Train Epoch: [6]  [1100/1943]  eta: 0:11:30  lr: 0.000032  temperature: 0.0162  video-loss_vtc: 0.0585  time: 0.7808  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T17:56:15 | INFO | utils.basic_utils : Train Epoch: [6]  [1200/1943]  eta: 0:10:07  lr: 0.000031  temperature: 0.0164  video-loss_vtc: 0.1282  time: 0.8057  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T17:57:35 | INFO | utils.basic_utils : Train Epoch: [6]  [1300/1943]  eta: 0:08:44  lr: 0.000030  temperature: 0.0166  video-loss_vtc: 0.0794  time: 0.7928  data: 0.0689  max mem: 29150 res mem: 31126
2024-05-31T17:58:55 | INFO | utils.basic_utils : Train Epoch: [6]  [1400/1943]  eta: 0:07:22  lr: 0.000029  temperature: 0.0165  video-loss_vtc: 0.0399  time: 0.7437  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:00:14 | INFO | utils.basic_utils : Train Epoch: [6]  [1500/1943]  eta: 0:06:00  lr: 0.000029  temperature: 0.0163  video-loss_vtc: 0.0974  time: 0.7698  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:01:36 | INFO | utils.basic_utils : Train Epoch: [6]  [1600/1943]  eta: 0:04:39  lr: 0.000028  temperature: 0.0164  video-loss_vtc: 0.0578  time: 0.8154  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T18:02:58 | INFO | utils.basic_utils : Train Epoch: [6]  [1700/1943]  eta: 0:03:17  lr: 0.000027  temperature: 0.0166  video-loss_vtc: 0.0806  time: 0.8658  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T18:04:21 | INFO | utils.basic_utils : Train Epoch: [6]  [1800/1943]  eta: 0:01:56  lr: 0.000026  temperature: 0.0164  video-loss_vtc: 0.0864  time: 0.8643  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T18:05:42 | INFO | utils.basic_utils : Train Epoch: [6]  [1900/1943]  eta: 0:00:35  lr: 0.000025  temperature: 0.0163  video-loss_vtc: 0.0744  time: 0.7805  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:06:16 | INFO | utils.basic_utils : Train Epoch: [6]  [1942/1943]  eta: 0:00:00  lr: 0.000025  temperature: 0.0165  video-loss_vtc: 0.0652  time: 0.7658  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T18:06:16 | INFO | utils.basic_utils : Train Epoch: [6] Total time: 0:26:23 (0.8152 s / it)
2024-05-31T18:06:16 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0166  video-loss_vtc: 0.0897
2024-05-31T18:06:16 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T18:06:16 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T18:06:28 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:10:42    time: 10.1929  data: 9.9798  max mem: 29150 res mem: 31126
2024-05-31T18:08:06 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4616  data: 1.2391  max mem: 29150 res mem: 31126
2024-05-31T18:08:06 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:48 (1.7213 s / it)
2024-05-31T18:08:11 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T18:08:11 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T18:08:11 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T18:08:11 | INFO | tasks.retrieval_utils : Evaluation time 0:01:54
2024-05-31T18:08:12 | INFO | __main__ : Epoch 6
2024-05-31T18:08:12 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25     0.50        0.27    0.05    0.25      0.5        0.27    0.27      0.0     0.00
test_emb/   26.80   60.40    70.65       52.62   27.35   71.30     82.7       60.45   56.53     49.8    49.65
2024-05-31T18:08:12 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T18:08:18 | INFO | utils.basic_utils : Train Epoch: [7]  [   0/1943]  eta: 3:12:28  lr: 0.000025  temperature: 0.0165  video-loss_vtc: 0.0767  time: 5.9438  data: 4.3520  max mem: 29150 res mem: 31126
2024-05-31T18:09:37 | INFO | utils.basic_utils : Train Epoch: [7]  [ 100/1943]  eta: 0:25:45  lr: 0.000024  temperature: 0.0161  video-loss_vtc: 0.1417  time: 0.7597  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:10:59 | INFO | utils.basic_utils : Train Epoch: [7]  [ 200/1943]  eta: 0:24:07  lr: 0.000023  temperature: 0.0158  video-loss_vtc: 0.0412  time: 0.8744  data: 0.0512  max mem: 29150 res mem: 31126
2024-05-31T18:12:20 | INFO | utils.basic_utils : Train Epoch: [7]  [ 300/1943]  eta: 0:22:30  lr: 0.000023  temperature: 0.0156  video-loss_vtc: 0.0740  time: 0.7723  data: 0.0649  max mem: 29150 res mem: 31126
2024-05-31T18:13:41 | INFO | utils.basic_utils : Train Epoch: [7]  [ 400/1943]  eta: 0:21:04  lr: 0.000022  temperature: 0.0157  video-loss_vtc: 0.0558  time: 0.7491  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T18:15:01 | INFO | utils.basic_utils : Train Epoch: [7]  [ 500/1943]  eta: 0:19:35  lr: 0.000021  temperature: 0.0160  video-loss_vtc: 0.0564  time: 0.7471  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T18:16:23 | INFO | utils.basic_utils : Train Epoch: [7]  [ 600/1943]  eta: 0:18:15  lr: 0.000020  temperature: 0.0160  video-loss_vtc: 0.0624  time: 0.7783  data: 0.0012  max mem: 29150 res mem: 31126
2024-05-31T18:17:44 | INFO | utils.basic_utils : Train Epoch: [7]  [ 700/1943]  eta: 0:16:54  lr: 0.000020  temperature: 0.0158  video-loss_vtc: 0.0410  time: 0.8406  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T18:19:04 | INFO | utils.basic_utils : Train Epoch: [7]  [ 800/1943]  eta: 0:15:29  lr: 0.000019  temperature: 0.0159  video-loss_vtc: 0.0813  time: 0.8062  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T18:20:27 | INFO | utils.basic_utils : Train Epoch: [7]  [ 900/1943]  eta: 0:14:09  lr: 0.000018  temperature: 0.0159  video-loss_vtc: 0.0419  time: 0.8109  data: 0.1076  max mem: 29150 res mem: 31126
2024-05-31T18:21:47 | INFO | utils.basic_utils : Train Epoch: [7]  [1000/1943]  eta: 0:12:47  lr: 0.000018  temperature: 0.0160  video-loss_vtc: 0.0587  time: 0.7915  data: 0.0911  max mem: 29150 res mem: 31126
2024-05-31T18:23:12 | INFO | utils.basic_utils : Train Epoch: [7]  [1100/1943]  eta: 0:11:29  lr: 0.000017  temperature: 0.0159  video-loss_vtc: 0.0379  time: 0.9073  data: 0.2073  max mem: 29150 res mem: 31126
2024-05-31T18:24:34 | INFO | utils.basic_utils : Train Epoch: [7]  [1200/1943]  eta: 0:10:07  lr: 0.000016  temperature: 0.0158  video-loss_vtc: 0.1258  time: 0.8638  data: 0.1612  max mem: 29150 res mem: 31126
2024-05-31T18:25:56 | INFO | utils.basic_utils : Train Epoch: [7]  [1300/1943]  eta: 0:08:45  lr: 0.000016  temperature: 0.0156  video-loss_vtc: 0.0978  time: 0.8118  data: 0.0021  max mem: 29150 res mem: 31126
2024-05-31T18:26:33 | WARNING | dataset.base_dataset : Caught exception Error reading /datassd1/WebVid/videos/3741062.mp4... when loading video /datassd1/WebVid/videos/3741062.mp4, randomly sample a new video as replacement
2024-05-31T18:27:19 | INFO | utils.basic_utils : Train Epoch: [7]  [1400/1943]  eta: 0:07:24  lr: 0.000015  temperature: 0.0155  video-loss_vtc: 0.0572  time: 0.8218  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T18:28:41 | INFO | utils.basic_utils : Train Epoch: [7]  [1500/1943]  eta: 0:06:02  lr: 0.000014  temperature: 0.0158  video-loss_vtc: 0.0875  time: 0.7830  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T18:30:02 | INFO | utils.basic_utils : Train Epoch: [7]  [1600/1943]  eta: 0:04:40  lr: 0.000014  temperature: 0.0157  video-loss_vtc: 0.0868  time: 0.7673  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:31:23 | INFO | utils.basic_utils : Train Epoch: [7]  [1700/1943]  eta: 0:03:18  lr: 0.000013  temperature: 0.0157  video-loss_vtc: 0.0206  time: 0.8388  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T18:32:46 | INFO | utils.basic_utils : Train Epoch: [7]  [1800/1943]  eta: 0:01:56  lr: 0.000013  temperature: 0.0155  video-loss_vtc: 0.0296  time: 0.8019  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:34:07 | INFO | utils.basic_utils : Train Epoch: [7]  [1900/1943]  eta: 0:00:35  lr: 0.000012  temperature: 0.0155  video-loss_vtc: 0.1542  time: 0.8763  data: 0.1779  max mem: 29150 res mem: 31126
2024-05-31T18:34:43 | INFO | utils.basic_utils : Train Epoch: [7]  [1942/1943]  eta: 0:00:00  lr: 0.000012  temperature: 0.0155  video-loss_vtc: 0.0347  time: 0.8125  data: 0.1213  max mem: 29150 res mem: 31126
2024-05-31T18:34:43 | INFO | utils.basic_utils : Train Epoch: [7] Total time: 0:26:30 (0.8184 s / it)
2024-05-31T18:34:43 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0158  video-loss_vtc: 0.0655
2024-05-31T18:34:43 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T18:34:43 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T18:34:54 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:10:44    time: 10.2304  data: 10.0211  max mem: 29150 res mem: 31126
2024-05-31T18:36:32 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.5186  data: 1.2935  max mem: 29150 res mem: 31126
2024-05-31T18:36:32 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:48 (1.7256 s / it)
2024-05-31T18:36:37 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T18:36:37 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T18:36:37 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T18:36:37 | INFO | tasks.retrieval_utils : Evaluation time 0:01:54
2024-05-31T18:36:38 | INFO | __main__ : Epoch 7
2024-05-31T18:36:38 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25     0.50        0.27    0.05    0.25     0.50        0.27    0.27      0.0      0.0
test_emb/   28.15   62.90    72.55       54.53   28.25   72.15    83.05       61.15   57.84     49.6     49.5
2024-05-31T18:36:39 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T18:36:45 | INFO | utils.basic_utils : Train Epoch: [8]  [   0/1943]  eta: 3:10:40  lr: 0.000012  temperature: 0.0155  video-loss_vtc: 0.0715  time: 5.8879  data: 3.4137  max mem: 29150 res mem: 31126
2024-05-31T18:38:07 | INFO | utils.basic_utils : Train Epoch: [8]  [ 100/1943]  eta: 0:26:48  lr: 0.000011  temperature: 0.0154  video-loss_vtc: 0.0318  time: 0.8882  data: 0.0019  max mem: 29150 res mem: 31126
2024-05-31T18:39:27 | INFO | utils.basic_utils : Train Epoch: [8]  [ 200/1943]  eta: 0:24:16  lr: 0.000011  temperature: 0.0153  video-loss_vtc: 0.0921  time: 0.8129  data: 0.0201  max mem: 29150 res mem: 31126
2024-05-31T18:40:46 | INFO | utils.basic_utils : Train Epoch: [8]  [ 300/1943]  eta: 0:22:24  lr: 0.000010  temperature: 0.0151  video-loss_vtc: 0.0160  time: 0.7601  data: 0.0190  max mem: 29150 res mem: 31126
2024-05-31T18:42:08 | INFO | utils.basic_utils : Train Epoch: [8]  [ 400/1943]  eta: 0:21:05  lr: 0.000009  temperature: 0.0150  video-loss_vtc: 0.0204  time: 0.8879  data: 0.0018  max mem: 29150 res mem: 31126
2024-05-31T18:43:25 | INFO | utils.basic_utils : Train Epoch: [8]  [ 500/1943]  eta: 0:19:28  lr: 0.000009  temperature: 0.0151  video-loss_vtc: 0.0702  time: 0.7617  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:44:48 | INFO | utils.basic_utils : Train Epoch: [8]  [ 600/1943]  eta: 0:18:11  lr: 0.000008  temperature: 0.0150  video-loss_vtc: 0.0336  time: 0.9776  data: 0.2777  max mem: 29150 res mem: 31126
2024-05-31T18:46:09 | INFO | utils.basic_utils : Train Epoch: [8]  [ 700/1943]  eta: 0:16:50  lr: 0.000008  temperature: 0.0151  video-loss_vtc: 0.0662  time: 0.7647  data: 0.0535  max mem: 29150 res mem: 31126
2024-05-31T18:47:30 | INFO | utils.basic_utils : Train Epoch: [8]  [ 800/1943]  eta: 0:15:28  lr: 0.000007  temperature: 0.0150  video-loss_vtc: 0.0708  time: 0.7838  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:48:53 | INFO | utils.basic_utils : Train Epoch: [8]  [ 900/1943]  eta: 0:14:09  lr: 0.000007  temperature: 0.0149  video-loss_vtc: 0.1010  time: 0.7073  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:50:15 | INFO | utils.basic_utils : Train Epoch: [8]  [1000/1943]  eta: 0:12:48  lr: 0.000007  temperature: 0.0148  video-loss_vtc: 0.0452  time: 0.8468  data: 0.0018  max mem: 29150 res mem: 31126
2024-05-31T18:51:34 | INFO | utils.basic_utils : Train Epoch: [8]  [1100/1943]  eta: 0:11:25  lr: 0.000006  temperature: 0.0149  video-loss_vtc: 0.0509  time: 0.7938  data: 0.0433  max mem: 29150 res mem: 31126
2024-05-31T18:52:57 | INFO | utils.basic_utils : Train Epoch: [8]  [1200/1943]  eta: 0:10:04  lr: 0.000006  temperature: 0.0149  video-loss_vtc: 0.0251  time: 0.8312  data: 0.0578  max mem: 29150 res mem: 31126
2024-05-31T18:54:16 | INFO | utils.basic_utils : Train Epoch: [8]  [1300/1943]  eta: 0:08:42  lr: 0.000005  temperature: 0.0149  video-loss_vtc: 0.0943  time: 0.7802  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T18:55:33 | WARNING | dataset.base_dataset : Caught exception [18:55:33] /github/workspace/src/video/video_reader.cc:486: Error: av_read_frame failed with 1094995529 when loading video /datassd1/WebVid/videos/1049845012.mp4, randomly sample a new video as replacement
2024-05-31T18:55:39 | INFO | utils.basic_utils : Train Epoch: [8]  [1400/1943]  eta: 0:07:21  lr: 0.000005  temperature: 0.0149  video-loss_vtc: 0.0583  time: 0.8694  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T18:57:03 | INFO | utils.basic_utils : Train Epoch: [8]  [1500/1943]  eta: 0:06:01  lr: 0.000005  temperature: 0.0149  video-loss_vtc: 0.0329  time: 0.7419  data: 0.0017  max mem: 29150 res mem: 31126
2024-05-31T18:58:28 | INFO | utils.basic_utils : Train Epoch: [8]  [1600/1943]  eta: 0:04:40  lr: 0.000004  temperature: 0.0148  video-loss_vtc: 0.0448  time: 0.8565  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T18:59:49 | INFO | utils.basic_utils : Train Epoch: [8]  [1700/1943]  eta: 0:03:18  lr: 0.000004  temperature: 0.0148  video-loss_vtc: 0.0274  time: 0.7430  data: 0.0026  max mem: 29150 res mem: 31126
2024-05-31T19:01:12 | INFO | utils.basic_utils : Train Epoch: [8]  [1800/1943]  eta: 0:01:56  lr: 0.000003  temperature: 0.0148  video-loss_vtc: 0.0238  time: 0.8243  data: 0.1207  max mem: 29150 res mem: 31126
2024-05-31T19:02:35 | INFO | utils.basic_utils : Train Epoch: [8]  [1900/1943]  eta: 0:00:35  lr: 0.000003  temperature: 0.0147  video-loss_vtc: 0.0995  time: 0.8374  data: 0.1295  max mem: 29150 res mem: 31126
2024-05-31T19:03:09 | INFO | utils.basic_utils : Train Epoch: [8]  [1942/1943]  eta: 0:00:00  lr: 0.000003  temperature: 0.0147  video-loss_vtc: 0.0402  time: 0.7371  data: 0.0425  max mem: 29150 res mem: 31126
2024-05-31T19:03:09 | INFO | utils.basic_utils : Train Epoch: [8] Total time: 0:26:29 (0.8181 s / it)
2024-05-31T19:03:09 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0150  video-loss_vtc: 0.0499
2024-05-31T19:03:09 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T19:03:09 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T19:03:20 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:10:59    time: 10.4733  data: 10.2589  max mem: 29150 res mem: 31126
2024-05-31T19:04:59 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4358  data: 1.2132  max mem: 29150 res mem: 31126
2024-05-31T19:04:59 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:49 (1.7374 s / it)
2024-05-31T19:05:05 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T19:05:05 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T19:05:05 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T19:05:05 | INFO | tasks.retrieval_utils : Evaluation time 0:01:55
2024-05-31T19:05:06 | INFO | __main__ : Epoch 8
2024-05-31T19:05:06 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25     0.50        0.27    0.05    0.25      0.5        0.27    0.27     0.00     0.00
test_emb/   28.75   62.40    72.75       54.63   27.45   73.35     83.1       61.30   57.97    49.75    49.05
2024-05-31T19:05:06 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 1943 batches in total
dataloader index=0 name=video, batch-size=32 length(#batches)=1943 
2024-05-31T19:05:12 | INFO | utils.basic_utils : Train Epoch: [9]  [   0/1943]  eta: 3:28:43  lr: 0.000003  temperature: 0.0147  video-loss_vtc: 0.0783  time: 6.4454  data: 5.4862  max mem: 29150 res mem: 31126
2024-05-31T19:06:33 | INFO | utils.basic_utils : Train Epoch: [9]  [ 100/1943]  eta: 0:26:33  lr: 0.000003  temperature: 0.0148  video-loss_vtc: 0.0245  time: 0.7477  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T19:07:57 | INFO | utils.basic_utils : Train Epoch: [9]  [ 200/1943]  eta: 0:24:42  lr: 0.000002  temperature: 0.0147  video-loss_vtc: 0.0633  time: 0.7611  data: 0.0590  max mem: 29150 res mem: 31126
2024-05-31T19:09:17 | INFO | utils.basic_utils : Train Epoch: [9]  [ 300/1943]  eta: 0:22:48  lr: 0.000002  temperature: 0.0147  video-loss_vtc: 0.0612  time: 0.7823  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T19:10:38 | INFO | utils.basic_utils : Train Epoch: [9]  [ 400/1943]  eta: 0:21:19  lr: 0.000002  temperature: 0.0147  video-loss_vtc: 0.0394  time: 0.8628  data: 0.0366  max mem: 29150 res mem: 31126
2024-05-31T19:12:03 | INFO | utils.basic_utils : Train Epoch: [9]  [ 500/1943]  eta: 0:20:00  lr: 0.000002  temperature: 0.0147  video-loss_vtc: 0.0231  time: 0.9145  data: 0.2192  max mem: 29150 res mem: 31126
2024-05-31T19:13:27 | INFO | utils.basic_utils : Train Epoch: [9]  [ 600/1943]  eta: 0:18:39  lr: 0.000001  temperature: 0.0147  video-loss_vtc: 0.0106  time: 0.8550  data: 0.0592  max mem: 29150 res mem: 31126
2024-05-31T19:14:49 | INFO | utils.basic_utils : Train Epoch: [9]  [ 700/1943]  eta: 0:17:14  lr: 0.000001  temperature: 0.0147  video-loss_vtc: 0.0360  time: 0.8228  data: 0.1227  max mem: 29150 res mem: 31126
2024-05-31T19:16:11 | INFO | utils.basic_utils : Train Epoch: [9]  [ 800/1943]  eta: 0:15:48  lr: 0.000001  temperature: 0.0147  video-loss_vtc: 0.0521  time: 0.7822  data: 0.0236  max mem: 29150 res mem: 31126
2024-05-31T19:17:35 | INFO | utils.basic_utils : Train Epoch: [9]  [ 900/1943]  eta: 0:14:27  lr: 0.000001  temperature: 0.0147  video-loss_vtc: 0.0779  time: 0.8596  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T19:18:58 | INFO | utils.basic_utils : Train Epoch: [9]  [1000/1943]  eta: 0:13:04  lr: 0.000001  temperature: 0.0147  video-loss_vtc: 0.0443  time: 0.8948  data: 0.1642  max mem: 29150 res mem: 31126
2024-05-31T19:20:19 | INFO | utils.basic_utils : Train Epoch: [9]  [1100/1943]  eta: 0:11:39  lr: 0.000001  temperature: 0.0147  video-loss_vtc: 0.0306  time: 0.7910  data: 0.0794  max mem: 29150 res mem: 31126
2024-05-31T19:21:42 | INFO | utils.basic_utils : Train Epoch: [9]  [1200/1943]  eta: 0:10:16  lr: 0.000001  temperature: 0.0147  video-loss_vtc: 0.0518  time: 0.8928  data: 0.0776  max mem: 29150 res mem: 31126
2024-05-31T19:23:07 | INFO | utils.basic_utils : Train Epoch: [9]  [1300/1943]  eta: 0:08:54  lr: 0.000001  temperature: 0.0146  video-loss_vtc: 0.0485  time: 0.9640  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T19:24:31 | INFO | utils.basic_utils : Train Epoch: [9]  [1400/1943]  eta: 0:07:31  lr: 0.000001  temperature: 0.0146  video-loss_vtc: 0.0295  time: 0.7833  data: 0.0016  max mem: 29150 res mem: 31126
2024-05-31T19:25:53 | INFO | utils.basic_utils : Train Epoch: [9]  [1500/1943]  eta: 0:06:07  lr: 0.000001  temperature: 0.0146  video-loss_vtc: 0.0285  time: 0.8582  data: 0.0015  max mem: 29150 res mem: 31126
2024-05-31T19:27:15 | INFO | utils.basic_utils : Train Epoch: [9]  [1600/1943]  eta: 0:04:44  lr: 0.000001  temperature: 0.0146  video-loss_vtc: 0.0199  time: 0.8057  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T19:28:40 | INFO | utils.basic_utils : Train Epoch: [9]  [1700/1943]  eta: 0:03:22  lr: 0.000001  temperature: 0.0146  video-loss_vtc: 0.0221  time: 0.8163  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T19:30:00 | INFO | utils.basic_utils : Train Epoch: [9]  [1800/1943]  eta: 0:01:58  lr: 0.000001  temperature: 0.0146  video-loss_vtc: 0.0240  time: 0.7467  data: 0.0014  max mem: 29150 res mem: 31126
2024-05-31T19:31:21 | INFO | utils.basic_utils : Train Epoch: [9]  [1900/1943]  eta: 0:00:35  lr: 0.000001  temperature: 0.0146  video-loss_vtc: 0.0652  time: 0.7791  data: 0.0281  max mem: 29150 res mem: 31126
2024-05-31T19:31:56 | INFO | utils.basic_utils : Train Epoch: [9]  [1942/1943]  eta: 0:00:00  lr: 0.000001  temperature: 0.0146  video-loss_vtc: 0.0499  time: 0.7701  data: 0.0013  max mem: 29150 res mem: 31126
2024-05-31T19:31:56 | INFO | utils.basic_utils : Train Epoch: [9] Total time: 0:26:50 (0.8289 s / it)
2024-05-31T19:31:56 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0147  video-loss_vtc: 0.0452
2024-05-31T19:31:56 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T19:31:56 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T19:32:07 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:10:38    time: 10.1412  data: 9.9256  max mem: 29150 res mem: 31126
2024-05-31T19:33:47 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4701  data: 1.2487  max mem: 29150 res mem: 31126
2024-05-31T19:33:47 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:49 (1.7426 s / it)
2024-05-31T19:33:52 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T19:33:52 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T19:33:52 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T19:33:52 | INFO | tasks.retrieval_utils : Evaluation time 0:01:55
2024-05-31T19:33:53 | INFO | __main__ : Epoch 9
2024-05-31T19:33:53 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25      0.5        0.27    0.05    0.25      0.5        0.27    0.27     0.00      0.0
test_emb/   29.05   62.60     73.8       55.15   28.05   74.45     83.9       62.13   58.64    49.55     48.7
2024-05-31T19:33:54 | INFO | __main__ : Training time 4:44:15
2024-05-31T19:33:54 | INFO | __main__ : best epoch 0 [config.stop_key test/]
2024-05-31T19:33:54 | INFO | __main__ : Checkpoints and Logs saved at exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm
2024-05-31T19:33:55 | INFO | __main__ : ===========> START eval_after_training [['test']]
2024-05-31T19:33:55 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/datassd1/WebVid/webvid_temporal_2M.json', '/datassd1/WebVid/videos', 'video'], 'test_file': {'test': ['/data2/dy/code/unmasked_teacher/umt_data/anno_downstream/reversed_in_time/test2k.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'rtime': False, 'webvid': True, 'pos_num': 1, 'neg_num': 1, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': False, 'num_frames': 4, 'num_frames_test': 4, 'batch_size': 32, 'max_txt_l': 50, 'evaluate': True, 'zero_shot': False, 'train_shuffle': True, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 4, 'sample_type': 'rand', 'num_frames_test': 4, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 50, 'video': 50}, 'batch_size': {'image': 32, 'video': 32}, 'batch_size_test': {'image': 32, 'video': 32}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT_webvid', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.1, 'num_frames': 4, 'tubelet_size': 1, 'use_checkpoint': False, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 0.0, 'vtm': 0.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 0.0001, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 10, 'min_lr_multi': 0.01, 'warmup_epochs': 1, 'num_training_steps': 19430, 'num_warmup_steps': 1943}, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm/eval_after_training', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 3407, 'save_latest': True, 'auto_resume': True, 'pretrained_path': 'exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm/ckpt_best.pth', 'rank': 0, 'world_size': 4, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl', 'result_dir': 'exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm/eval_after_training'}
2024-05-31T19:33:55 | INFO | __main__ : train_file: ['/datassd1/WebVid/webvid_temporal_2M.json', '/datassd1/WebVid/videos', 'video']
2024-05-31T19:33:55 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-31T19:33:56 | INFO | tasks.shared_utils : Creating model
2024-05-31T19:34:06 | INFO | models.umt : Build vision_encoder: vit_b16
2024-05-31T19:34:06 | INFO | models.backbones.vit.vit : Num of patches: 784
2024-05-31T19:34:06 | INFO | models.backbones.vit.vit : Use checkpoint: False
2024-05-31T19:34:06 | INFO | models.backbones.vit.vit : Checkpoint number: 12
2024-05-31T19:34:06 | INFO | models.backbones.vit.vit : Student return index: []
2024-05-31T19:34:10 | INFO | models.backbones.vit.vit : Loading pretrained weights from /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
2024-05-31T19:34:10 | INFO | models.umt : Build text_encoder bert_base
2024-05-31T19:34:11 | INFO | models.backbones.bert.xbert : build bert with cross_module: ca
2024-05-31T19:34:11 | INFO | models.criterions : Norm type: l2
2024-05-31T19:34:11 | INFO | models.criterions : Loss type: l2
2024-05-31T19:34:12 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.temp: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.embeddings.word_embeddings.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.embeddings.position_embeddings.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.embeddings.token_type_embeddings.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.embeddings.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.embeddings.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.query.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.query.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.key.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.key.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.value.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.value.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.dense.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.vision_proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_proj.weight: wd: 0.02, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : param module.text_proj.bias: wd: 0, lr: 0.0001
2024-05-31T19:34:12 | INFO | utils.optimizer : optimizer -- lr=0.0001 wd=0.02 len(p)=139
2024-05-31T19:34:12 | INFO | utils.optimizer : optimizer -- lr=0.0001 wd=0 len(p)=255
2024-05-31T19:34:12 | INFO | tasks.shared_utils : Auto resuming
2024-05-31T19:34:12 | INFO | tasks.shared_utils : Not found checkpoint in exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm/eval_after_training
2024-05-31T19:34:13 | INFO | tasks.shared_utils : <All keys matched successfully>
2024-05-31T19:34:13 | INFO | tasks.shared_utils : Loaded checkpoint from exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm/ckpt_best.pth
2024-05-31T19:34:13 | INFO | __main__ : Start evaluation
2024-05-31T19:34:13 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-31T19:34:13 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:15 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-31T19:34:26 | INFO | utils.basic_utils : extracting image feats  [ 0/63]  eta: 0:11:44    time: 11.1883  data: 10.9297  max mem: 29150 res mem: 31126
2024-05-31T19:36:04 | INFO | utils.basic_utils : extracting image feats  [62/63]  eta: 0:00:01    time: 1.4499  data: 1.2278  max mem: 29150 res mem: 31126
2024-05-31T19:36:04 | INFO | utils.basic_utils : extracting image feats Total time: 0:01:49 (1.7440 s / it)
2024-05-31T19:36:09 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-31T19:36:09 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-31T19:36:09 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-31T19:36:09 | INFO | tasks.retrieval_utils : Evaluation time 0:01:55
2024-05-31T19:36:10 | INFO | __main__ : Epoch 0
2024-05-31T19:36:10 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        0.05    0.25     0.50        0.27    0.05    0.25     0.50        0.27    0.27     0.00     0.00
test_emb/   26.00   64.25    74.25       54.83   24.50   65.35    76.45       55.43   55.13    49.65    48.85
2024-05-31T19:36:10 | INFO | __main__ : Training time 0:01:56
2024-05-31T19:36:10 | INFO | __main__ : best epoch 0 [config.stop_key test/]
2024-05-31T19:36:10 | INFO | __main__ : Checkpoints and Logs saved at exp/finetuning/ret_rtime_vlp/webvid_without_rewrite_1pos1neg_novtm/eval_after_training
