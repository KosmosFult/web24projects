2024-05-27T07:44:26 | INFO | umt : Logging to: exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg/train.log
2024-05-27T07:44:26 | INFO | utils.config_utils : config: {
  data_dir: /data2/dy/code/unmasked_teacher/umt_data
  data_root: /data2/dy/code/unmasked_teacher/umt_data/videos_images
  anno_root_pt: /data2/dy/code/unmasked_teacher/umt_data/anno_pretrain
  anno_root_downstream: /data2/dy/code/unmasked_teacher/umt_data/anno_downstream
  TextEncoders: {
      bert: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      bert_large: {
          name: bert_large
          pretrained: bert-large-uncased
          config: configs/config_bert_large.json
          d_model: 1024
          fusion_layer: 19 } }
  train_file: ['/data2/dy/temporal_video/dataset/without_webvid_annotations/VLM_split/allreverse_rewrite.json', '/data2/dy/temporal_video/dataset', 'video']
  test_file: {
      test: ['/data2/dy/temporal_video/dataset/without_webvid_annotations/test_reverse.json', '/data2/dy/temporal_video/dataset', 'video'] }
  test_types: ['test']
  num_workers: 6
  rtime: True
  pos_num: 1
  neg_num: 10
  stop_key: test/
  is_paragraph_retrieval: False
  is_pretrain: False
  num_frames: 12
  num_frames_test: 12
  batch_size: 16
  max_txt_l: 50
  evaluate: False
  train_shuffle: True
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 12
          sample_type: rand
          num_frames_test: 12
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 50
          video: 50 }
      batch_size: {
          image: 16
          video: 16 }
      batch_size_test: {
          image: 16
          video: 16 } }
  text_enc: bert
  model: {
      model_cls: UMT_temporal
      vision_encoder: {
          name: vit_b16
          img_size: 224
          patch_size: 16
          d_model: 768
          encoder_embed_dim: 768
          encoder_depth: 12
          encoder_num_heads: 12
          drop_path_rate: 0.2
          num_frames: 12
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 12
          clip_decoder_embed_dim: 768
          clip_output_dim: 512
          clip_return_layer: 0
          clip_student_return_interval: 1
          pretrained: /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
          clip_teacher: none
          clip_img_size: 224
          clip_return_interval: 1
          video_mask_type: attention
          video_mask_ratio: 0.0
          video_double_mask_ratio: 0.0
          image_mask_type: attention
          image_mask_ratio: 0.0
          image_double_mask_ratio: 0.0
          keep_temporal: True }
      text_encoder: {
          name: bert_base
          pretrained: bert-base-uncased
          config: configs/config_bert.json
          d_model: 768
          fusion_layer: 9 }
      multimodal: {
          enable: True }
      embed_dim: 512
      temp: 0.07 }
  criterion: {
      loss_weight: {
          vtc: 1.0
          mlm: 0.0
          vtm: 1.0
          uta: 0.0 }
      vtm_hard_neg: True
      mlm_masking_prob: 0.5
      uta_norm_type: l2
      uta_loss_type: l2 }
  optimizer: {
      opt: adamW
      lr: 1e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 5
      min_lr_multi: 0.01
      warmup_epochs: 1 }
  zero_shot: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: user
      project: umt_ret }
  dist_url: env://
  device: cuda
  mode: pt
  output_dir: exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg
  resume: False
  debug: False
  log_freq: 100
  seed: 3407
  save_latest: True
  auto_resume: True
  pretrained_path: /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
  rank: 0
  world_size: 2
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-05-27T07:44:26 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/data2/dy/temporal_video/dataset/without_webvid_annotations/VLM_split/allreverse_rewrite.json', '/data2/dy/temporal_video/dataset', 'video'], 'test_file': {'test': ['/data2/dy/temporal_video/dataset/without_webvid_annotations/test_reverse.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'rtime': True, 'pos_num': 1, 'neg_num': 10, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': False, 'num_frames': 12, 'num_frames_test': 12, 'batch_size': 16, 'max_txt_l': 50, 'evaluate': False, 'train_shuffle': True, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 12, 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 50, 'video': 50}, 'batch_size': {'image': 16, 'video': 16}, 'batch_size_test': {'image': 16, 'video': 16}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT_temporal', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.2, 'num_frames': 12, 'tubelet_size': 1, 'use_checkpoint': True, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 0.0, 'vtm': 1.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 5, 'min_lr_multi': 0.01, 'warmup_epochs': 1}, 'zero_shot': False, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 3407, 'save_latest': True, 'auto_resume': True, 'pretrained_path': '/datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth', 'rank': 0, 'world_size': 2, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl'}
2024-05-27T07:44:26 | INFO | __main__ : train_file: ['/data2/dy/temporal_video/dataset/without_webvid_annotations/VLM_split/allreverse_rewrite.json', '/data2/dy/temporal_video/dataset', 'video']
2024-05-27T07:44:26 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-27T07:44:27 | INFO | tasks.shared_utils : Creating model
2024-05-27T07:44:37 | INFO | models.umt : Build vision_encoder: vit_b16
2024-05-27T07:44:37 | INFO | models.backbones.vit.vit : Num of patches: 2352
2024-05-27T07:44:37 | INFO | models.backbones.vit.vit : Use checkpoint: True
2024-05-27T07:44:37 | INFO | models.backbones.vit.vit : Checkpoint number: 12
2024-05-27T07:44:37 | INFO | models.backbones.vit.vit : Student return index: []
2024-05-27T07:44:48 | INFO | models.backbones.vit.vit : Loading pretrained weights from /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
2024-05-27T07:44:48 | INFO | models.umt : Build text_encoder bert_base
2024-05-27T07:44:50 | INFO | models.backbones.bert.xbert : build bert with cross_module: ca
2024-05-27T07:44:50 | INFO | models.criterions : Norm type: l2
2024-05-27T07:44:50 | INFO | models.criterions : Loss type: l2
2024-05-27T07:44:50 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.temp: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.embeddings.word_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.embeddings.position_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.embeddings.token_type_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.embeddings.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.embeddings.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.vision_proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.text_proj.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.itm_head.weight: wd: 0.02, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : param module.itm_head.bias: wd: 0, lr: 1e-05
2024-05-27T07:44:50 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0.02 len(p)=140
2024-05-27T07:44:50 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0 len(p)=256
2024-05-27T07:44:50 | INFO | tasks.shared_utils : Auto resuming
2024-05-27T07:44:50 | INFO | tasks.shared_utils : Not found checkpoint in exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg
2024-05-27T07:44:51 | INFO | tasks.shared_utils : _IncompatibleKeys(missing_keys=[], unexpected_keys=['clip_teacher.class_embedding', 'clip_teacher.positional_embedding', 'clip_teacher.proj', 'clip_teacher.conv1.weight', 'clip_teacher.ln_pre.weight', 'clip_teacher.ln_pre.bias', 'clip_teacher.transformer.resblocks.0.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.0.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.0.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.0.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.0.ln_1.weight', 'clip_teacher.transformer.resblocks.0.ln_1.bias', 'clip_teacher.transformer.resblocks.0.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.0.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.0.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.0.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.0.ln_2.weight', 'clip_teacher.transformer.resblocks.0.ln_2.bias', 'clip_teacher.transformer.resblocks.1.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.1.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.1.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.1.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.1.ln_1.weight', 'clip_teacher.transformer.resblocks.1.ln_1.bias', 'clip_teacher.transformer.resblocks.1.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.1.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.1.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.1.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.1.ln_2.weight', 'clip_teacher.transformer.resblocks.1.ln_2.bias', 'clip_teacher.transformer.resblocks.2.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.2.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.2.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.2.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.2.ln_1.weight', 'clip_teacher.transformer.resblocks.2.ln_1.bias', 'clip_teacher.transformer.resblocks.2.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.2.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.2.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.2.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.2.ln_2.weight', 'clip_teacher.transformer.resblocks.2.ln_2.bias', 'clip_teacher.transformer.resblocks.3.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.3.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.3.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.3.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.3.ln_1.weight', 'clip_teacher.transformer.resblocks.3.ln_1.bias', 'clip_teacher.transformer.resblocks.3.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.3.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.3.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.3.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.3.ln_2.weight', 'clip_teacher.transformer.resblocks.3.ln_2.bias', 'clip_teacher.transformer.resblocks.4.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.4.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.4.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.4.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.4.ln_1.weight', 'clip_teacher.transformer.resblocks.4.ln_1.bias', 'clip_teacher.transformer.resblocks.4.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.4.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.4.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.4.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.4.ln_2.weight', 'clip_teacher.transformer.resblocks.4.ln_2.bias', 'clip_teacher.transformer.resblocks.5.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.5.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.5.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.5.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.5.ln_1.weight', 'clip_teacher.transformer.resblocks.5.ln_1.bias', 'clip_teacher.transformer.resblocks.5.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.5.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.5.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.5.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.5.ln_2.weight', 'clip_teacher.transformer.resblocks.5.ln_2.bias', 'clip_teacher.transformer.resblocks.6.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.6.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.6.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.6.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.6.ln_1.weight', 'clip_teacher.transformer.resblocks.6.ln_1.bias', 'clip_teacher.transformer.resblocks.6.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.6.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.6.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.6.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.6.ln_2.weight', 'clip_teacher.transformer.resblocks.6.ln_2.bias', 'clip_teacher.transformer.resblocks.7.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.7.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.7.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.7.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.7.ln_1.weight', 'clip_teacher.transformer.resblocks.7.ln_1.bias', 'clip_teacher.transformer.resblocks.7.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.7.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.7.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.7.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.7.ln_2.weight', 'clip_teacher.transformer.resblocks.7.ln_2.bias', 'clip_teacher.transformer.resblocks.8.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.8.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.8.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.8.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.8.ln_1.weight', 'clip_teacher.transformer.resblocks.8.ln_1.bias', 'clip_teacher.transformer.resblocks.8.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.8.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.8.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.8.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.8.ln_2.weight', 'clip_teacher.transformer.resblocks.8.ln_2.bias', 'clip_teacher.transformer.resblocks.9.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.9.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.9.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.9.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.9.ln_1.weight', 'clip_teacher.transformer.resblocks.9.ln_1.bias', 'clip_teacher.transformer.resblocks.9.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.9.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.9.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.9.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.9.ln_2.weight', 'clip_teacher.transformer.resblocks.9.ln_2.bias', 'clip_teacher.transformer.resblocks.10.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.10.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.10.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.10.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.10.ln_1.weight', 'clip_teacher.transformer.resblocks.10.ln_1.bias', 'clip_teacher.transformer.resblocks.10.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.10.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.10.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.10.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.10.ln_2.weight', 'clip_teacher.transformer.resblocks.10.ln_2.bias', 'clip_teacher.transformer.resblocks.11.attn.in_proj_weight', 'clip_teacher.transformer.resblocks.11.attn.in_proj_bias', 'clip_teacher.transformer.resblocks.11.attn.out_proj.weight', 'clip_teacher.transformer.resblocks.11.attn.out_proj.bias', 'clip_teacher.transformer.resblocks.11.ln_1.weight', 'clip_teacher.transformer.resblocks.11.ln_1.bias', 'clip_teacher.transformer.resblocks.11.mlp.c_fc.weight', 'clip_teacher.transformer.resblocks.11.mlp.c_fc.bias', 'clip_teacher.transformer.resblocks.11.mlp.c_proj.weight', 'clip_teacher.transformer.resblocks.11.mlp.c_proj.bias', 'clip_teacher.transformer.resblocks.11.ln_2.weight', 'clip_teacher.transformer.resblocks.11.ln_2.bias', 'clip_teacher.ln_post.weight', 'clip_teacher.ln_post.bias', 'vision_encoder.clip_decoder.0.head.weight', 'vision_encoder.clip_decoder.0.head.bias', 'vision_encoder.clip_decoder.0.norm.weight', 'vision_encoder.clip_decoder.0.norm.bias', 'vision_encoder.clip_decoder.1.head.weight', 'vision_encoder.clip_decoder.1.head.bias', 'vision_encoder.clip_decoder.1.norm.weight', 'vision_encoder.clip_decoder.1.norm.bias', 'vision_encoder.clip_decoder.2.head.weight', 'vision_encoder.clip_decoder.2.head.bias', 'vision_encoder.clip_decoder.2.norm.weight', 'vision_encoder.clip_decoder.2.norm.bias', 'vision_encoder.clip_decoder.3.head.weight', 'vision_encoder.clip_decoder.3.head.bias', 'vision_encoder.clip_decoder.3.norm.weight', 'vision_encoder.clip_decoder.3.norm.bias', 'vision_encoder.clip_decoder.4.head.weight', 'vision_encoder.clip_decoder.4.head.bias', 'vision_encoder.clip_decoder.4.norm.weight', 'vision_encoder.clip_decoder.4.norm.bias', 'vision_encoder.clip_decoder.5.head.weight', 'vision_encoder.clip_decoder.5.head.bias', 'vision_encoder.clip_decoder.5.norm.weight', 'vision_encoder.clip_decoder.5.norm.bias', 'text_encoder.cls.predictions.bias', 'text_encoder.cls.predictions.transform.dense.weight', 'text_encoder.cls.predictions.transform.dense.bias', 'text_encoder.cls.predictions.transform.LayerNorm.weight', 'text_encoder.cls.predictions.transform.LayerNorm.bias', 'text_encoder.cls.predictions.decoder.weight', 'text_encoder.cls.predictions.decoder.bias'])
2024-05-27T07:44:51 | INFO | tasks.shared_utils : Loaded checkpoint from /datassd2/pretrained_models/Unmasked_Teacher/multimodality/b16_25m.pth
2024-05-27T07:44:51 | INFO | __main__ : training
2024-05-27T07:44:52 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 211 batches in total
dataloader index=0 name=video, batch-size=16 length(#batches)=211 
2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:44:52 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T07:45:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T07:45:12 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T07:45:17 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

2024-05-27T07:45:17 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  builtin_warn(*args, **kwargs)

2024-05-27T07:45:17 | INFO | utils.basic_utils : Train Epoch: [0]  [  0/211]  eta: 1:29:07  lr: 0.000000  temperature: 0.0112  video-loss_vtc: 10.0596  video-loss_vtm: 1.8503  time: 25.3459  data: 19.3097  max mem: 33274 res mem: 39980
2024-05-27T07:52:32 | INFO | utils.basic_utils : Train Epoch: [0]  [100/211]  eta: 0:08:25  lr: 0.000005  temperature: 0.0113  video-loss_vtc: 8.7403  video-loss_vtm: 0.7346  time: 4.3979  data: 0.0063  max mem: 34822 res mem: 40092
2024-05-27T07:59:46 | INFO | utils.basic_utils : Train Epoch: [0]  [200/211]  eta: 0:00:48  lr: 0.000010  temperature: 0.0115  video-loss_vtc: 8.3601  video-loss_vtm: 0.6575  time: 4.3342  data: 0.0068  max mem: 34823 res mem: 40092
2024-05-27T08:00:29 | INFO | utils.basic_utils : Train Epoch: [0]  [210/211]  eta: 0:00:04  lr: 0.000010  temperature: 0.0115  video-loss_vtc: 8.3043  video-loss_vtm: 0.6488  time: 4.3211  data: 0.0043  max mem: 34823 res mem: 40092
2024-05-27T08:00:29 | INFO | utils.basic_utils : Train Epoch: [0] Total time: 0:15:37 (4.4414 s / it)
2024-05-27T08:00:29 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0113  video-loss_vtc: 9.1797  video-loss_vtm: 0.8943
2024-05-27T08:00:29 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-27T08:00:29 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-27T08:00:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:31 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:41 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:41 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T08:00:42 | INFO | utils.basic_utils : extracting image feats  [  0/125]  eta: 0:25:06    time: 12.0535  data: 10.9317  max mem: 34823 res mem: 40092
2024-05-27T08:03:39 | INFO | utils.basic_utils : extracting image feats  [100/125]  eta: 0:00:46    time: 1.6069  data: 1.0420  max mem: 34823 res mem: 40092
2024-05-27T08:04:21 | INFO | utils.basic_utils : extracting image feats  [124/125]  eta: 0:00:01    time: 1.6474  data: 1.0840  max mem: 34823 res mem: 40092
2024-05-27T08:04:21 | INFO | utils.basic_utils : extracting image feats Total time: 0:03:50 (1.8443 s / it)
2024-05-27T08:04:36 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-27T08:04:36 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-27T08:04:36 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-27T08:04:36 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-27T08:04:36 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([1001, 2000])
2024-05-27T08:04:36 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-27T08:04:36 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:00:56    time: 0.0560  data: 0.0019  max mem: 34823 res mem: 40092
2024-05-27T08:04:44 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:01:15    time: 0.0844  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:04:53 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:01:07    time: 0.0851  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:05:01 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:00:59    time: 0.0853  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:05:10 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:00:51    time: 0.0852  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:05:18 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:00:42    time: 0.0860  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:05:27 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:00:34    time: 0.0858  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:05:36 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:00:25    time: 0.0855  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:05:44 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:00:17    time: 0.0858  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:05:53 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:00:08    time: 0.0855  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:06:01 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:00    time: 0.0860  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:06:01 | INFO | utils.basic_utils : Evaluation: Total time: 0:01:25 (0.0855 s / it)
2024-05-27T08:06:01 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([1001, 2000])
2024-05-27T08:06:03 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:20:23    time: 1.2218  data: 0.0020  max mem: 34823 res mem: 40092
2024-05-27T08:07:54 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:16:40    time: 1.0987  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:09:45 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:14:52    time: 1.1340  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:11:37 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:13:02    time: 1.1112  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:13:30 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:11:13    time: 1.1683  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:15:25 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:09:23    time: 1.1315  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:17:19 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:07:31    time: 1.1473  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:19:11 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:05:38    time: 1.0977  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:21:04 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:03:46    time: 1.1829  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:22:59 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:01:54    time: 1.1179  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:24:52 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:01    time: 1.1089  data: 0.0000  max mem: 34823 res mem: 40092
2024-05-27T08:24:52 | INFO | utils.basic_utils : Evaluation: Total time: 0:18:50 (1.1293 s / it)
2024-05-27T08:24:52 | INFO | tasks.retrieval_utils : Evaluation time 0:24:22
2024-05-27T08:24:54 | INFO | __main__ : Epoch 0
2024-05-27T08:24:54 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        3.45   12.50    18.90       11.62    5.30   15.75     22.7       14.58   13.10    50.45    50.45
test_emb/   36.30   85.65    92.45       71.47   34.15   84.45     92.8       70.47   70.97    50.00    48.00
2024-05-27T08:24:57 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 211 batches in total
dataloader index=0 name=video, batch-size=16 length(#batches)=211 
2024-05-27T08:25:18 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T08:25:18 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T08:25:22 | INFO | utils.basic_utils : Train Epoch: [1]  [  0/211]  eta: 1:26:21  lr: 0.000010  temperature: 0.0115  video-loss_vtc: 8.5163  video-loss_vtm: 0.6474  time: 24.5576  data: 20.2658  max mem: 34823 res mem: 40092
2024-05-27T08:32:35 | INFO | utils.basic_utils : Train Epoch: [1]  [100/211]  eta: 0:08:23  lr: 0.000010  temperature: 0.0116  video-loss_vtc: 7.9095  video-loss_vtm: 0.6319  time: 4.3448  data: 0.0051  max mem: 34823 res mem: 40092
2024-05-27T08:39:52 | INFO | utils.basic_utils : Train Epoch: [1]  [200/211]  eta: 0:00:48  lr: 0.000009  temperature: 0.0117  video-loss_vtc: 7.8052  video-loss_vtm: 0.6419  time: 4.3559  data: 0.0091  max mem: 34824 res mem: 40092
2024-05-27T08:40:34 | INFO | utils.basic_utils : Train Epoch: [1]  [210/211]  eta: 0:00:04  lr: 0.000009  temperature: 0.0117  video-loss_vtc: 8.2448  video-loss_vtm: 0.6369  time: 4.3250  data: 0.0062  max mem: 34824 res mem: 40092
2024-05-27T08:40:34 | INFO | utils.basic_utils : Train Epoch: [1] Total time: 0:15:37 (4.4414 s / it)
2024-05-27T08:40:35 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0116  video-loss_vtc: 8.1103  video-loss_vtm: 0.6404
2024-05-27T08:40:35 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-27T08:40:35 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-27T08:40:46 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T08:40:46 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T08:40:46 | INFO | utils.basic_utils : extracting image feats  [  0/125]  eta: 0:21:22    time: 10.2617  data: 9.6590  max mem: 34824 res mem: 40092
2024-05-27T08:43:48 | INFO | utils.basic_utils : extracting image feats  [100/125]  eta: 0:00:47    time: 1.8643  data: 1.2842  max mem: 34824 res mem: 40092
2024-05-27T08:44:35 | INFO | utils.basic_utils : extracting image feats  [124/125]  eta: 0:00:01    time: 1.9650  data: 1.3854  max mem: 34824 res mem: 40092
2024-05-27T08:44:35 | INFO | utils.basic_utils : extracting image feats Total time: 0:03:59 (1.9170 s / it)
2024-05-27T08:44:52 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-27T08:44:52 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-27T08:44:52 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-27T08:44:52 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-27T08:44:52 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([1001, 2000])
2024-05-27T08:44:52 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-27T08:44:52 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:00:39    time: 0.0391  data: 0.0027  max mem: 34824 res mem: 40092
2024-05-27T08:45:00 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:01:16    time: 0.0847  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:45:09 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:01:08    time: 0.0855  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:45:17 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:00:59    time: 0.0859  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:45:26 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:00:51    time: 0.0859  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:45:35 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:00:42    time: 0.0858  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:45:43 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:00:34    time: 0.0863  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:45:52 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:00:25    time: 0.0866  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:46:00 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:00:17    time: 0.0864  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:46:09 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:00:08    time: 0.0868  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:46:18 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:00    time: 0.0862  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:46:18 | INFO | utils.basic_utils : Evaluation: Total time: 0:01:26 (0.0859 s / it)
2024-05-27T08:46:18 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([1001, 2000])
2024-05-27T08:46:19 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:21:31    time: 1.2905  data: 0.0024  max mem: 34824 res mem: 40092
2024-05-27T08:48:16 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:17:32    time: 1.1481  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:50:13 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:15:37    time: 1.1567  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:52:09 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:13:37    time: 1.1726  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:54:05 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:11:39    time: 1.1491  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:56:00 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:09:41    time: 1.1369  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:57:57 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:07:46    time: 1.1899  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T08:59:54 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:05:50    time: 1.1323  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:01:47 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:03:53    time: 1.1536  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:03:40 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:01:56    time: 1.1120  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:05:34 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:01    time: 1.1511  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:05:34 | INFO | utils.basic_utils : Evaluation: Total time: 0:19:16 (1.1552 s / it)
2024-05-27T09:05:34 | INFO | tasks.retrieval_utils : Evaluation time 0:24:59
2024-05-27T09:05:36 | INFO | __main__ : Epoch 1
2024-05-27T09:05:36 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        4.40   16.75     26.0       15.72    7.30   23.80    33.55       21.55   18.63    51.55    51.05
test_emb/   37.45   87.75     94.4       73.20   37.05   86.55    94.55       72.72   72.96    50.20    50.30
2024-05-27T09:05:52 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 211 batches in total
dataloader index=0 name=video, batch-size=16 length(#batches)=211 
2024-05-27T09:06:13 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T09:06:13 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T09:06:17 | INFO | utils.basic_utils : Train Epoch: [2]  [  0/211]  eta: 1:25:36  lr: 0.000009  temperature: 0.0117  video-loss_vtc: 7.7055  video-loss_vtm: 0.6349  time: 24.3422  data: 20.1407  max mem: 34824 res mem: 40092
2024-05-27T09:13:33 | INFO | utils.basic_utils : Train Epoch: [2]  [100/211]  eta: 0:08:26  lr: 0.000007  temperature: 0.0118  video-loss_vtc: 7.7113  video-loss_vtm: 0.6433  time: 4.3581  data: 0.0107  max mem: 34824 res mem: 40092
2024-05-27T09:20:50 | INFO | utils.basic_utils : Train Epoch: [2]  [200/211]  eta: 0:00:49  lr: 0.000005  temperature: 0.0118  video-loss_vtc: 7.9860  video-loss_vtm: 0.6403  time: 4.3691  data: 0.0124  max mem: 34824 res mem: 40092
2024-05-27T09:21:33 | INFO | utils.basic_utils : Train Epoch: [2]  [210/211]  eta: 0:00:04  lr: 0.000005  temperature: 0.0118  video-loss_vtc: 7.8544  video-loss_vtm: 0.6345  time: 4.3323  data: 0.0078  max mem: 34824 res mem: 40092
2024-05-27T09:21:33 | INFO | utils.basic_utils : Train Epoch: [2] Total time: 0:15:40 (4.4557 s / it)
2024-05-27T09:21:33 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0118  video-loss_vtc: 7.8898  video-loss_vtm: 0.6375
2024-05-27T09:21:33 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-27T09:21:33 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-27T09:21:45 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T09:21:45 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T09:21:45 | INFO | utils.basic_utils : extracting image feats  [  0/125]  eta: 0:23:20    time: 11.2055  data: 10.6079  max mem: 34824 res mem: 40092
2024-05-27T09:24:50 | INFO | utils.basic_utils : extracting image feats  [100/125]  eta: 0:00:48    time: 1.8435  data: 1.2698  max mem: 34824 res mem: 40092
2024-05-27T09:25:31 | INFO | utils.basic_utils : extracting image feats  [124/125]  eta: 0:00:01    time: 1.6242  data: 1.0566  max mem: 34824 res mem: 40092
2024-05-27T09:25:31 | INFO | utils.basic_utils : extracting image feats Total time: 0:03:56 (1.8922 s / it)
2024-05-27T09:25:46 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-27T09:25:46 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-27T09:25:46 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-27T09:25:46 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-27T09:25:46 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([1001, 2000])
2024-05-27T09:25:46 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-27T09:25:46 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:00:37    time: 0.0371  data: 0.0024  max mem: 34824 res mem: 40092
2024-05-27T09:25:54 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:01:15    time: 0.0847  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:26:03 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:01:07    time: 0.0846  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:26:11 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:00:59    time: 0.0851  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:26:20 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:00:51    time: 0.0857  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:26:29 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:00:42    time: 0.0854  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:26:37 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:00:34    time: 0.0855  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:26:46 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:00:25    time: 0.0859  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:26:54 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:00:17    time: 0.0864  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:27:03 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:00:08    time: 0.0858  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:27:12 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:00    time: 0.0864  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:27:12 | INFO | utils.basic_utils : Evaluation: Total time: 0:01:25 (0.0856 s / it)
2024-05-27T09:27:12 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([1001, 2000])
2024-05-27T09:27:13 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:21:16    time: 1.2752  data: 0.0023  max mem: 34824 res mem: 40092
2024-05-27T09:29:12 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:17:49    time: 1.1659  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:31:05 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:15:29    time: 1.1428  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:32:58 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:13:27    time: 1.1257  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:34:50 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:11:27    time: 1.1023  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:36:44 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:09:32    time: 1.1485  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:38:38 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:07:38    time: 1.1452  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:40:32 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:05:43    time: 1.1334  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:42:27 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:03:49    time: 1.1783  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:44:25 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:01:55    time: 1.1478  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:46:19 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:01    time: 1.1252  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T09:46:19 | INFO | utils.basic_utils : Evaluation: Total time: 0:19:07 (1.1467 s / it)
2024-05-27T09:46:19 | INFO | tasks.retrieval_utils : Evaluation time 0:24:46
2024-05-27T09:46:21 | INFO | __main__ : Epoch 2
2024-05-27T09:46:21 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/        9.75   31.35     44.0       28.37    14.5   42.15    54.25       36.97   32.67    50.90     52.5
test_emb/   38.75   88.05     94.6       73.80    37.4   87.75    94.95       73.37   73.58    50.45     50.5
2024-05-27T09:46:37 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 211 batches in total
dataloader index=0 name=video, batch-size=16 length(#batches)=211 
2024-05-27T09:46:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T09:46:57 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T09:47:02 | INFO | utils.basic_utils : Train Epoch: [3]  [  0/211]  eta: 1:26:26  lr: 0.000005  temperature: 0.0118  video-loss_vtc: 7.7284  video-loss_vtm: 0.6395  time: 24.5820  data: 19.7721  max mem: 34824 res mem: 40092
2024-05-27T09:54:17 | INFO | utils.basic_utils : Train Epoch: [3]  [100/211]  eta: 0:08:25  lr: 0.000003  temperature: 0.0119  video-loss_vtc: 7.7218  video-loss_vtm: 0.6314  time: 4.3667  data: 0.0076  max mem: 34824 res mem: 40092
2024-05-27T10:01:32 | INFO | utils.basic_utils : Train Epoch: [3]  [200/211]  eta: 0:00:48  lr: 0.000002  temperature: 0.0119  video-loss_vtc: 7.6518  video-loss_vtm: 0.6414  time: 4.3421  data: 0.0055  max mem: 34824 res mem: 40092
2024-05-27T10:02:15 | INFO | utils.basic_utils : Train Epoch: [3]  [210/211]  eta: 0:00:04  lr: 0.000001  temperature: 0.0119  video-loss_vtc: 7.8920  video-loss_vtm: 0.6389  time: 4.3250  data: 0.0048  max mem: 34824 res mem: 40092
2024-05-27T10:02:15 | INFO | utils.basic_utils : Train Epoch: [3] Total time: 0:15:37 (4.4452 s / it)
2024-05-27T10:02:15 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0119  video-loss_vtc: 7.7661  video-loss_vtm: 0.6375
2024-05-27T10:02:15 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-27T10:02:15 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-27T10:02:27 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T10:02:27 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T10:02:28 | INFO | utils.basic_utils : extracting image feats  [  0/125]  eta: 0:22:57    time: 11.0167  data: 10.4177  max mem: 34824 res mem: 40092
2024-05-27T10:05:27 | INFO | utils.basic_utils : extracting image feats  [100/125]  eta: 0:00:47    time: 1.8149  data: 1.2518  max mem: 34824 res mem: 40092
2024-05-27T10:06:06 | INFO | utils.basic_utils : extracting image feats  [124/125]  eta: 0:00:01    time: 1.5481  data: 0.9902  max mem: 34824 res mem: 40092
2024-05-27T10:06:06 | INFO | utils.basic_utils : extracting image feats Total time: 0:03:49 (1.8364 s / it)
2024-05-27T10:06:22 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-27T10:06:22 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-27T10:06:22 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-27T10:06:22 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-27T10:06:22 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([1001, 2000])
2024-05-27T10:06:22 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-27T10:06:22 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:00:35    time: 0.0352  data: 0.0025  max mem: 34824 res mem: 40092
2024-05-27T10:06:30 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:01:15    time: 0.0849  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:06:39 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:01:07    time: 0.0844  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:06:47 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:00:59    time: 0.0852  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:06:56 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:00:50    time: 0.0853  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:07:04 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:00:42    time: 0.0855  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:07:13 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:00:34    time: 0.0855  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:07:21 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:00:25    time: 0.0860  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:07:30 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:00:17    time: 0.0862  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:07:38 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:00:08    time: 0.0857  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:07:47 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:00    time: 0.0858  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:07:47 | INFO | utils.basic_utils : Evaluation: Total time: 0:01:25 (0.0853 s / it)
2024-05-27T10:07:47 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([1001, 2000])
2024-05-27T10:07:48 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:19:47    time: 1.1868  data: 0.0017  max mem: 34824 res mem: 40092
2024-05-27T10:09:42 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:17:04    time: 1.1317  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:11:34 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:15:05    time: 1.1390  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:13:26 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:13:09    time: 1.1098  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:15:16 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:11:12    time: 1.1118  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:17:08 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:09:21    time: 1.1235  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:18:59 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:07:27    time: 1.0861  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:20:48 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:05:35    time: 1.0861  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:22:38 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:03:43    time: 1.1105  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:24:29 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:01:52    time: 1.1302  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:26:21 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:01    time: 1.1057  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:26:21 | INFO | utils.basic_utils : Evaluation: Total time: 0:18:33 (1.1128 s / it)
2024-05-27T10:26:21 | INFO | tasks.retrieval_utils : Evaluation time 0:24:05
2024-05-27T10:26:23 | INFO | __main__ : Epoch 3
2024-05-27T10:26:23 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       10.65   33.75    46.95       30.45    16.0   44.70    57.75       39.48   34.97     51.1    53.05
test_emb/   38.85   88.65    95.05       74.18    37.4   87.95    95.30       73.55   73.87     50.4    50.25
2024-05-27T10:26:40 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 211 batches in total
dataloader index=0 name=video, batch-size=16 length(#batches)=211 
2024-05-27T10:27:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T10:27:01 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T10:27:06 | INFO | utils.basic_utils : Train Epoch: [4]  [  0/211]  eta: 1:31:28  lr: 0.000001  temperature: 0.0119  video-loss_vtc: 7.6064  video-loss_vtm: 0.6378  time: 26.0141  data: 20.6294  max mem: 34824 res mem: 40092
2024-05-27T10:34:20 | INFO | utils.basic_utils : Train Epoch: [4]  [100/211]  eta: 0:08:25  lr: 0.000000  temperature: 0.0119  video-loss_vtc: 7.8901  video-loss_vtm: 0.6392  time: 4.3527  data: 0.0048  max mem: 34824 res mem: 40092
2024-05-27T10:41:34 | INFO | utils.basic_utils : Train Epoch: [4]  [200/211]  eta: 0:00:48  lr: 0.000000  temperature: 0.0119  video-loss_vtc: 7.5988  video-loss_vtm: 0.6388  time: 4.3593  data: 0.0110  max mem: 34824 res mem: 40092
2024-05-27T10:42:17 | INFO | utils.basic_utils : Train Epoch: [4]  [210/211]  eta: 0:00:04  lr: 0.000000  temperature: 0.0119  video-loss_vtc: 7.5356  video-loss_vtm: 0.6349  time: 4.3313  data: 0.0087  max mem: 34824 res mem: 40092
2024-05-27T10:42:17 | INFO | utils.basic_utils : Train Epoch: [4] Total time: 0:15:37 (4.4427 s / it)
2024-05-27T10:42:17 | INFO | __main__ : Averaged train stats: lr: 0.0000  temperature: 0.0119  video-loss_vtc: 7.7564  video-loss_vtm: 0.6372
2024-05-27T10:42:17 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-27T10:42:17 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-27T10:42:28 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T10:42:28 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T10:42:29 | INFO | utils.basic_utils : extracting image feats  [  0/125]  eta: 0:21:34    time: 10.3525  data: 9.7692  max mem: 34824 res mem: 40092
2024-05-27T10:45:24 | INFO | utils.basic_utils : extracting image feats  [100/125]  eta: 0:00:45    time: 1.6709  data: 1.1099  max mem: 34824 res mem: 40092
2024-05-27T10:46:05 | INFO | utils.basic_utils : extracting image feats  [124/125]  eta: 0:00:01    time: 1.6486  data: 1.0869  max mem: 34824 res mem: 40092
2024-05-27T10:46:05 | INFO | utils.basic_utils : extracting image feats Total time: 0:03:46 (1.8153 s / it)
2024-05-27T10:46:20 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-27T10:46:20 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-27T10:46:20 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-27T10:46:20 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-27T10:46:20 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([1001, 2000])
2024-05-27T10:46:20 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-27T10:46:20 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:00:36    time: 0.0368  data: 0.0023  max mem: 34824 res mem: 40092
2024-05-27T10:46:29 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:01:15    time: 0.0844  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:46:37 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:01:07    time: 0.0848  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:46:46 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:00:59    time: 0.0849  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:46:54 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:00:50    time: 0.0852  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:47:03 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:00:42    time: 0.0848  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:47:11 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:00:34    time: 0.0856  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:47:20 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:00:25    time: 0.0858  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:47:28 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:00:17    time: 0.0860  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:47:37 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:00:08    time: 0.0860  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:47:46 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:00    time: 0.0859  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:47:46 | INFO | utils.basic_utils : Evaluation: Total time: 0:01:25 (0.0853 s / it)
2024-05-27T10:47:46 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([1001, 2000])
2024-05-27T10:47:47 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:22:00    time: 1.3192  data: 0.0022  max mem: 34824 res mem: 40092
2024-05-27T10:49:42 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:17:17    time: 1.1275  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:51:35 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:15:15    time: 1.1244  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:53:29 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:13:19    time: 1.1639  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:55:23 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:11:25    time: 1.1161  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:57:15 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:09:29    time: 1.1286  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T10:59:08 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:07:35    time: 1.1157  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:01:01 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:05:41    time: 1.1247  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:02:54 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:03:47    time: 1.1321  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:04:46 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:01:54    time: 1.1280  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:06:42 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:01    time: 1.1559  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:06:42 | INFO | utils.basic_utils : Evaluation: Total time: 0:18:55 (1.1348 s / it)
2024-05-27T11:06:42 | INFO | tasks.retrieval_utils : Evaluation time 0:24:24
2024-05-27T11:06:43 | INFO | __main__ : Epoch 4
2024-05-27T11:06:43 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       11.65   35.65    49.55       32.28    17.3   48.05    61.30       42.22   37.25    50.90    53.05
test_emb/   38.95   89.00    95.30       74.42    37.5   88.20    95.25       73.65   74.03    50.55    50.10
2024-05-27T11:07:01 | INFO | __main__ : Training time 3:22:09
2024-05-27T11:07:01 | INFO | __main__ : best epoch 4 [config.stop_key test/]
2024-05-27T11:07:01 | INFO | __main__ : Checkpoints and Logs saved at exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg
2024-05-27T11:07:02 | INFO | __main__ : ===========> START eval_after_training [['test']]
2024-05-27T11:07:02 | INFO | __main__ : config: 
{'data_dir': '/data2/dy/code/unmasked_teacher/umt_data', 'data_root': '/data2/dy/code/unmasked_teacher/umt_data/videos_images', 'anno_root_pt': '/data2/dy/code/unmasked_teacher/umt_data/anno_pretrain', 'anno_root_downstream': '/data2/dy/code/unmasked_teacher/umt_data/anno_downstream', 'TextEncoders': {'bert': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'bert_large': {'name': 'bert_large', 'pretrained': 'bert-large-uncased', 'config': 'configs/config_bert_large.json', 'd_model': 1024, 'fusion_layer': 19}}, 'train_file': ['/data2/dy/temporal_video/dataset/without_webvid_annotations/VLM_split/allreverse_rewrite.json', '/data2/dy/temporal_video/dataset', 'video'], 'test_file': {'test': ['/data2/dy/temporal_video/dataset/without_webvid_annotations/test_reverse.json', '/data2/dy/temporal_video/dataset', 'video']}, 'test_types': ['test'], 'num_workers': 6, 'rtime': True, 'pos_num': 1, 'neg_num': 10, 'stop_key': 'test/', 'is_paragraph_retrieval': False, 'is_pretrain': False, 'num_frames': 12, 'num_frames_test': 12, 'batch_size': 16, 'max_txt_l': 50, 'evaluate': True, 'train_shuffle': True, 'inputs': {'image_res': 224, 'video_input': {'num_frames': 12, 'sample_type': 'rand', 'num_frames_test': 12, 'sample_type_test': 'middle', 'random_aug': False}, 'max_txt_l': {'image': 50, 'video': 50}, 'batch_size': {'image': 16, 'video': 16}, 'batch_size_test': {'image': 16, 'video': 16}}, 'text_enc': 'bert', 'model': {'model_cls': 'UMT_temporal', 'vision_encoder': {'name': 'vit_b16', 'img_size': 224, 'patch_size': 16, 'd_model': 768, 'encoder_embed_dim': 768, 'encoder_depth': 12, 'encoder_num_heads': 12, 'drop_path_rate': 0.2, 'num_frames': 12, 'tubelet_size': 1, 'use_checkpoint': True, 'checkpoint_num': 12, 'clip_decoder_embed_dim': 768, 'clip_output_dim': 512, 'clip_return_layer': 0, 'clip_student_return_interval': 1, 'pretrained': '/datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth', 'clip_teacher': 'none', 'clip_img_size': 224, 'clip_return_interval': 1, 'video_mask_type': 'attention', 'video_mask_ratio': 0.0, 'video_double_mask_ratio': 0.0, 'image_mask_type': 'attention', 'image_mask_ratio': 0.0, 'image_double_mask_ratio': 0.0, 'keep_temporal': True}, 'text_encoder': {'name': 'bert_base', 'pretrained': 'bert-base-uncased', 'config': 'configs/config_bert.json', 'd_model': 768, 'fusion_layer': 9}, 'multimodal': {'enable': True}, 'embed_dim': 512, 'temp': 0.07}, 'criterion': {'loss_weight': {'vtc': 1.0, 'mlm': 0.0, 'vtm': 1.0, 'uta': 0.0}, 'vtm_hard_neg': True, 'mlm_masking_prob': 0.5, 'uta_norm_type': 'l2', 'uta_loss_type': 'l2'}, 'optimizer': {'opt': 'adamW', 'lr': 1e-05, 'opt_betas': [0.9, 0.999], 'weight_decay': 0.02, 'max_grad_norm': -1, 'different_lr': {'enable': False, 'module_names': [], 'lr': 0.001}}, 'scheduler': {'sched': 'cosine', 'epochs': 5, 'min_lr_multi': 0.01, 'warmup_epochs': 1, 'num_training_steps': 1055, 'num_warmup_steps': 211}, 'zero_shot': False, 'deep_fusion': False, 'evaluation': {'eval_frame_ensemble': 'concat', 'eval_x_only': False, 'k_test': 128, 'eval_offload': True}, 'fp16': True, 'gradient_checkpointing': True, 'wandb': {'enable': False, 'entity': 'user', 'project': 'umt_ret'}, 'dist_url': 'env://', 'device': 'cuda', 'mode': 'pt', 'output_dir': 'exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg/eval_after_training', 'resume': False, 'debug': False, 'log_freq': 100, 'seed': 3407, 'save_latest': True, 'auto_resume': True, 'pretrained_path': 'exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg/ckpt_best.pth', 'rank': 0, 'world_size': 2, 'gpu': 0, 'distributed': True, 'dist_backend': 'nccl', 'result_dir': 'exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg/eval_after_training'}
2024-05-27T11:07:02 | INFO | __main__ : train_file: ['/data2/dy/temporal_video/dataset/without_webvid_annotations/VLM_split/allreverse_rewrite.json', '/data2/dy/temporal_video/dataset', 'video']
2024-05-27T11:07:02 | INFO | tasks.pretrain : Creating dataset for ret
2024-05-27T11:07:02 | INFO | tasks.shared_utils : Creating model
2024-05-27T11:07:12 | INFO | models.umt : Build vision_encoder: vit_b16
2024-05-27T11:07:12 | INFO | models.backbones.vit.vit : Num of patches: 2352
2024-05-27T11:07:12 | INFO | models.backbones.vit.vit : Use checkpoint: True
2024-05-27T11:07:12 | INFO | models.backbones.vit.vit : Checkpoint number: 12
2024-05-27T11:07:12 | INFO | models.backbones.vit.vit : Student return index: []
2024-05-27T11:07:25 | INFO | models.backbones.vit.vit : Loading pretrained weights from /datassd2/pretrained_models/Unmasked_Teacher/b16_ptk710_f8_res224.pth
2024-05-27T11:07:25 | INFO | models.umt : Build text_encoder bert_base
2024-05-27T11:07:27 | INFO | models.backbones.bert.xbert : build bert with cross_module: ca
2024-05-27T11:07:27 | INFO | models.criterions : Norm type: l2
2024-05-27T11:07:27 | INFO | models.criterions : Loss type: l2
2024-05-27T11:07:28 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.temp: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.encoder.norm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_encoder.pool_norm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.embeddings.word_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.embeddings.position_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.embeddings.token_type_embeddings.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.embeddings.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.embeddings.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.query.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.query.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.key.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.key.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.value.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.self.value.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.crossattention.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.dense.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_encoder.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.vision_proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_proj.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.text_proj.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.itm_head.weight: wd: 0.02, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : param module.itm_head.bias: wd: 0, lr: 1e-05
2024-05-27T11:07:28 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0.02 len(p)=140
2024-05-27T11:07:28 | INFO | utils.optimizer : optimizer -- lr=1e-05 wd=0 len(p)=256
2024-05-27T11:07:28 | INFO | tasks.shared_utils : Auto resuming
2024-05-27T11:07:28 | INFO | tasks.shared_utils : Not found checkpoint in exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg/eval_after_training
2024-05-27T11:07:30 | INFO | tasks.shared_utils : <All keys matched successfully>
2024-05-27T11:07:30 | INFO | tasks.shared_utils : Loaded checkpoint from exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg/ckpt_best.pth
2024-05-27T11:07:30 | INFO | __main__ : Start evaluation
2024-05-27T11:07:30 | INFO | tasks.retrieval_utils : Start evaluation for media_type=video
2024-05-27T11:07:30 | INFO | tasks.retrieval_utils : Computing dual encoder features...
2024-05-27T11:07:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:32 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:33 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:33 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:33 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:33 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:33 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:33 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:42 | WARNING | py.warnings : /data2/dy/code/unmasked_teacher/multi_modality/utils/distributed.py:18: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  builtin_warn(*args, **kwargs)

2024-05-27T11:07:43 | INFO | utils.basic_utils : extracting image feats  [  0/125]  eta: 0:24:17    time: 11.6596  data: 11.0190  max mem: 34824 res mem: 40092
2024-05-27T11:10:45 | INFO | utils.basic_utils : extracting image feats  [100/125]  eta: 0:00:47    time: 1.6148  data: 1.0476  max mem: 34824 res mem: 40092
2024-05-27T11:11:25 | INFO | utils.basic_utils : extracting image feats  [124/125]  eta: 0:00:01    time: 1.6116  data: 1.0537  max mem: 34824 res mem: 40092
2024-05-27T11:11:25 | INFO | utils.basic_utils : extracting image feats Total time: 0:03:54 (1.8730 s / it)
2024-05-27T11:11:41 | INFO | tasks.retrieval_utils : Finished feature extraction
2024-05-27T11:11:41 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product]
2024-05-27T11:11:41 | INFO | tasks.retrieval_utils : Computing ITC scores [dot-product], done!
2024-05-27T11:11:41 | INFO | tasks.retrieval_utils : Rerank dual-encoder results with cross-encoder...
2024-05-27T11:11:41 | INFO | tasks.retrieval_utils : i2t_scores.shape torch.Size([1001, 2000])
2024-05-27T11:11:41 | INFO | tasks.retrieval_utils : n_clip_per_video=1, with eval_frame_ensemble=concat
2024-05-27T11:11:41 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:00:36    time: 0.0362  data: 0.0026  max mem: 34824 res mem: 40092
2024-05-27T11:11:49 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:01:15    time: 0.0852  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:11:58 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:01:07    time: 0.0853  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:12:06 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:00:59    time: 0.0856  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:12:15 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:00:50    time: 0.0856  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:12:23 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:00:42    time: 0.0857  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:12:32 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:00:34    time: 0.0859  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:12:40 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:00:25    time: 0.0856  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:12:49 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:00:17    time: 0.0858  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:12:58 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:00:08    time: 0.0862  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:13:06 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:00    time: 0.0864  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:13:06 | INFO | utils.basic_utils : Evaluation: Total time: 0:01:25 (0.0855 s / it)
2024-05-27T11:13:06 | INFO | tasks.retrieval_utils : t2i_scores.shape torch.Size([1001, 2000])
2024-05-27T11:13:08 | INFO | utils.basic_utils : Evaluation:  [   0/1001]  eta: 0:21:25    time: 1.2844  data: 0.0019  max mem: 34824 res mem: 40092
2024-05-27T11:15:05 | INFO | utils.basic_utils : Evaluation:  [ 100/1001]  eta: 0:17:35    time: 1.1646  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:17:01 | INFO | utils.basic_utils : Evaluation:  [ 200/1001]  eta: 0:15:35    time: 1.1711  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:18:57 | INFO | utils.basic_utils : Evaluation:  [ 300/1001]  eta: 0:13:35    time: 1.1525  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:20:52 | INFO | utils.basic_utils : Evaluation:  [ 400/1001]  eta: 0:11:37    time: 1.1525  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:22:48 | INFO | utils.basic_utils : Evaluation:  [ 500/1001]  eta: 0:09:41    time: 1.1484  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:24:44 | INFO | utils.basic_utils : Evaluation:  [ 600/1001]  eta: 0:07:45    time: 1.1737  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:26:39 | INFO | utils.basic_utils : Evaluation:  [ 700/1001]  eta: 0:05:49    time: 1.1469  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:28:35 | INFO | utils.basic_utils : Evaluation:  [ 800/1001]  eta: 0:03:53    time: 1.1707  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:30:31 | INFO | utils.basic_utils : Evaluation:  [ 900/1001]  eta: 0:01:57    time: 1.1371  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:32:34 | INFO | utils.basic_utils : Evaluation:  [1000/1001]  eta: 0:00:01    time: 1.2371  data: 0.0000  max mem: 34824 res mem: 40092
2024-05-27T11:32:34 | INFO | utils.basic_utils : Evaluation: Total time: 0:19:27 (1.1668 s / it)
2024-05-27T11:32:35 | INFO | tasks.retrieval_utils : Evaluation time 0:25:05
2024-05-27T11:32:37 | INFO | __main__ : Epoch 0
2024-05-27T11:32:37 | INFO | __main__ : 
           txt_r1  txt_r5  txt_r10  txt_r_mean  img_r1  img_r5  img_r10  img_r_mean  r_mean  txt_acc  img_acc
test/       11.65   35.65    49.55       32.28    17.3   48.05    61.30       42.22   37.25    50.90    53.05
test_emb/   38.95   89.00    95.30       74.42    37.5   88.20    95.25       73.65   74.03    50.55    50.10
2024-05-27T11:32:37 | INFO | __main__ : Training time 0:25:07
2024-05-27T11:32:37 | INFO | __main__ : best epoch 0 [config.stop_key test/]
2024-05-27T11:32:37 | INFO | __main__ : Checkpoints and Logs saved at exp/finetuning/ret_rtime_vlp/without_rewrite_1pos10neg/eval_after_training
